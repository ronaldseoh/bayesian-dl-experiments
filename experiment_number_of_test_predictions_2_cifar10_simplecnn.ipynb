{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bka_bK83VFHh"
   },
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D5NSPs_cJZe"
   },
   "source": [
    "### Random seed / PyTorch / CUDA related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6214,
     "status": "ok",
     "timestamp": 1575020214134,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "pHbfpytEVFHu",
    "outputId": "2a30e001-783e-4de2-f1ce-8211ba20613d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # If there's a package I need to install separately, do it here\n",
    "    !pip install pyro-ppl\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd 'drive/My Drive/Colab Notebooks/bayesian-dl-experiments'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls\n",
    "\n",
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seeds\n",
    "# Based on https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random_seed = 682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eag8z68GGbRu"
   },
   "source": [
    "### Third party libraries (NumPy, PyTorch, Pyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6773,
     "status": "ok",
     "timestamp": 1575020214716,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "dHn9F3GMGbR1",
    "outputId": "97858cb4-449b-4120-901e-3eff9141c218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Version: 1.17.2\n",
      "PyTorch Version: 1.3.1\n",
      "Pyro Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Third party libraries import\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print version information\n",
    "print(\"NumPy Version: \" + np.__version__)\n",
    "print(\"PyTorch Version: \" + torch.__version__)\n",
    "print(\"Pyro Version: \" + pyro.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7006,
     "status": "ok",
     "timestamp": 1575020214967,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "PrhSLV8pGbSB",
    "outputId": "adeab5e6-c83e-4ff0-8b87-e0e885bf0570"
   },
   "outputs": [],
   "source": [
    "# More imports...\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader, RandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pyro.infer import SVI, Trace_ELBO, HMC, MCMC\n",
    "\n",
    "# Import model and dataset classes from ronald_bdl\n",
    "from ronald_bdl import models, datasets\n",
    "from ronald_bdl.models import utils\n",
    "\n",
    "# pyplot setting\n",
    "%matplotlib inline\n",
    "\n",
    "# torch.device / CUDA Setup\n",
    "use_cuda = True\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "            \n",
    "    # Disable 'benchmark' mode\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    use_pin_memory = True # Faster Host to GPU copies with page-locked memory\n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))    \n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIFRoH3AcJZn"
   },
   "source": [
    "### Variable settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NBuZI0cHnHy"
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqiyKf1PHtR6"
   },
   "outputs": [],
   "source": [
    "# CIFAR10 data transformation setting\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Set the proportion of the original dataset to be available as a whole\n",
    "subset_proportions = [0.01, 0.1, 1]\n",
    "\n",
    "# Proportion of the dataset (after getting the subset) to be used for training\n",
    "train_prop = 0.9\n",
    "\n",
    "# Number of dataset splits\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBszD08sH4c4"
   },
   "source": [
    "#### NN settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6Z8-H-YH8kQ"
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "dropout_rates = [0.1, 0.3, 0.5]\n",
    "\n",
    "# Length-scale\n",
    "length_scale_values = [1e-2]\n",
    "\n",
    "# Model Precision\n",
    "tau_values = [0.1, 0.15, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0U0QTMyEIVDr"
   },
   "source": [
    "#### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_pzGq1_cJZp"
   },
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "n_epoch = 4000\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Data batch sizes\n",
    "n_training_batch = 512\n",
    "\n",
    "# Number of test predictions (for each data point)\n",
    "prediction_runs = [4, 10, 40, 100, 400, 1000]\n",
    "\n",
    "# Cross Entropy to minimize\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test start time\n",
    "test_start_time = datetime.datetime.today().strftime('%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PpzPMI8VFKE"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1585801,
     "status": "ok",
     "timestamp": 1575025822894,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "m4kavCiTVFKf",
    "outputId": "78d6dac6-74f5-4ea5-d45b-347f4b118049",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "subset size = (500, 32, 32, 3)\n",
      "training set size = 450\n",
      "test set size = 50\n",
      "SimpleCIFAR10(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc1_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (fc3_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "reg_strength = tensor(1.0000e-06)\n",
      "\n",
      "Starting subset 0.010000, dropout_rate 0.100000, length_scale 0.010000, tau 0.100000\n",
      "epoch 0 loss = 2.303156\n",
      "epoch 1 loss = 2.286091\n",
      "epoch 2 loss = 2.270812\n",
      "epoch 3 loss = 2.255121\n",
      "epoch 4 loss = 2.246520\n",
      "epoch 5 loss = 2.220996\n",
      "epoch 6 loss = 2.202120\n",
      "epoch 7 loss = 2.186728\n",
      "epoch 8 loss = 2.172418\n",
      "epoch 9 loss = 2.136952\n",
      "epoch 10 loss = 2.110613\n",
      "epoch 11 loss = 2.098691\n",
      "epoch 12 loss = 2.052109\n",
      "epoch 13 loss = 2.020241\n",
      "epoch 14 loss = 2.034080\n",
      "epoch 15 loss = 1.982670\n",
      "epoch 16 loss = 1.941279\n",
      "epoch 17 loss = 1.933552\n",
      "epoch 18 loss = 1.923811\n",
      "epoch 19 loss = 1.881739\n",
      "epoch 20 loss = 1.846834\n",
      "epoch 21 loss = 1.829060\n",
      "epoch 22 loss = 1.810489\n",
      "epoch 23 loss = 1.781709\n",
      "epoch 24 loss = 1.739780\n",
      "epoch 25 loss = 1.724597\n",
      "epoch 26 loss = 1.677939\n",
      "epoch 27 loss = 1.678437\n",
      "epoch 28 loss = 1.660247\n",
      "epoch 29 loss = 1.581303\n",
      "epoch 30 loss = 1.577104\n",
      "epoch 31 loss = 1.569100\n",
      "epoch 32 loss = 1.561665\n",
      "epoch 33 loss = 1.520937\n",
      "epoch 34 loss = 1.521094\n",
      "epoch 35 loss = 1.533150\n",
      "epoch 36 loss = 1.496060\n",
      "epoch 37 loss = 1.466390\n",
      "epoch 38 loss = 1.362408\n",
      "epoch 39 loss = 1.368217\n",
      "epoch 40 loss = 1.415269\n",
      "epoch 41 loss = 1.319766\n",
      "epoch 42 loss = 1.336263\n",
      "epoch 43 loss = 1.281432\n",
      "epoch 44 loss = 1.267524\n",
      "epoch 45 loss = 1.234456\n",
      "epoch 46 loss = 1.221480\n",
      "epoch 47 loss = 1.223939\n",
      "epoch 48 loss = 1.201849\n",
      "epoch 49 loss = 1.111244\n",
      "epoch 50 loss = 1.109243\n",
      "epoch 51 loss = 1.069954\n",
      "epoch 52 loss = 1.104803\n",
      "epoch 53 loss = 1.089431\n",
      "epoch 54 loss = 1.031400\n",
      "epoch 55 loss = 1.013234\n",
      "epoch 56 loss = 1.004766\n",
      "epoch 57 loss = 0.989651\n",
      "epoch 58 loss = 0.948708\n",
      "epoch 59 loss = 0.958209\n",
      "epoch 60 loss = 0.927737\n",
      "epoch 61 loss = 0.891365\n",
      "epoch 62 loss = 0.871648\n",
      "epoch 63 loss = 0.879350\n",
      "epoch 64 loss = 0.838515\n",
      "epoch 65 loss = 0.874229\n",
      "epoch 66 loss = 0.776827\n",
      "epoch 67 loss = 0.804918\n",
      "epoch 68 loss = 0.759899\n",
      "epoch 69 loss = 0.744756\n",
      "epoch 70 loss = 0.794184\n",
      "epoch 71 loss = 0.720496\n",
      "epoch 72 loss = 0.737580\n",
      "epoch 73 loss = 0.706933\n",
      "epoch 74 loss = 0.678268\n",
      "epoch 75 loss = 0.728649\n",
      "epoch 76 loss = 0.648678\n",
      "epoch 77 loss = 0.598957\n",
      "epoch 78 loss = 0.629092\n",
      "epoch 79 loss = 0.610230\n",
      "epoch 80 loss = 0.591738\n",
      "epoch 81 loss = 0.538962\n",
      "epoch 82 loss = 0.592017\n",
      "epoch 83 loss = 0.523717\n",
      "epoch 84 loss = 0.573538\n",
      "epoch 85 loss = 0.533037\n",
      "epoch 86 loss = 0.522617\n",
      "epoch 87 loss = 0.549654\n",
      "epoch 88 loss = 0.490765\n",
      "epoch 89 loss = 0.475071\n",
      "epoch 90 loss = 0.464546\n",
      "epoch 91 loss = 0.520172\n",
      "epoch 92 loss = 0.448487\n",
      "epoch 93 loss = 0.471412\n",
      "epoch 94 loss = 0.443356\n",
      "epoch 95 loss = 0.445730\n",
      "epoch 96 loss = 0.443477\n",
      "epoch 97 loss = 0.400125\n",
      "epoch 98 loss = 0.397442\n",
      "epoch 99 loss = 0.413761\n",
      "epoch 100 loss = 0.353419\n",
      "epoch 101 loss = 0.362966\n",
      "epoch 102 loss = 0.379417\n",
      "epoch 103 loss = 0.352202\n",
      "epoch 104 loss = 0.385372\n",
      "epoch 105 loss = 0.338573\n",
      "epoch 106 loss = 0.368047\n",
      "epoch 107 loss = 0.334092\n",
      "epoch 108 loss = 0.348941\n",
      "epoch 109 loss = 0.298753\n",
      "epoch 110 loss = 0.344048\n",
      "epoch 111 loss = 0.315323\n",
      "epoch 112 loss = 0.285473\n",
      "epoch 113 loss = 0.300813\n",
      "epoch 114 loss = 0.262660\n",
      "epoch 115 loss = 0.272488\n",
      "epoch 116 loss = 0.295697\n",
      "epoch 117 loss = 0.300974\n",
      "epoch 118 loss = 0.261352\n",
      "epoch 119 loss = 0.259735\n",
      "epoch 120 loss = 0.300937\n",
      "epoch 121 loss = 0.223583\n",
      "epoch 122 loss = 0.282889\n",
      "epoch 123 loss = 0.222681\n",
      "epoch 124 loss = 0.253108\n",
      "epoch 125 loss = 0.251665\n",
      "epoch 126 loss = 0.278409\n",
      "epoch 127 loss = 0.231368\n",
      "epoch 128 loss = 0.240380\n",
      "epoch 129 loss = 0.228864\n",
      "epoch 130 loss = 0.246837\n",
      "epoch 131 loss = 0.230543\n",
      "epoch 132 loss = 0.227894\n",
      "epoch 133 loss = 0.226909\n",
      "epoch 134 loss = 0.251417\n",
      "epoch 135 loss = 0.219953\n",
      "epoch 136 loss = 0.219972\n",
      "epoch 137 loss = 0.183400\n",
      "epoch 138 loss = 0.173400\n",
      "epoch 139 loss = 0.209789\n",
      "epoch 140 loss = 0.206921\n",
      "epoch 141 loss = 0.186111\n",
      "epoch 142 loss = 0.171607\n",
      "epoch 143 loss = 0.205362\n",
      "epoch 144 loss = 0.171974\n",
      "epoch 145 loss = 0.183306\n",
      "epoch 146 loss = 0.175597\n",
      "epoch 147 loss = 0.197660\n",
      "epoch 148 loss = 0.181490\n",
      "epoch 149 loss = 0.197731\n",
      "epoch 150 loss = 0.173847\n",
      "epoch 151 loss = 0.178649\n",
      "epoch 152 loss = 0.146464\n",
      "epoch 153 loss = 0.143571\n",
      "epoch 154 loss = 0.150194\n",
      "epoch 155 loss = 0.191074\n",
      "epoch 156 loss = 0.142863\n",
      "epoch 157 loss = 0.138851\n",
      "epoch 158 loss = 0.148630\n",
      "epoch 159 loss = 0.156150\n",
      "epoch 160 loss = 0.152790\n",
      "epoch 161 loss = 0.159882\n",
      "epoch 162 loss = 0.151097\n",
      "epoch 163 loss = 0.168456\n",
      "epoch 164 loss = 0.143047\n",
      "epoch 165 loss = 0.142989\n",
      "epoch 166 loss = 0.146481\n",
      "epoch 167 loss = 0.141758\n",
      "epoch 168 loss = 0.144210\n",
      "epoch 169 loss = 0.168374\n",
      "epoch 170 loss = 0.103814\n",
      "epoch 171 loss = 0.138167\n",
      "epoch 172 loss = 0.134370\n",
      "epoch 173 loss = 0.116913\n",
      "epoch 174 loss = 0.140283\n",
      "epoch 175 loss = 0.145102\n",
      "epoch 176 loss = 0.126563\n",
      "epoch 177 loss = 0.157136\n",
      "epoch 178 loss = 0.119452\n",
      "epoch 179 loss = 0.154831\n",
      "epoch 180 loss = 0.110805\n",
      "epoch 181 loss = 0.116657\n",
      "epoch 182 loss = 0.142380\n",
      "epoch 183 loss = 0.130452\n",
      "epoch 184 loss = 0.102473\n",
      "epoch 185 loss = 0.148002\n",
      "epoch 186 loss = 0.097232\n",
      "epoch 187 loss = 0.116785\n",
      "epoch 188 loss = 0.105147\n",
      "epoch 189 loss = 0.096630\n",
      "epoch 190 loss = 0.147847\n",
      "epoch 191 loss = 0.136566\n",
      "epoch 192 loss = 0.129440\n",
      "epoch 193 loss = 0.138677\n",
      "epoch 194 loss = 0.104465\n",
      "epoch 195 loss = 0.127686\n",
      "epoch 196 loss = 0.128397\n",
      "epoch 197 loss = 0.132471\n",
      "epoch 198 loss = 0.097979\n",
      "epoch 199 loss = 0.108363\n",
      "epoch 200 loss = 0.121724\n",
      "epoch 201 loss = 0.118104\n",
      "epoch 202 loss = 0.098759\n",
      "epoch 203 loss = 0.117043\n",
      "epoch 204 loss = 0.129498\n",
      "epoch 205 loss = 0.110291\n",
      "epoch 206 loss = 0.118479\n",
      "epoch 207 loss = 0.110597\n",
      "epoch 208 loss = 0.096805\n",
      "epoch 209 loss = 0.143517\n",
      "epoch 210 loss = 0.119028\n",
      "epoch 211 loss = 0.107895\n",
      "epoch 212 loss = 0.129228\n",
      "epoch 213 loss = 0.114233\n",
      "epoch 214 loss = 0.108544\n",
      "epoch 215 loss = 0.102808\n",
      "epoch 216 loss = 0.111069\n",
      "epoch 217 loss = 0.135192\n",
      "epoch 218 loss = 0.128548\n",
      "epoch 219 loss = 0.118001\n",
      "epoch 220 loss = 0.121688\n",
      "epoch 221 loss = 0.107320\n",
      "epoch 222 loss = 0.118322\n",
      "epoch 223 loss = 0.134610\n",
      "epoch 224 loss = 0.112316\n",
      "epoch 225 loss = 0.139666\n",
      "epoch 226 loss = 0.093733\n",
      "epoch 227 loss = 0.111723\n",
      "epoch 228 loss = 0.086463\n",
      "epoch 229 loss = 0.095499\n",
      "epoch 230 loss = 0.075700\n",
      "epoch 231 loss = 0.090299\n",
      "epoch 232 loss = 0.103523\n",
      "epoch 233 loss = 0.110496\n",
      "epoch 234 loss = 0.099586\n",
      "epoch 235 loss = 0.076737\n",
      "epoch 236 loss = 0.104882\n",
      "epoch 237 loss = 0.127322\n",
      "epoch 238 loss = 0.081820\n",
      "epoch 239 loss = 0.103128\n",
      "epoch 240 loss = 0.106210\n",
      "epoch 241 loss = 0.107898\n",
      "epoch 242 loss = 0.088173\n",
      "epoch 243 loss = 0.087713\n",
      "epoch 244 loss = 0.097300\n",
      "epoch 245 loss = 0.089267\n",
      "epoch 246 loss = 0.093783\n",
      "epoch 247 loss = 0.096307\n",
      "epoch 248 loss = 0.103047\n",
      "epoch 249 loss = 0.068747\n",
      "epoch 250 loss = 0.088868\n",
      "epoch 251 loss = 0.097842\n",
      "epoch 252 loss = 0.085055\n",
      "epoch 253 loss = 0.078543\n",
      "epoch 254 loss = 0.110742\n",
      "epoch 255 loss = 0.099179\n",
      "epoch 256 loss = 0.082271\n",
      "epoch 257 loss = 0.100546\n",
      "epoch 258 loss = 0.088351\n",
      "epoch 259 loss = 0.087016\n",
      "epoch 260 loss = 0.085089\n",
      "epoch 261 loss = 0.084697\n",
      "epoch 262 loss = 0.086810\n",
      "epoch 263 loss = 0.078356\n",
      "epoch 264 loss = 0.070808\n",
      "epoch 265 loss = 0.095534\n",
      "epoch 266 loss = 0.097613\n",
      "epoch 267 loss = 0.072003\n",
      "epoch 268 loss = 0.071590\n",
      "epoch 269 loss = 0.086589\n",
      "epoch 270 loss = 0.074205\n",
      "epoch 271 loss = 0.084637\n",
      "epoch 272 loss = 0.082874\n",
      "epoch 273 loss = 0.089342\n",
      "epoch 274 loss = 0.079497\n",
      "epoch 275 loss = 0.081330\n",
      "epoch 276 loss = 0.060375\n",
      "epoch 277 loss = 0.091520\n",
      "epoch 278 loss = 0.101557\n",
      "epoch 279 loss = 0.081067\n",
      "epoch 280 loss = 0.078121\n",
      "epoch 281 loss = 0.074258\n",
      "epoch 282 loss = 0.058964\n",
      "epoch 283 loss = 0.082584\n",
      "epoch 284 loss = 0.073127\n",
      "epoch 285 loss = 0.083284\n",
      "epoch 286 loss = 0.061528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 287 loss = 0.056473\n",
      "epoch 288 loss = 0.089532\n",
      "epoch 289 loss = 0.090490\n",
      "epoch 290 loss = 0.073714\n",
      "epoch 291 loss = 0.097418\n",
      "epoch 292 loss = 0.085376\n",
      "epoch 293 loss = 0.087050\n",
      "epoch 294 loss = 0.072535\n",
      "epoch 295 loss = 0.080948\n",
      "epoch 296 loss = 0.111284\n",
      "epoch 297 loss = 0.098631\n",
      "epoch 298 loss = 0.082034\n",
      "epoch 299 loss = 0.071110\n",
      "epoch 300 loss = 0.080521\n",
      "epoch 301 loss = 0.083964\n",
      "epoch 302 loss = 0.059756\n",
      "epoch 303 loss = 0.110373\n",
      "epoch 304 loss = 0.079649\n",
      "epoch 305 loss = 0.088312\n",
      "epoch 306 loss = 0.081529\n",
      "epoch 307 loss = 0.059407\n",
      "epoch 308 loss = 0.066661\n",
      "epoch 309 loss = 0.076289\n",
      "epoch 310 loss = 0.069648\n",
      "epoch 311 loss = 0.081049\n",
      "epoch 312 loss = 0.055855\n",
      "epoch 313 loss = 0.084752\n",
      "epoch 314 loss = 0.073649\n",
      "epoch 315 loss = 0.095642\n",
      "epoch 316 loss = 0.066193\n",
      "epoch 317 loss = 0.055927\n",
      "epoch 318 loss = 0.070524\n",
      "epoch 319 loss = 0.090904\n",
      "epoch 320 loss = 0.073267\n",
      "epoch 321 loss = 0.081660\n",
      "epoch 322 loss = 0.055308\n",
      "epoch 323 loss = 0.063947\n",
      "epoch 324 loss = 0.066035\n",
      "epoch 325 loss = 0.083924\n",
      "epoch 326 loss = 0.063729\n",
      "epoch 327 loss = 0.079342\n",
      "epoch 328 loss = 0.093752\n",
      "epoch 329 loss = 0.089361\n",
      "epoch 330 loss = 0.112012\n",
      "epoch 331 loss = 0.057208\n",
      "epoch 332 loss = 0.082256\n",
      "epoch 333 loss = 0.076624\n",
      "epoch 334 loss = 0.056662\n",
      "epoch 335 loss = 0.075253\n",
      "epoch 336 loss = 0.103063\n",
      "epoch 337 loss = 0.082272\n",
      "epoch 338 loss = 0.083384\n",
      "epoch 339 loss = 0.101574\n",
      "epoch 340 loss = 0.093391\n",
      "epoch 341 loss = 0.066732\n",
      "epoch 342 loss = 0.081296\n",
      "epoch 343 loss = 0.076131\n",
      "epoch 344 loss = 0.067267\n",
      "epoch 345 loss = 0.069851\n",
      "epoch 346 loss = 0.061736\n",
      "epoch 347 loss = 0.085921\n",
      "epoch 348 loss = 0.081521\n",
      "epoch 349 loss = 0.091222\n",
      "epoch 350 loss = 0.087270\n",
      "epoch 351 loss = 0.062685\n",
      "epoch 352 loss = 0.089060\n",
      "epoch 353 loss = 0.082906\n",
      "epoch 354 loss = 0.096265\n",
      "epoch 355 loss = 0.065045\n",
      "epoch 356 loss = 0.069945\n",
      "epoch 357 loss = 0.066877\n",
      "epoch 358 loss = 0.100173\n",
      "epoch 359 loss = 0.071742\n",
      "epoch 360 loss = 0.054066\n",
      "epoch 361 loss = 0.071103\n",
      "epoch 362 loss = 0.072139\n",
      "epoch 363 loss = 0.069665\n",
      "epoch 364 loss = 0.067791\n",
      "epoch 365 loss = 0.081280\n",
      "epoch 366 loss = 0.059414\n",
      "epoch 367 loss = 0.087646\n",
      "epoch 368 loss = 0.060219\n",
      "epoch 369 loss = 0.062621\n",
      "epoch 370 loss = 0.065288\n",
      "epoch 371 loss = 0.093575\n",
      "epoch 372 loss = 0.089879\n",
      "epoch 373 loss = 0.072687\n",
      "epoch 374 loss = 0.060393\n",
      "epoch 375 loss = 0.074370\n",
      "epoch 376 loss = 0.084648\n",
      "epoch 377 loss = 0.069944\n",
      "epoch 378 loss = 0.070768\n",
      "epoch 379 loss = 0.078606\n",
      "epoch 380 loss = 0.059627\n",
      "epoch 381 loss = 0.068533\n",
      "epoch 382 loss = 0.071193\n",
      "epoch 383 loss = 0.068709\n",
      "epoch 384 loss = 0.057593\n",
      "epoch 385 loss = 0.061657\n",
      "epoch 386 loss = 0.064844\n",
      "epoch 387 loss = 0.059563\n",
      "epoch 388 loss = 0.061890\n",
      "epoch 389 loss = 0.078660\n",
      "epoch 390 loss = 0.064711\n",
      "epoch 391 loss = 0.064692\n",
      "epoch 392 loss = 0.067149\n",
      "epoch 393 loss = 0.059405\n",
      "epoch 394 loss = 0.055366\n",
      "epoch 395 loss = 0.062502\n",
      "epoch 396 loss = 0.072122\n",
      "epoch 397 loss = 0.066837\n",
      "epoch 398 loss = 0.077043\n",
      "epoch 399 loss = 0.072330\n",
      "epoch 400 loss = 0.092315\n",
      "epoch 401 loss = 0.064985\n",
      "epoch 402 loss = 0.069065\n",
      "epoch 403 loss = 0.069416\n",
      "epoch 404 loss = 0.075109\n",
      "epoch 405 loss = 0.083422\n",
      "epoch 406 loss = 0.057650\n",
      "epoch 407 loss = 0.067657\n",
      "epoch 408 loss = 0.057029\n",
      "epoch 409 loss = 0.056838\n",
      "epoch 410 loss = 0.081692\n",
      "epoch 411 loss = 0.068616\n",
      "epoch 412 loss = 0.062663\n",
      "epoch 413 loss = 0.064216\n",
      "epoch 414 loss = 0.062059\n",
      "epoch 415 loss = 0.082248\n",
      "epoch 416 loss = 0.058242\n",
      "epoch 417 loss = 0.070035\n",
      "epoch 418 loss = 0.069964\n",
      "epoch 419 loss = 0.064518\n",
      "epoch 420 loss = 0.060011\n",
      "epoch 421 loss = 0.068696\n",
      "epoch 422 loss = 0.073922\n",
      "epoch 423 loss = 0.093308\n",
      "epoch 424 loss = 0.076433\n",
      "epoch 425 loss = 0.069311\n",
      "epoch 426 loss = 0.052002\n",
      "epoch 427 loss = 0.083163\n",
      "epoch 428 loss = 0.078192\n",
      "epoch 429 loss = 0.068773\n",
      "epoch 430 loss = 0.055161\n",
      "epoch 431 loss = 0.066853\n",
      "epoch 432 loss = 0.058964\n",
      "epoch 433 loss = 0.075169\n",
      "epoch 434 loss = 0.056520\n",
      "epoch 435 loss = 0.086145\n",
      "epoch 436 loss = 0.052932\n",
      "epoch 437 loss = 0.075721\n",
      "epoch 438 loss = 0.048303\n",
      "epoch 439 loss = 0.059564\n",
      "epoch 440 loss = 0.076341\n",
      "epoch 441 loss = 0.082896\n",
      "epoch 442 loss = 0.064905\n",
      "epoch 443 loss = 0.056442\n",
      "epoch 444 loss = 0.084799\n",
      "epoch 445 loss = 0.059672\n",
      "epoch 446 loss = 0.058398\n",
      "epoch 447 loss = 0.048814\n",
      "epoch 448 loss = 0.105756\n",
      "epoch 449 loss = 0.058085\n",
      "epoch 450 loss = 0.078123\n",
      "epoch 451 loss = 0.053660\n",
      "epoch 452 loss = 0.049551\n",
      "epoch 453 loss = 0.065441\n",
      "epoch 454 loss = 0.061331\n",
      "epoch 455 loss = 0.070739\n",
      "epoch 456 loss = 0.069380\n",
      "epoch 457 loss = 0.052102\n",
      "epoch 458 loss = 0.046465\n",
      "epoch 459 loss = 0.059580\n",
      "epoch 460 loss = 0.073679\n",
      "epoch 461 loss = 0.073933\n",
      "epoch 462 loss = 0.072877\n",
      "epoch 463 loss = 0.058747\n",
      "epoch 464 loss = 0.074153\n",
      "epoch 465 loss = 0.059919\n",
      "epoch 466 loss = 0.043424\n",
      "epoch 467 loss = 0.067022\n",
      "epoch 468 loss = 0.064823\n",
      "epoch 469 loss = 0.074219\n",
      "epoch 470 loss = 0.064711\n",
      "epoch 471 loss = 0.082044\n",
      "epoch 472 loss = 0.071486\n",
      "epoch 473 loss = 0.090387\n",
      "epoch 474 loss = 0.078245\n",
      "epoch 475 loss = 0.091607\n",
      "epoch 476 loss = 0.070982\n",
      "epoch 477 loss = 0.067491\n",
      "epoch 478 loss = 0.055371\n",
      "epoch 479 loss = 0.077200\n",
      "epoch 480 loss = 0.063897\n",
      "epoch 481 loss = 0.076693\n",
      "epoch 482 loss = 0.047218\n",
      "epoch 483 loss = 0.084631\n",
      "epoch 484 loss = 0.072695\n",
      "epoch 485 loss = 0.051619\n",
      "epoch 486 loss = 0.084383\n",
      "epoch 487 loss = 0.048280\n",
      "epoch 488 loss = 0.057365\n",
      "epoch 489 loss = 0.068680\n",
      "epoch 490 loss = 0.045150\n",
      "epoch 491 loss = 0.070437\n",
      "epoch 492 loss = 0.056293\n",
      "epoch 493 loss = 0.064689\n",
      "epoch 494 loss = 0.072091\n",
      "epoch 495 loss = 0.054082\n",
      "epoch 496 loss = 0.070056\n",
      "epoch 497 loss = 0.086466\n",
      "epoch 498 loss = 0.072549\n",
      "epoch 499 loss = 0.047266\n",
      "epoch 500 loss = 0.056817\n",
      "epoch 501 loss = 0.088128\n",
      "epoch 502 loss = 0.064269\n",
      "epoch 503 loss = 0.064823\n",
      "epoch 504 loss = 0.061068\n",
      "epoch 505 loss = 0.054519\n",
      "epoch 506 loss = 0.084174\n",
      "epoch 507 loss = 0.052754\n",
      "epoch 508 loss = 0.080235\n",
      "epoch 509 loss = 0.075469\n",
      "epoch 510 loss = 0.082745\n",
      "epoch 511 loss = 0.050912\n",
      "epoch 512 loss = 0.075784\n",
      "epoch 513 loss = 0.072503\n",
      "epoch 514 loss = 0.078762\n",
      "epoch 515 loss = 0.077389\n",
      "epoch 516 loss = 0.072251\n",
      "epoch 517 loss = 0.062700\n",
      "epoch 518 loss = 0.068306\n",
      "epoch 519 loss = 0.080664\n",
      "epoch 520 loss = 0.053175\n",
      "epoch 521 loss = 0.054803\n",
      "epoch 522 loss = 0.065029\n",
      "epoch 523 loss = 0.057854\n",
      "epoch 524 loss = 0.076391\n",
      "epoch 525 loss = 0.062504\n",
      "epoch 526 loss = 0.074240\n",
      "epoch 527 loss = 0.070733\n",
      "epoch 528 loss = 0.059924\n",
      "epoch 529 loss = 0.052678\n",
      "epoch 530 loss = 0.059118\n",
      "epoch 531 loss = 0.044201\n",
      "epoch 532 loss = 0.046147\n",
      "epoch 533 loss = 0.081510\n",
      "epoch 534 loss = 0.072550\n",
      "epoch 535 loss = 0.072888\n",
      "epoch 536 loss = 0.094564\n",
      "epoch 537 loss = 0.051683\n",
      "epoch 538 loss = 0.065923\n",
      "epoch 539 loss = 0.057689\n",
      "epoch 540 loss = 0.056107\n",
      "epoch 541 loss = 0.066995\n",
      "epoch 542 loss = 0.060376\n",
      "epoch 543 loss = 0.059608\n",
      "epoch 544 loss = 0.059338\n",
      "epoch 545 loss = 0.060741\n",
      "epoch 546 loss = 0.096402\n",
      "epoch 547 loss = 0.075722\n",
      "epoch 548 loss = 0.085028\n",
      "epoch 549 loss = 0.066794\n",
      "epoch 550 loss = 0.048569\n",
      "epoch 551 loss = 0.048742\n",
      "epoch 552 loss = 0.052316\n",
      "epoch 553 loss = 0.049350\n",
      "epoch 554 loss = 0.055631\n",
      "epoch 555 loss = 0.055568\n",
      "epoch 556 loss = 0.056647\n",
      "epoch 557 loss = 0.066555\n",
      "epoch 558 loss = 0.061028\n",
      "epoch 559 loss = 0.060831\n",
      "epoch 560 loss = 0.071747\n",
      "epoch 561 loss = 0.069493\n",
      "epoch 562 loss = 0.058535\n",
      "epoch 563 loss = 0.073453\n",
      "epoch 564 loss = 0.041495\n",
      "epoch 565 loss = 0.070979\n",
      "epoch 566 loss = 0.077932\n",
      "epoch 567 loss = 0.062256\n",
      "epoch 568 loss = 0.062485\n",
      "epoch 569 loss = 0.066314\n",
      "epoch 570 loss = 0.068250\n",
      "epoch 571 loss = 0.060162\n",
      "epoch 572 loss = 0.056934\n",
      "epoch 573 loss = 0.066814\n",
      "epoch 574 loss = 0.046673\n",
      "epoch 575 loss = 0.067329\n",
      "epoch 576 loss = 0.073682\n",
      "epoch 577 loss = 0.056873\n",
      "epoch 578 loss = 0.049994\n",
      "epoch 579 loss = 0.053719\n",
      "epoch 580 loss = 0.045910\n",
      "epoch 581 loss = 0.051346\n",
      "epoch 582 loss = 0.071598\n",
      "epoch 583 loss = 0.041122\n",
      "epoch 584 loss = 0.067599\n",
      "epoch 585 loss = 0.055420\n",
      "epoch 586 loss = 0.053539\n",
      "epoch 587 loss = 0.077192\n",
      "epoch 588 loss = 0.069109\n",
      "epoch 589 loss = 0.055142\n",
      "epoch 590 loss = 0.066195\n",
      "epoch 591 loss = 0.061978\n",
      "epoch 592 loss = 0.055967\n",
      "epoch 593 loss = 0.069321\n",
      "epoch 594 loss = 0.063327\n",
      "epoch 595 loss = 0.055822\n",
      "epoch 596 loss = 0.055298\n",
      "epoch 597 loss = 0.049143\n",
      "epoch 598 loss = 0.050852\n",
      "epoch 599 loss = 0.051378\n",
      "epoch 600 loss = 0.052316\n",
      "epoch 601 loss = 0.060927\n",
      "epoch 602 loss = 0.053962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 603 loss = 0.056630\n",
      "epoch 604 loss = 0.055198\n",
      "epoch 605 loss = 0.057462\n",
      "epoch 606 loss = 0.059737\n",
      "epoch 607 loss = 0.072978\n",
      "epoch 608 loss = 0.070772\n",
      "epoch 609 loss = 0.051276\n",
      "epoch 610 loss = 0.069434\n",
      "epoch 611 loss = 0.070574\n",
      "epoch 612 loss = 0.062726\n",
      "epoch 613 loss = 0.069680\n",
      "epoch 614 loss = 0.056649\n",
      "epoch 615 loss = 0.044121\n",
      "epoch 616 loss = 0.051062\n",
      "epoch 617 loss = 0.065692\n",
      "epoch 618 loss = 0.053326\n",
      "epoch 619 loss = 0.052260\n",
      "epoch 620 loss = 0.060334\n",
      "epoch 621 loss = 0.066994\n",
      "epoch 622 loss = 0.065786\n",
      "epoch 623 loss = 0.075845\n",
      "epoch 624 loss = 0.073019\n",
      "epoch 625 loss = 0.055459\n",
      "epoch 626 loss = 0.081499\n",
      "epoch 627 loss = 0.075067\n",
      "epoch 628 loss = 0.058020\n",
      "epoch 629 loss = 0.058493\n",
      "epoch 630 loss = 0.044651\n",
      "epoch 631 loss = 0.060678\n",
      "epoch 632 loss = 0.079276\n",
      "epoch 633 loss = 0.059222\n",
      "epoch 634 loss = 0.056256\n",
      "epoch 635 loss = 0.063784\n",
      "epoch 636 loss = 0.055654\n",
      "epoch 637 loss = 0.049103\n",
      "epoch 638 loss = 0.054159\n",
      "epoch 639 loss = 0.060080\n",
      "epoch 640 loss = 0.060614\n",
      "epoch 641 loss = 0.060651\n",
      "epoch 642 loss = 0.074603\n",
      "epoch 643 loss = 0.076661\n",
      "epoch 644 loss = 0.063506\n",
      "epoch 645 loss = 0.050275\n",
      "epoch 646 loss = 0.049446\n",
      "epoch 647 loss = 0.067580\n",
      "epoch 648 loss = 0.063585\n",
      "epoch 649 loss = 0.059915\n",
      "epoch 650 loss = 0.073511\n",
      "epoch 651 loss = 0.077389\n",
      "epoch 652 loss = 0.071755\n",
      "epoch 653 loss = 0.054643\n",
      "epoch 654 loss = 0.064241\n",
      "epoch 655 loss = 0.048078\n",
      "epoch 656 loss = 0.047489\n",
      "epoch 657 loss = 0.077720\n",
      "epoch 658 loss = 0.071085\n",
      "epoch 659 loss = 0.060357\n",
      "epoch 660 loss = 0.058932\n",
      "epoch 661 loss = 0.065289\n",
      "epoch 662 loss = 0.072061\n",
      "epoch 663 loss = 0.076276\n",
      "epoch 664 loss = 0.055922\n",
      "epoch 665 loss = 0.057012\n",
      "epoch 666 loss = 0.061040\n",
      "epoch 667 loss = 0.072914\n",
      "epoch 668 loss = 0.067693\n",
      "epoch 669 loss = 0.069233\n",
      "epoch 670 loss = 0.083497\n",
      "epoch 671 loss = 0.089733\n",
      "epoch 672 loss = 0.057818\n",
      "epoch 673 loss = 0.059559\n",
      "epoch 674 loss = 0.055346\n",
      "epoch 675 loss = 0.059993\n",
      "epoch 676 loss = 0.078881\n",
      "epoch 677 loss = 0.058128\n",
      "epoch 678 loss = 0.063429\n",
      "epoch 679 loss = 0.075407\n",
      "epoch 680 loss = 0.068654\n",
      "epoch 681 loss = 0.054301\n",
      "epoch 682 loss = 0.049078\n",
      "epoch 683 loss = 0.059955\n",
      "epoch 684 loss = 0.048435\n",
      "epoch 685 loss = 0.070713\n",
      "epoch 686 loss = 0.066437\n",
      "epoch 687 loss = 0.076988\n",
      "epoch 688 loss = 0.071024\n",
      "epoch 689 loss = 0.057684\n",
      "epoch 690 loss = 0.067021\n",
      "epoch 691 loss = 0.051635\n",
      "epoch 692 loss = 0.067102\n",
      "epoch 693 loss = 0.071384\n",
      "epoch 694 loss = 0.066987\n",
      "epoch 695 loss = 0.074564\n",
      "epoch 696 loss = 0.028198\n",
      "epoch 697 loss = 0.067952\n",
      "epoch 698 loss = 0.067920\n",
      "epoch 699 loss = 0.055593\n",
      "epoch 700 loss = 0.064993\n",
      "epoch 701 loss = 0.054463\n",
      "epoch 702 loss = 0.076288\n",
      "epoch 703 loss = 0.062614\n",
      "epoch 704 loss = 0.065793\n",
      "epoch 705 loss = 0.055289\n",
      "epoch 706 loss = 0.061909\n",
      "epoch 707 loss = 0.077432\n",
      "epoch 708 loss = 0.066675\n",
      "epoch 709 loss = 0.048862\n",
      "epoch 710 loss = 0.074573\n",
      "epoch 711 loss = 0.061445\n",
      "epoch 712 loss = 0.057880\n",
      "epoch 713 loss = 0.074786\n",
      "epoch 714 loss = 0.041411\n",
      "epoch 715 loss = 0.053762\n",
      "epoch 716 loss = 0.043539\n",
      "epoch 717 loss = 0.071729\n",
      "epoch 718 loss = 0.053732\n",
      "epoch 719 loss = 0.076254\n",
      "epoch 720 loss = 0.050666\n",
      "epoch 721 loss = 0.080740\n",
      "epoch 722 loss = 0.057332\n",
      "epoch 723 loss = 0.068280\n",
      "epoch 724 loss = 0.060834\n",
      "epoch 725 loss = 0.060593\n",
      "epoch 726 loss = 0.061839\n",
      "epoch 727 loss = 0.076819\n",
      "epoch 728 loss = 0.061760\n",
      "epoch 729 loss = 0.056072\n",
      "epoch 730 loss = 0.052724\n",
      "epoch 731 loss = 0.083401\n",
      "epoch 732 loss = 0.072437\n",
      "epoch 733 loss = 0.062800\n",
      "epoch 734 loss = 0.054460\n",
      "epoch 735 loss = 0.076609\n",
      "epoch 736 loss = 0.067862\n",
      "epoch 737 loss = 0.083031\n",
      "epoch 738 loss = 0.073329\n",
      "epoch 739 loss = 0.065990\n",
      "epoch 740 loss = 0.056314\n",
      "epoch 741 loss = 0.061217\n",
      "epoch 742 loss = 0.067717\n",
      "epoch 743 loss = 0.050044\n",
      "epoch 744 loss = 0.054598\n",
      "epoch 745 loss = 0.081991\n",
      "epoch 746 loss = 0.063418\n",
      "epoch 747 loss = 0.074241\n",
      "epoch 748 loss = 0.060151\n",
      "epoch 749 loss = 0.069321\n",
      "epoch 750 loss = 0.067684\n",
      "epoch 751 loss = 0.048079\n",
      "epoch 752 loss = 0.056002\n",
      "epoch 753 loss = 0.069149\n",
      "epoch 754 loss = 0.067586\n",
      "epoch 755 loss = 0.064368\n",
      "epoch 756 loss = 0.072636\n",
      "epoch 757 loss = 0.058223\n",
      "epoch 758 loss = 0.054111\n",
      "epoch 759 loss = 0.072683\n",
      "epoch 760 loss = 0.065210\n",
      "epoch 761 loss = 0.079884\n",
      "epoch 762 loss = 0.066513\n",
      "epoch 763 loss = 0.058713\n",
      "epoch 764 loss = 0.069269\n",
      "epoch 765 loss = 0.050787\n",
      "epoch 766 loss = 0.077660\n",
      "epoch 767 loss = 0.075337\n",
      "epoch 768 loss = 0.057578\n",
      "epoch 769 loss = 0.058923\n",
      "epoch 770 loss = 0.066648\n",
      "epoch 771 loss = 0.056456\n",
      "epoch 772 loss = 0.072623\n",
      "epoch 773 loss = 0.077281\n",
      "epoch 774 loss = 0.060170\n",
      "epoch 775 loss = 0.046870\n",
      "epoch 776 loss = 0.041796\n",
      "epoch 777 loss = 0.075621\n",
      "epoch 778 loss = 0.083598\n",
      "epoch 779 loss = 0.047620\n",
      "epoch 780 loss = 0.062396\n",
      "epoch 781 loss = 0.050502\n",
      "epoch 782 loss = 0.065667\n",
      "epoch 783 loss = 0.054375\n",
      "epoch 784 loss = 0.050703\n",
      "epoch 785 loss = 0.069225\n",
      "epoch 786 loss = 0.064471\n",
      "epoch 787 loss = 0.053471\n",
      "epoch 788 loss = 0.054756\n",
      "epoch 789 loss = 0.061899\n",
      "epoch 790 loss = 0.064782\n",
      "epoch 791 loss = 0.054332\n",
      "epoch 792 loss = 0.071703\n",
      "epoch 793 loss = 0.072473\n",
      "epoch 794 loss = 0.041208\n",
      "epoch 795 loss = 0.085208\n",
      "epoch 796 loss = 0.074218\n",
      "epoch 797 loss = 0.046894\n",
      "epoch 798 loss = 0.065466\n",
      "epoch 799 loss = 0.080899\n",
      "epoch 800 loss = 0.058612\n",
      "epoch 801 loss = 0.059498\n",
      "epoch 802 loss = 0.055295\n",
      "epoch 803 loss = 0.048543\n",
      "epoch 804 loss = 0.045129\n",
      "epoch 805 loss = 0.047513\n",
      "epoch 806 loss = 0.051396\n",
      "epoch 807 loss = 0.034330\n",
      "epoch 808 loss = 0.065797\n",
      "epoch 809 loss = 0.044074\n",
      "epoch 810 loss = 0.040952\n",
      "epoch 811 loss = 0.076035\n",
      "epoch 812 loss = 0.050122\n",
      "epoch 813 loss = 0.057418\n",
      "epoch 814 loss = 0.058825\n",
      "epoch 815 loss = 0.058610\n",
      "epoch 816 loss = 0.060320\n",
      "epoch 817 loss = 0.066309\n",
      "epoch 818 loss = 0.073854\n",
      "epoch 819 loss = 0.061350\n",
      "epoch 820 loss = 0.063718\n",
      "epoch 821 loss = 0.075764\n",
      "epoch 822 loss = 0.048357\n",
      "epoch 823 loss = 0.061606\n",
      "epoch 824 loss = 0.075154\n",
      "epoch 825 loss = 0.056185\n",
      "epoch 826 loss = 0.063832\n",
      "epoch 827 loss = 0.074117\n",
      "epoch 828 loss = 0.056814\n",
      "epoch 829 loss = 0.048980\n",
      "epoch 830 loss = 0.047574\n",
      "epoch 831 loss = 0.048869\n",
      "epoch 832 loss = 0.076397\n",
      "epoch 833 loss = 0.057825\n",
      "epoch 834 loss = 0.069989\n",
      "epoch 835 loss = 0.065714\n",
      "epoch 836 loss = 0.053298\n",
      "epoch 837 loss = 0.047729\n",
      "epoch 838 loss = 0.055861\n",
      "epoch 839 loss = 0.053180\n",
      "epoch 840 loss = 0.054526\n",
      "epoch 841 loss = 0.073720\n",
      "epoch 842 loss = 0.074979\n"
     ]
    }
   ],
   "source": [
    "for subset_prop, dropout_rate, length_scale, tau in itertools.product(\n",
    "    subset_proportions,\n",
    "    dropout_rates, length_scale_values, tau_values,\n",
    "):\n",
    "\n",
    "    # Reset the random number generator for each method (to produce identical results)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    pyro.set_rng_seed(random_seed)\n",
    "\n",
    "    \"\"\"\n",
    "    Results file storage\n",
    "    \"\"\"\n",
    " \n",
    "    # Create directory to store results for the current test configuration\n",
    "    test_results_path = os.path.join(\n",
    "        './test_results',\n",
    "        'number_of_test_predictions_2',\n",
    "        'CIFAR-10',\n",
    "        test_start_time,\n",
    "        (\n",
    "            str(subset_prop)\n",
    "            + '_' + str(dropout_rate) \n",
    "            + '_' + str(length_scale)\n",
    "            + '_' + str(tau)),\n",
    "    )\n",
    "    \n",
    "    os.makedirs(test_results_path, exist_ok=True)\n",
    "    \n",
    "    test_results_accuracy_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_mc.txt\"\n",
    "    )\n",
    "    \n",
    "    test_results_accuracy_non_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_non_mc.txt\"\n",
    "    )\n",
    "\n",
    "    # Prepare new subset of the original dataset\n",
    "    subset = datasets.CIFAR10(\n",
    "        root='./datasets_files', limit_size=subset_prop, transform=transform, download=True)\n",
    "\n",
    "    # Determine sizes of training and testing set\n",
    "    train_size = int(train_prop * len(subset))\n",
    "    test_size = len(subset) - train_size\n",
    "    \n",
    "    # Print the size of the subset\n",
    "    print(\"subset size = \" + str(subset.data.shape))\n",
    "    print(\"training set size = %d\" % train_size)\n",
    "    print(\"test set size = %d\" % test_size)\n",
    "    \n",
    "    train, test = random_split(subset, lengths=[train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "    # Prepare network\n",
    "    network = models.SimpleCIFAR10(\n",
    "        dropout_rate=dropout_rate,\n",
    "        dropout_type='bernoulli',\n",
    "    )\n",
    "    \n",
    "    # Send the whole model to the selected torch.device\n",
    "    network.to(torch_device)\n",
    "\n",
    "    # Print the network structure\n",
    "    print(network)\n",
    "    \n",
    "    # Model to train mode\n",
    "    network.train()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    # https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam\n",
    "\n",
    "    # NOTE: Need to set L2 regularization from here\n",
    "    reg_strength = utils.reg_strength(dropout_rate, length_scale, train_size, tau)\n",
    "    print('reg_strength = ' + str(reg_strength))\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        network.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=reg_strength, # L2 regularization\n",
    "    )\n",
    "\n",
    "    print()\n",
    "\n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        \"Starting subset %f, dropout_rate %f, length_scale %f, tau %f\"\n",
    "        % (subset_prop, dropout_rate, length_scale, tau))\n",
    "\n",
    "    # Record training start time (for this split)\n",
    "    tic = time.time()\n",
    "\n",
    "    for epoch in range(n_epoch): # loop over the dataset multiple times\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, targets = data\n",
    "\n",
    "            # Store the batch to torch_device's memory\n",
    "            inputs = inputs.to(torch_device)\n",
    "            targets = targets.to(torch_device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = network(inputs)\n",
    "\n",
    "            loss = objective(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"epoch %d loss = %f\" % (epoch, loss.item()))\n",
    "            \n",
    "    # Record training end time\n",
    "    toc = time.time()\n",
    "\n",
    "    # Report the final loss\n",
    "    print(\"final loss = %f\" % (loss.item()))\n",
    "\n",
    "    \"\"\"\n",
    "    Testing\n",
    "    \"\"\"\n",
    "\n",
    "    # Model to eval mode\n",
    "    network.eval()\n",
    "\n",
    "    for n_predictions in prediction_runs:\n",
    "        print(str(n_predictions) + \" test runs...\")\n",
    "\n",
    "        # Get the test data\n",
    "        test_loader = DataLoader(test, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "        # Record testing start time (for this split)\n",
    "        tic_testing = time.time()\n",
    "\n",
    "        _, mean, metrics = network.predict_dist(test_loader, n_predictions)\n",
    "\n",
    "        # Record testing end time\n",
    "        toc_testing = time.time()\n",
    "\n",
    "        # store additional metrics\n",
    "        if len(metrics) > 0:\n",
    "\n",
    "            for key, value in metrics.items():\n",
    "                print(str(key) + \" = \" + str(value))\n",
    "\n",
    "                if key == 'accuracy_mc':\n",
    "                    with open(test_results_accuracy_mc_path, 'a+') as accuracy_mc_file:\n",
    "                        accuracy_mc_file.write('%d %f \\n' % (n_predictions, value))\n",
    "\n",
    "                elif key == 'accuracy_non_mc':\n",
    "                    with open(test_results_accuracy_non_mc_path, 'a+') as accuracy_non_mc_file:\n",
    "                        accuracy_non_mc_file.write('%d %f \\n' % (n_predictions, value))\n",
    "                print()\n",
    "            \n",
    "    # Report the total training time\n",
    "    print(\"training time = \" + str(toc - tic) + \" seconds\")\n",
    "    \n",
    "    # Report the total testing time\n",
    "    print(\"testing time (last run) = \" + str(toc_testing - tic_testing) + \" seconds\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDxkRM5aVrdf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment_nn_capacity_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
