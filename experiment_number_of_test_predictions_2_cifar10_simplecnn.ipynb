{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bka_bK83VFHh"
   },
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D5NSPs_cJZe"
   },
   "source": [
    "### Random seed / PyTorch / CUDA related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6214,
     "status": "ok",
     "timestamp": 1575020214134,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "pHbfpytEVFHu",
    "outputId": "2a30e001-783e-4de2-f1ce-8211ba20613d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.39.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.3.1)\n",
      "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.4)\n",
      "/content/drive/My Drive/Colab Notebooks/bayesian-dl-experiments\n",
      "datasets_files\t\t\t experiment_nn_capacity_1.ipynb  ronald_bdl\n",
      "experiment_comparison_toy.ipynb  experiment_nn_capacity_2.ipynb  test_results\n",
      "experiment_convergence_1.ipynb\t LICENSE\n",
      "experiment_convergence_2.ipynb\t README.md\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # If there's a package I need to install separately, do it here\n",
    "    !pip install pyro-ppl\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd 'drive/My Drive/Colab Notebooks/bayesian-dl-experiments'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls\n",
    "\n",
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seeds\n",
    "# Based on https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random_seed = 682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eag8z68GGbRu"
   },
   "source": [
    "### Third party libraries (NumPy, PyTorch, Pyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6773,
     "status": "ok",
     "timestamp": 1575020214716,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "dHn9F3GMGbR1",
    "outputId": "97858cb4-449b-4120-901e-3eff9141c218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Version: 1.17.4\n",
      "PyTorch Version: 1.3.1\n",
      "Pyro Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Third party libraries import\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print version information\n",
    "print(\"NumPy Version: \" + np.__version__)\n",
    "print(\"PyTorch Version: \" + torch.__version__)\n",
    "print(\"Pyro Version: \" + pyro.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7006,
     "status": "ok",
     "timestamp": 1575020214967,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "PrhSLV8pGbSB",
    "outputId": "adeab5e6-c83e-4ff0-8b87-e0e885bf0570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 10.1.243\n",
      "cuDNN Version: 7603\n",
      "CUDA Device Name: Tesla P100-PCIE-16GB\n",
      "CUDA Capabilities: (6, 0)\n"
     ]
    }
   ],
   "source": [
    "# More imports...\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader, RandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pyro.infer import SVI, Trace_ELBO, HMC, MCMC\n",
    "\n",
    "# Import model and dataset classes from ronald_bdl\n",
    "from ronald_bdl import models, datasets\n",
    "\n",
    "# pyplot setting\n",
    "%matplotlib inline\n",
    "\n",
    "# torch.device / CUDA Setup\n",
    "use_cuda = True\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "            \n",
    "    # Disable 'benchmark' mode\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    use_pin_memory = True # Faster Host to GPU copies with page-locked memory\n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))    \n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIFRoH3AcJZn"
   },
   "source": [
    "### Variable settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NBuZI0cHnHy"
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqiyKf1PHtR6"
   },
   "outputs": [],
   "source": [
    "# CIFAR10 data transformation setting\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Set the proportion of the original dataset to be available as a whole\n",
    "subset_proportions = [0.01, 0.1, 1]\n",
    "\n",
    "# Proportion of the dataset (after getting the subset) to be used for training\n",
    "train_prop = 0.9\n",
    "\n",
    "# Number of dataset splits\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBszD08sH4c4"
   },
   "source": [
    "#### NN settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6Z8-H-YH8kQ"
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "dropout_rates = [0.1, 0.3, 0.5]\n",
    "\n",
    "# Regularization strengths\n",
    "reg_strengths = [0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0U0QTMyEIVDr"
   },
   "source": [
    "#### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_pzGq1_cJZp"
   },
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "n_epoch = 1000\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data batch sizes\n",
    "n_training_batch = 512\n",
    "\n",
    "# Number of test predictions (for each data point)\n",
    "prediction_runs = [3, 10, 30, 100, 300, 1000]\n",
    "\n",
    "# Cross Entropy to minimize\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test start time\n",
    "test_start_time = datetime.datetime.today().strftime('%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PpzPMI8VFKE"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1585801,
     "status": "ok",
     "timestamp": 1575025822894,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "m4kavCiTVFKf",
    "outputId": "78d6dac6-74f5-4ea5-d45b-347f4b118049",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "subset size = (5000, 32, 32, 3)\n",
      "training set size = 4000\n",
      "test set size = 1000\n",
      "SimpleCIFAR10MCDropout(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc1_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (fc3_dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Starting subset 0.100000, dropout_rate 0.500000, reg_strength 0.000500\n",
      "epoch 0 loss = 2.272900\n",
      "epoch 1 loss = 2.193749\n",
      "epoch 2 loss = 2.207439\n",
      "epoch 3 loss = 2.071395\n",
      "epoch 4 loss = 2.066001\n",
      "epoch 5 loss = 2.034516\n",
      "epoch 6 loss = 1.971157\n",
      "epoch 7 loss = 1.944085\n",
      "epoch 8 loss = 1.891730\n",
      "epoch 9 loss = 1.938705\n",
      "epoch 10 loss = 1.895282\n",
      "epoch 11 loss = 1.971985\n",
      "epoch 12 loss = 1.926789\n",
      "epoch 13 loss = 1.868439\n",
      "epoch 14 loss = 1.866822\n",
      "epoch 15 loss = 1.880656\n",
      "epoch 16 loss = 1.690618\n",
      "epoch 17 loss = 1.884518\n",
      "epoch 18 loss = 1.841704\n",
      "epoch 19 loss = 1.783598\n",
      "epoch 20 loss = 1.811441\n",
      "epoch 21 loss = 1.900739\n",
      "epoch 22 loss = 1.704467\n",
      "epoch 23 loss = 1.657892\n",
      "epoch 24 loss = 1.707794\n",
      "epoch 25 loss = 1.505000\n",
      "epoch 26 loss = 1.626471\n",
      "epoch 27 loss = 1.814498\n",
      "epoch 28 loss = 1.776951\n",
      "epoch 29 loss = 1.525468\n",
      "epoch 30 loss = 1.861239\n",
      "epoch 31 loss = 1.690562\n",
      "epoch 32 loss = 1.622453\n",
      "epoch 33 loss = 1.438437\n",
      "epoch 34 loss = 1.403433\n",
      "epoch 35 loss = 1.738857\n",
      "epoch 36 loss = 1.326277\n",
      "epoch 37 loss = 1.582783\n",
      "epoch 38 loss = 1.625726\n",
      "epoch 39 loss = 1.645103\n",
      "epoch 40 loss = 1.415444\n",
      "epoch 41 loss = 1.471359\n",
      "epoch 42 loss = 1.504798\n",
      "epoch 43 loss = 1.116921\n",
      "epoch 44 loss = 1.478405\n",
      "epoch 45 loss = 1.406903\n",
      "epoch 46 loss = 1.445383\n",
      "epoch 47 loss = 1.446048\n",
      "epoch 48 loss = 1.706417\n",
      "epoch 49 loss = 1.302468\n",
      "epoch 50 loss = 1.439674\n",
      "epoch 51 loss = 1.197845\n",
      "epoch 52 loss = 1.419484\n",
      "epoch 53 loss = 1.411442\n",
      "epoch 54 loss = 1.463616\n",
      "epoch 55 loss = 1.410942\n",
      "epoch 56 loss = 1.474338\n",
      "epoch 57 loss = 1.443141\n",
      "epoch 58 loss = 1.610452\n",
      "epoch 59 loss = 1.554991\n",
      "epoch 60 loss = 1.419343\n",
      "epoch 61 loss = 1.423034\n",
      "epoch 62 loss = 1.691601\n",
      "epoch 63 loss = 1.405928\n",
      "epoch 64 loss = 1.554196\n",
      "epoch 65 loss = 1.564323\n",
      "epoch 66 loss = 1.461652\n",
      "epoch 67 loss = 1.072113\n",
      "epoch 68 loss = 1.239925\n",
      "epoch 69 loss = 1.268079\n",
      "epoch 70 loss = 1.229485\n",
      "epoch 71 loss = 1.470781\n",
      "epoch 72 loss = 1.521734\n",
      "epoch 73 loss = 1.322196\n",
      "epoch 74 loss = 1.327872\n",
      "epoch 75 loss = 1.190025\n",
      "epoch 76 loss = 1.215916\n",
      "epoch 77 loss = 1.337837\n",
      "epoch 78 loss = 1.524863\n",
      "epoch 79 loss = 1.364001\n",
      "epoch 80 loss = 1.195687\n",
      "epoch 81 loss = 1.348230\n",
      "epoch 82 loss = 1.187179\n",
      "epoch 83 loss = 1.308694\n",
      "epoch 84 loss = 1.090860\n",
      "epoch 85 loss = 1.188096\n",
      "epoch 86 loss = 1.126923\n",
      "epoch 87 loss = 1.292353\n",
      "epoch 88 loss = 1.124580\n",
      "epoch 89 loss = 1.129939\n",
      "epoch 90 loss = 1.275461\n",
      "epoch 91 loss = 1.340331\n",
      "epoch 92 loss = 1.231716\n",
      "epoch 93 loss = 1.127222\n",
      "epoch 94 loss = 1.206533\n",
      "epoch 95 loss = 1.335478\n",
      "epoch 96 loss = 1.100996\n",
      "epoch 97 loss = 1.379314\n",
      "epoch 98 loss = 1.260530\n",
      "epoch 99 loss = 1.262901\n",
      "epoch 100 loss = 0.927016\n",
      "epoch 101 loss = 1.577642\n",
      "epoch 102 loss = 1.146415\n",
      "epoch 103 loss = 1.476432\n",
      "epoch 104 loss = 1.151234\n",
      "epoch 105 loss = 1.074767\n",
      "epoch 106 loss = 1.358415\n",
      "epoch 107 loss = 1.341529\n",
      "epoch 108 loss = 1.117269\n",
      "epoch 109 loss = 1.452867\n",
      "epoch 110 loss = 1.138714\n",
      "epoch 111 loss = 1.398340\n",
      "epoch 112 loss = 1.277768\n",
      "epoch 113 loss = 1.201633\n",
      "epoch 114 loss = 1.177467\n",
      "epoch 115 loss = 1.121834\n",
      "epoch 116 loss = 1.240626\n",
      "epoch 117 loss = 1.106257\n",
      "epoch 118 loss = 1.319108\n",
      "epoch 119 loss = 1.115914\n",
      "epoch 120 loss = 1.153325\n",
      "epoch 121 loss = 1.035436\n",
      "epoch 122 loss = 1.101491\n",
      "epoch 123 loss = 0.907239\n",
      "epoch 124 loss = 1.256718\n",
      "epoch 125 loss = 1.248209\n",
      "epoch 126 loss = 0.956875\n",
      "epoch 127 loss = 1.030508\n",
      "epoch 128 loss = 1.263317\n",
      "epoch 129 loss = 1.265318\n",
      "epoch 130 loss = 0.946538\n",
      "epoch 131 loss = 1.134975\n",
      "epoch 132 loss = 1.027315\n",
      "epoch 133 loss = 1.112054\n",
      "epoch 134 loss = 1.132790\n",
      "epoch 135 loss = 1.122884\n",
      "epoch 136 loss = 1.317979\n",
      "epoch 137 loss = 0.973224\n",
      "epoch 138 loss = 1.121038\n",
      "epoch 139 loss = 1.152052\n",
      "epoch 140 loss = 1.410811\n",
      "epoch 141 loss = 1.213245\n",
      "epoch 142 loss = 1.136586\n",
      "epoch 143 loss = 1.221759\n",
      "epoch 144 loss = 1.267373\n",
      "epoch 145 loss = 1.155951\n",
      "epoch 146 loss = 1.233190\n",
      "epoch 147 loss = 1.201345\n",
      "epoch 148 loss = 1.727356\n",
      "epoch 149 loss = 0.868250\n",
      "epoch 150 loss = 1.408306\n",
      "epoch 151 loss = 1.280110\n",
      "epoch 152 loss = 1.092803\n",
      "epoch 153 loss = 0.970861\n",
      "epoch 154 loss = 1.203436\n",
      "epoch 155 loss = 1.331463\n",
      "epoch 156 loss = 0.920318\n",
      "epoch 157 loss = 1.443881\n",
      "epoch 158 loss = 0.998237\n",
      "epoch 159 loss = 1.151882\n",
      "epoch 160 loss = 0.936442\n",
      "epoch 161 loss = 0.966499\n",
      "epoch 162 loss = 1.121956\n",
      "epoch 163 loss = 0.884788\n",
      "epoch 164 loss = 1.089657\n",
      "epoch 165 loss = 1.132781\n",
      "epoch 166 loss = 1.497673\n",
      "epoch 167 loss = 1.164163\n",
      "epoch 168 loss = 0.958619\n",
      "epoch 169 loss = 1.392533\n",
      "epoch 170 loss = 1.189114\n",
      "epoch 171 loss = 1.002513\n",
      "epoch 172 loss = 0.910328\n",
      "epoch 173 loss = 0.992426\n",
      "epoch 174 loss = 0.956299\n",
      "epoch 175 loss = 1.317610\n",
      "epoch 176 loss = 0.880830\n",
      "epoch 177 loss = 0.995030\n",
      "epoch 178 loss = 1.051383\n",
      "epoch 179 loss = 1.143163\n",
      "epoch 180 loss = 0.952024\n",
      "epoch 181 loss = 1.198917\n",
      "epoch 182 loss = 1.072287\n",
      "epoch 183 loss = 1.172708\n",
      "epoch 184 loss = 1.139297\n",
      "epoch 185 loss = 1.195488\n",
      "epoch 186 loss = 1.352466\n",
      "epoch 187 loss = 1.313221\n",
      "epoch 188 loss = 1.017721\n",
      "epoch 189 loss = 0.918111\n",
      "epoch 190 loss = 1.664392\n",
      "epoch 191 loss = 1.146300\n",
      "epoch 192 loss = 1.193959\n",
      "epoch 193 loss = 1.487315\n",
      "epoch 194 loss = 0.932759\n",
      "epoch 195 loss = 1.117121\n",
      "epoch 196 loss = 1.133890\n",
      "epoch 197 loss = 1.187659\n",
      "epoch 198 loss = 1.182583\n",
      "epoch 199 loss = 1.248132\n",
      "epoch 200 loss = 1.116737\n",
      "epoch 201 loss = 1.222222\n",
      "epoch 202 loss = 1.208166\n",
      "epoch 203 loss = 1.062770\n",
      "epoch 204 loss = 1.236688\n",
      "epoch 205 loss = 1.032622\n",
      "epoch 206 loss = 1.192424\n",
      "epoch 207 loss = 0.951969\n",
      "epoch 208 loss = 1.061359\n",
      "epoch 209 loss = 1.089802\n",
      "epoch 210 loss = 1.338625\n",
      "epoch 211 loss = 1.331136\n",
      "epoch 212 loss = 1.253581\n",
      "epoch 213 loss = 0.956123\n",
      "epoch 214 loss = 1.144366\n",
      "epoch 215 loss = 0.873747\n",
      "epoch 216 loss = 0.742822\n",
      "epoch 217 loss = 1.161392\n",
      "epoch 218 loss = 0.800651\n",
      "epoch 219 loss = 0.987001\n",
      "epoch 220 loss = 1.172191\n",
      "epoch 221 loss = 1.352757\n",
      "epoch 222 loss = 1.224728\n",
      "epoch 223 loss = 0.901357\n",
      "epoch 224 loss = 0.985449\n",
      "epoch 225 loss = 1.169556\n",
      "epoch 226 loss = 1.318198\n",
      "epoch 227 loss = 1.195318\n",
      "epoch 228 loss = 0.744321\n",
      "epoch 229 loss = 1.123572\n",
      "epoch 230 loss = 0.996084\n",
      "epoch 231 loss = 1.478231\n",
      "epoch 232 loss = 1.097847\n",
      "epoch 233 loss = 1.142164\n",
      "epoch 234 loss = 0.997783\n",
      "epoch 235 loss = 1.164679\n",
      "epoch 236 loss = 0.935135\n",
      "epoch 237 loss = 1.095233\n",
      "epoch 238 loss = 1.200605\n",
      "epoch 239 loss = 1.016745\n",
      "epoch 240 loss = 1.123809\n",
      "epoch 241 loss = 1.063910\n",
      "epoch 242 loss = 1.008422\n",
      "epoch 243 loss = 1.128627\n",
      "epoch 244 loss = 1.197302\n",
      "epoch 245 loss = 1.073457\n",
      "epoch 246 loss = 1.188794\n",
      "epoch 247 loss = 1.069799\n",
      "epoch 248 loss = 0.961797\n",
      "epoch 249 loss = 0.955115\n",
      "epoch 250 loss = 0.954713\n",
      "epoch 251 loss = 1.000677\n",
      "epoch 252 loss = 0.827019\n",
      "epoch 253 loss = 1.119187\n",
      "epoch 254 loss = 1.026485\n",
      "epoch 255 loss = 0.871143\n",
      "epoch 256 loss = 1.144858\n",
      "epoch 257 loss = 1.271265\n",
      "epoch 258 loss = 1.258696\n",
      "epoch 259 loss = 1.063790\n",
      "epoch 260 loss = 1.084654\n",
      "epoch 261 loss = 1.171051\n",
      "epoch 262 loss = 1.234419\n",
      "epoch 263 loss = 1.306174\n",
      "epoch 264 loss = 1.050932\n",
      "epoch 265 loss = 1.164094\n",
      "epoch 266 loss = 1.079348\n",
      "epoch 267 loss = 1.341586\n",
      "epoch 268 loss = 1.383670\n",
      "epoch 269 loss = 1.074204\n",
      "epoch 270 loss = 1.071653\n",
      "epoch 271 loss = 1.153575\n",
      "epoch 272 loss = 0.928241\n",
      "epoch 273 loss = 0.878297\n",
      "epoch 274 loss = 1.279949\n",
      "epoch 275 loss = 1.098162\n",
      "epoch 276 loss = 1.242289\n",
      "epoch 277 loss = 1.143280\n",
      "epoch 278 loss = 1.263836\n",
      "epoch 279 loss = 0.941294\n",
      "epoch 280 loss = 1.146457\n",
      "epoch 281 loss = 1.074013\n",
      "epoch 282 loss = 0.989267\n",
      "epoch 283 loss = 1.331761\n",
      "epoch 284 loss = 0.881336\n",
      "epoch 285 loss = 0.952048\n",
      "epoch 286 loss = 0.955153\n",
      "epoch 287 loss = 0.792121\n",
      "epoch 288 loss = 1.127167\n",
      "epoch 289 loss = 0.993690\n",
      "epoch 290 loss = 1.072062\n",
      "epoch 291 loss = 1.178421\n",
      "epoch 292 loss = 1.057541\n",
      "epoch 293 loss = 1.053542\n",
      "epoch 294 loss = 1.167521\n",
      "epoch 295 loss = 1.274464\n",
      "epoch 296 loss = 0.701718\n",
      "epoch 297 loss = 1.100757\n",
      "epoch 298 loss = 1.098934\n",
      "epoch 299 loss = 0.942495\n",
      "epoch 300 loss = 1.061399\n",
      "epoch 301 loss = 1.238658\n",
      "epoch 302 loss = 0.936324\n",
      "epoch 303 loss = 0.898074\n",
      "epoch 304 loss = 1.156389\n",
      "epoch 305 loss = 1.216673\n",
      "epoch 306 loss = 0.859727\n",
      "epoch 307 loss = 0.885844\n",
      "epoch 308 loss = 1.305638\n",
      "epoch 309 loss = 1.163891\n",
      "epoch 310 loss = 1.254013\n",
      "epoch 311 loss = 1.090752\n",
      "epoch 312 loss = 1.071404\n",
      "epoch 313 loss = 1.174902\n",
      "epoch 314 loss = 1.220307\n",
      "epoch 315 loss = 1.106083\n",
      "epoch 316 loss = 1.018040\n",
      "epoch 317 loss = 1.051528\n",
      "epoch 318 loss = 1.042647\n",
      "epoch 319 loss = 1.231828\n",
      "epoch 320 loss = 1.253792\n",
      "epoch 321 loss = 1.001370\n",
      "epoch 322 loss = 1.231911\n",
      "epoch 323 loss = 1.127633\n",
      "epoch 324 loss = 1.030128\n",
      "epoch 325 loss = 0.848764\n",
      "epoch 326 loss = 0.988460\n",
      "epoch 327 loss = 0.956491\n",
      "epoch 328 loss = 1.017695\n",
      "epoch 329 loss = 1.374524\n",
      "epoch 330 loss = 1.143186\n",
      "epoch 331 loss = 0.937958\n",
      "epoch 332 loss = 1.512578\n",
      "epoch 333 loss = 1.038978\n",
      "epoch 334 loss = 0.893894\n",
      "epoch 335 loss = 1.128268\n",
      "epoch 336 loss = 0.950006\n",
      "epoch 337 loss = 1.170973\n",
      "epoch 338 loss = 0.731674\n",
      "epoch 339 loss = 1.144277\n",
      "epoch 340 loss = 1.144256\n",
      "epoch 341 loss = 1.075834\n",
      "epoch 342 loss = 1.075160\n",
      "epoch 343 loss = 1.134105\n",
      "epoch 344 loss = 0.867244\n",
      "epoch 345 loss = 1.061933\n",
      "epoch 346 loss = 1.068560\n",
      "epoch 347 loss = 1.238973\n",
      "epoch 348 loss = 1.316194\n",
      "epoch 349 loss = 1.151084\n",
      "epoch 350 loss = 0.889518\n",
      "epoch 351 loss = 1.316191\n",
      "epoch 352 loss = 1.220389\n",
      "epoch 353 loss = 0.736081\n",
      "epoch 354 loss = 0.872435\n",
      "epoch 355 loss = 1.228824\n",
      "epoch 356 loss = 1.179597\n",
      "epoch 357 loss = 1.229384\n",
      "epoch 358 loss = 0.807027\n",
      "epoch 359 loss = 1.211408\n",
      "epoch 360 loss = 1.175965\n",
      "epoch 361 loss = 1.149984\n",
      "epoch 362 loss = 1.189276\n",
      "epoch 363 loss = 1.156161\n",
      "epoch 364 loss = 1.017729\n",
      "epoch 365 loss = 0.883837\n",
      "epoch 366 loss = 0.980488\n",
      "epoch 367 loss = 1.242179\n",
      "epoch 368 loss = 1.445551\n",
      "epoch 369 loss = 1.061714\n",
      "epoch 370 loss = 1.038622\n",
      "epoch 371 loss = 0.803762\n",
      "epoch 372 loss = 1.072688\n",
      "epoch 373 loss = 1.070579\n",
      "epoch 374 loss = 1.113527\n",
      "epoch 375 loss = 1.208096\n",
      "epoch 376 loss = 1.062783\n",
      "epoch 377 loss = 1.033355\n",
      "epoch 378 loss = 0.946865\n",
      "epoch 379 loss = 1.182579\n",
      "epoch 380 loss = 1.377041\n",
      "epoch 381 loss = 0.995635\n",
      "epoch 382 loss = 0.815222\n",
      "epoch 383 loss = 1.210833\n",
      "epoch 384 loss = 1.225026\n",
      "epoch 385 loss = 1.246030\n",
      "epoch 386 loss = 1.254816\n",
      "epoch 387 loss = 1.221711\n",
      "epoch 388 loss = 0.860383\n",
      "epoch 389 loss = 1.006445\n",
      "epoch 390 loss = 1.194358\n",
      "epoch 391 loss = 1.303374\n",
      "epoch 392 loss = 0.975219\n",
      "epoch 393 loss = 1.090511\n",
      "epoch 394 loss = 0.818052\n",
      "epoch 395 loss = 1.046915\n",
      "epoch 396 loss = 1.284786\n",
      "epoch 397 loss = 0.772966\n",
      "epoch 398 loss = 0.852252\n",
      "epoch 399 loss = 1.043938\n",
      "epoch 400 loss = 1.245505\n",
      "epoch 401 loss = 1.065109\n",
      "epoch 402 loss = 1.099727\n",
      "epoch 403 loss = 1.177223\n",
      "epoch 404 loss = 1.422487\n",
      "epoch 405 loss = 0.891815\n",
      "epoch 406 loss = 1.025428\n",
      "epoch 407 loss = 0.894440\n",
      "epoch 408 loss = 1.180767\n",
      "epoch 409 loss = 0.840540\n",
      "epoch 410 loss = 1.101402\n",
      "epoch 411 loss = 1.077443\n",
      "epoch 412 loss = 0.936943\n",
      "epoch 413 loss = 1.207563\n",
      "epoch 414 loss = 0.991568\n",
      "epoch 415 loss = 0.835420\n",
      "epoch 416 loss = 0.897899\n",
      "epoch 417 loss = 1.184681\n",
      "epoch 418 loss = 1.411689\n",
      "epoch 419 loss = 1.008132\n",
      "epoch 420 loss = 1.012134\n",
      "epoch 421 loss = 1.019218\n",
      "epoch 422 loss = 1.066699\n",
      "epoch 423 loss = 0.974659\n",
      "epoch 424 loss = 0.920338\n",
      "epoch 425 loss = 1.252162\n",
      "epoch 426 loss = 0.860267\n",
      "epoch 427 loss = 1.133667\n",
      "epoch 428 loss = 1.096264\n",
      "epoch 429 loss = 1.020028\n",
      "epoch 430 loss = 1.244076\n",
      "epoch 431 loss = 1.329259\n",
      "epoch 432 loss = 1.016865\n",
      "epoch 433 loss = 0.991592\n",
      "epoch 434 loss = 1.161341\n",
      "epoch 435 loss = 1.072191\n",
      "epoch 436 loss = 1.015419\n",
      "epoch 437 loss = 1.227735\n",
      "epoch 438 loss = 0.955886\n",
      "epoch 439 loss = 0.988038\n",
      "epoch 440 loss = 0.884047\n",
      "epoch 441 loss = 0.933874\n",
      "epoch 442 loss = 1.158523\n",
      "epoch 443 loss = 1.189902\n",
      "epoch 444 loss = 1.154920\n",
      "epoch 445 loss = 1.126410\n",
      "epoch 446 loss = 1.294263\n",
      "epoch 447 loss = 0.985097\n",
      "epoch 448 loss = 1.219198\n",
      "epoch 449 loss = 0.941442\n",
      "epoch 450 loss = 0.928465\n",
      "epoch 451 loss = 1.224741\n",
      "epoch 452 loss = 0.845299\n",
      "epoch 453 loss = 0.964786\n",
      "epoch 454 loss = 1.062766\n",
      "epoch 455 loss = 1.078759\n",
      "epoch 456 loss = 1.040582\n",
      "epoch 457 loss = 1.208220\n",
      "epoch 458 loss = 0.906194\n",
      "epoch 459 loss = 0.893300\n",
      "epoch 460 loss = 0.765047\n",
      "epoch 461 loss = 1.133884\n",
      "epoch 462 loss = 0.907814\n",
      "epoch 463 loss = 1.097194\n",
      "epoch 464 loss = 1.152472\n",
      "epoch 465 loss = 0.958749\n",
      "epoch 466 loss = 1.159032\n",
      "epoch 467 loss = 1.042906\n",
      "epoch 468 loss = 1.230083\n",
      "epoch 469 loss = 0.691535\n",
      "epoch 470 loss = 0.997862\n",
      "epoch 471 loss = 1.049125\n",
      "epoch 472 loss = 0.971366\n",
      "epoch 473 loss = 0.872909\n",
      "epoch 474 loss = 0.933358\n",
      "epoch 475 loss = 1.225791\n",
      "epoch 476 loss = 0.979862\n",
      "epoch 477 loss = 0.997815\n",
      "epoch 478 loss = 1.077323\n",
      "epoch 479 loss = 0.902479\n",
      "epoch 480 loss = 0.945399\n",
      "epoch 481 loss = 1.223440\n",
      "epoch 482 loss = 0.941291\n",
      "epoch 483 loss = 0.842546\n",
      "epoch 484 loss = 1.028486\n",
      "epoch 485 loss = 0.926568\n",
      "epoch 486 loss = 0.991779\n",
      "epoch 487 loss = 1.167573\n",
      "epoch 488 loss = 1.235077\n",
      "epoch 489 loss = 1.036982\n",
      "epoch 490 loss = 1.156707\n",
      "epoch 491 loss = 0.964931\n",
      "epoch 492 loss = 1.115069\n",
      "epoch 493 loss = 0.977118\n",
      "epoch 494 loss = 1.015890\n",
      "epoch 495 loss = 0.926436\n",
      "epoch 496 loss = 0.810574\n",
      "epoch 497 loss = 1.012746\n",
      "epoch 498 loss = 0.979764\n",
      "epoch 499 loss = 1.019951\n",
      "final loss = 1.019951\n",
      "3 test runs...\n",
      "accuracy_mc = tensor(0.5230, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.5352, device='cuda:0')\n",
      "\n",
      "10 test runs...\n",
      "accuracy_mc = tensor(0.5180, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.5352, device='cuda:0')\n",
      "\n",
      "30 test runs...\n",
      "accuracy_mc = tensor(0.5827, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.5352, device='cuda:0')\n",
      "\n",
      "100 test runs...\n",
      "accuracy_mc = tensor(0.5779, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.5352, device='cuda:0')\n",
      "\n",
      "300 test runs...\n",
      "accuracy_mc = tensor(0.5798, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.5352, device='cuda:0')\n",
      "\n",
      "1000 test runs...\n",
      "accuracy_mc = tensor(0.5775, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.5352, device='cuda:0')\n",
      "\n",
      "training time = 353.86936807632446 seconds\n",
      "testing time (last run) = 3.6854450702667236 seconds\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (25000, 32, 32, 3)\n",
      "training set size = 20000\n",
      "test set size = 5000\n",
      "SimpleCIFAR10MCDropout(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc1_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (fc3_dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Starting subset 0.500000, dropout_rate 0.500000, reg_strength 0.000500\n",
      "epoch 0 loss = 2.137607\n",
      "epoch 1 loss = 1.878527\n",
      "epoch 2 loss = 1.954206\n",
      "epoch 3 loss = 1.917852\n",
      "epoch 4 loss = 1.850321\n",
      "epoch 5 loss = 1.895373\n",
      "epoch 6 loss = 1.716431\n",
      "epoch 7 loss = 1.765258\n",
      "epoch 8 loss = 1.660350\n",
      "epoch 9 loss = 1.653816\n",
      "epoch 10 loss = 1.635900\n",
      "epoch 11 loss = 1.615405\n",
      "epoch 12 loss = 1.696007\n",
      "epoch 13 loss = 1.451282\n",
      "epoch 14 loss = 1.784889\n",
      "epoch 15 loss = 1.502056\n",
      "epoch 16 loss = 1.659781\n",
      "epoch 17 loss = 1.675352\n",
      "epoch 18 loss = 1.610381\n",
      "epoch 19 loss = 1.645438\n",
      "epoch 20 loss = 1.577591\n",
      "epoch 21 loss = 1.480705\n",
      "epoch 22 loss = 1.621024\n",
      "epoch 23 loss = 1.329638\n",
      "epoch 24 loss = 1.693184\n",
      "epoch 25 loss = 1.512421\n",
      "epoch 26 loss = 1.396713\n",
      "epoch 27 loss = 1.359386\n",
      "epoch 28 loss = 1.408798\n",
      "epoch 29 loss = 1.644569\n",
      "epoch 30 loss = 1.599443\n",
      "epoch 31 loss = 1.456044\n",
      "epoch 32 loss = 1.389675\n",
      "epoch 33 loss = 1.450837\n",
      "epoch 34 loss = 1.471146\n",
      "epoch 35 loss = 1.417653\n",
      "epoch 36 loss = 1.518886\n",
      "epoch 37 loss = 1.592629\n",
      "epoch 38 loss = 1.291288\n",
      "epoch 39 loss = 1.484204\n",
      "epoch 40 loss = 1.495337\n",
      "epoch 41 loss = 1.684535\n",
      "epoch 42 loss = 1.289043\n",
      "epoch 43 loss = 1.572172\n",
      "epoch 44 loss = 1.499003\n",
      "epoch 45 loss = 1.695363\n",
      "epoch 46 loss = 1.454168\n",
      "epoch 47 loss = 1.267200\n",
      "epoch 48 loss = 1.648880\n",
      "epoch 49 loss = 1.611054\n",
      "epoch 50 loss = 1.561907\n",
      "epoch 51 loss = 1.407519\n",
      "epoch 52 loss = 1.563468\n",
      "epoch 53 loss = 1.420143\n",
      "epoch 54 loss = 1.635090\n",
      "epoch 55 loss = 1.541184\n",
      "epoch 56 loss = 1.173450\n",
      "epoch 57 loss = 1.129547\n",
      "epoch 58 loss = 1.518043\n",
      "epoch 59 loss = 1.368377\n",
      "epoch 60 loss = 1.624140\n",
      "epoch 61 loss = 1.526032\n",
      "epoch 62 loss = 1.405548\n",
      "epoch 63 loss = 1.296631\n",
      "epoch 64 loss = 1.337554\n",
      "epoch 65 loss = 1.520293\n",
      "epoch 66 loss = 1.456546\n",
      "epoch 67 loss = 1.350590\n",
      "epoch 68 loss = 1.318211\n",
      "epoch 69 loss = 1.433504\n",
      "epoch 70 loss = 1.566303\n",
      "epoch 71 loss = 1.572308\n",
      "epoch 72 loss = 1.513350\n",
      "epoch 73 loss = 1.470168\n",
      "epoch 74 loss = 1.316928\n",
      "epoch 75 loss = 1.433256\n",
      "epoch 76 loss = 1.302734\n",
      "epoch 77 loss = 1.580370\n",
      "epoch 78 loss = 1.595045\n",
      "epoch 79 loss = 1.546402\n",
      "epoch 80 loss = 1.617734\n",
      "epoch 81 loss = 1.304225\n",
      "epoch 82 loss = 1.336369\n",
      "epoch 83 loss = 1.587992\n",
      "epoch 84 loss = 1.235008\n",
      "epoch 85 loss = 1.314394\n",
      "epoch 86 loss = 1.502993\n",
      "epoch 87 loss = 1.368201\n",
      "epoch 88 loss = 1.447650\n",
      "epoch 89 loss = 1.322013\n",
      "epoch 90 loss = 1.390520\n",
      "epoch 91 loss = 1.348458\n",
      "epoch 92 loss = 1.399081\n",
      "epoch 93 loss = 1.200473\n",
      "epoch 94 loss = 1.672397\n",
      "epoch 95 loss = 1.340304\n",
      "epoch 96 loss = 1.300924\n",
      "epoch 97 loss = 1.416325\n",
      "epoch 98 loss = 1.416357\n",
      "epoch 99 loss = 1.287896\n",
      "epoch 100 loss = 1.368894\n",
      "epoch 101 loss = 1.256226\n",
      "epoch 102 loss = 1.320600\n",
      "epoch 103 loss = 1.382451\n",
      "epoch 104 loss = 1.373100\n",
      "epoch 105 loss = 1.454641\n",
      "epoch 106 loss = 1.386844\n",
      "epoch 107 loss = 1.399062\n",
      "epoch 108 loss = 1.550300\n",
      "epoch 109 loss = 1.410175\n",
      "epoch 110 loss = 1.621082\n",
      "epoch 111 loss = 1.352320\n",
      "epoch 112 loss = 1.423153\n",
      "epoch 113 loss = 1.551438\n",
      "epoch 114 loss = 1.587504\n",
      "epoch 115 loss = 1.627227\n",
      "epoch 116 loss = 1.556314\n",
      "epoch 117 loss = 1.298681\n",
      "epoch 118 loss = 1.590836\n",
      "epoch 119 loss = 1.432157\n",
      "epoch 120 loss = 1.425472\n",
      "epoch 121 loss = 1.404469\n",
      "epoch 122 loss = 1.540685\n",
      "epoch 123 loss = 1.599246\n",
      "epoch 124 loss = 1.090908\n",
      "epoch 125 loss = 1.493440\n",
      "epoch 126 loss = 1.570009\n",
      "epoch 127 loss = 1.578278\n",
      "epoch 128 loss = 1.472230\n",
      "epoch 129 loss = 1.242668\n",
      "epoch 130 loss = 1.459405\n",
      "epoch 131 loss = 1.463493\n",
      "epoch 132 loss = 1.646384\n",
      "epoch 133 loss = 1.553217\n",
      "epoch 134 loss = 1.105064\n",
      "epoch 135 loss = 1.307994\n",
      "epoch 136 loss = 1.009274\n",
      "epoch 137 loss = 1.092758\n",
      "epoch 138 loss = 1.297281\n",
      "epoch 139 loss = 1.468581\n",
      "epoch 140 loss = 1.180000\n",
      "epoch 141 loss = 1.608110\n",
      "epoch 142 loss = 1.509605\n",
      "epoch 143 loss = 1.402117\n",
      "epoch 144 loss = 1.382223\n",
      "epoch 145 loss = 1.230516\n",
      "epoch 146 loss = 1.362997\n",
      "epoch 147 loss = 1.513102\n",
      "epoch 148 loss = 1.411522\n",
      "epoch 149 loss = 1.388923\n",
      "epoch 150 loss = 1.611962\n",
      "epoch 151 loss = 1.245195\n",
      "epoch 152 loss = 1.287856\n",
      "epoch 153 loss = 1.512068\n",
      "epoch 154 loss = 1.241343\n",
      "epoch 155 loss = 1.548169\n",
      "epoch 156 loss = 1.285775\n",
      "epoch 157 loss = 1.205948\n",
      "epoch 158 loss = 1.415465\n",
      "epoch 159 loss = 1.211727\n",
      "epoch 160 loss = 1.676003\n",
      "epoch 161 loss = 1.654246\n",
      "epoch 162 loss = 1.458905\n",
      "epoch 163 loss = 1.208813\n",
      "epoch 164 loss = 1.479290\n",
      "epoch 165 loss = 1.239573\n",
      "epoch 166 loss = 1.159658\n",
      "epoch 167 loss = 1.426361\n",
      "epoch 168 loss = 1.561096\n",
      "epoch 169 loss = 1.333335\n",
      "epoch 170 loss = 1.342128\n",
      "epoch 171 loss = 1.186009\n",
      "epoch 172 loss = 1.304440\n",
      "epoch 173 loss = 1.340115\n",
      "epoch 174 loss = 1.624135\n",
      "epoch 175 loss = 1.357020\n",
      "epoch 176 loss = 1.351689\n",
      "epoch 177 loss = 1.454568\n",
      "epoch 178 loss = 1.384092\n",
      "epoch 179 loss = 1.245146\n",
      "epoch 180 loss = 1.263455\n",
      "epoch 181 loss = 1.488493\n",
      "epoch 182 loss = 1.112045\n",
      "epoch 183 loss = 1.414012\n",
      "epoch 184 loss = 1.188645\n",
      "epoch 185 loss = 1.613866\n",
      "epoch 186 loss = 1.307171\n",
      "epoch 187 loss = 1.278729\n",
      "epoch 188 loss = 1.614531\n",
      "epoch 189 loss = 1.454706\n",
      "epoch 190 loss = 1.275869\n",
      "epoch 191 loss = 0.972778\n",
      "epoch 192 loss = 1.519869\n",
      "epoch 193 loss = 1.302953\n",
      "epoch 194 loss = 1.557878\n",
      "epoch 195 loss = 1.170660\n",
      "epoch 196 loss = 1.323791\n",
      "epoch 197 loss = 1.352652\n",
      "epoch 198 loss = 1.577727\n",
      "epoch 199 loss = 1.253162\n",
      "epoch 200 loss = 1.390600\n",
      "epoch 201 loss = 1.315298\n",
      "epoch 202 loss = 1.345540\n",
      "epoch 203 loss = 1.393141\n",
      "epoch 204 loss = 1.504299\n",
      "epoch 205 loss = 1.160836\n",
      "epoch 206 loss = 1.356722\n",
      "epoch 207 loss = 1.329282\n",
      "epoch 208 loss = 1.294135\n",
      "epoch 209 loss = 1.526856\n",
      "epoch 210 loss = 1.192353\n",
      "epoch 211 loss = 1.644204\n",
      "epoch 212 loss = 1.460107\n",
      "epoch 213 loss = 1.206210\n",
      "epoch 214 loss = 1.493608\n",
      "epoch 215 loss = 1.430925\n",
      "epoch 216 loss = 1.378811\n",
      "epoch 217 loss = 1.471777\n",
      "epoch 218 loss = 1.433593\n",
      "epoch 219 loss = 1.564863\n",
      "epoch 220 loss = 1.405491\n",
      "epoch 221 loss = 1.350361\n",
      "epoch 222 loss = 1.346321\n",
      "epoch 223 loss = 1.318519\n",
      "epoch 224 loss = 1.503538\n",
      "epoch 225 loss = 1.268908\n",
      "epoch 226 loss = 1.611781\n",
      "epoch 227 loss = 1.574057\n",
      "epoch 228 loss = 1.342351\n",
      "epoch 229 loss = 1.225254\n",
      "epoch 230 loss = 1.639475\n",
      "epoch 231 loss = 1.229913\n",
      "epoch 232 loss = 1.329935\n",
      "epoch 233 loss = 1.232983\n",
      "epoch 234 loss = 1.257259\n",
      "epoch 235 loss = 1.242845\n",
      "epoch 236 loss = 1.349036\n",
      "epoch 237 loss = 1.436947\n",
      "epoch 238 loss = 1.292120\n",
      "epoch 239 loss = 1.274930\n",
      "epoch 240 loss = 1.308945\n",
      "epoch 241 loss = 1.566262\n",
      "epoch 242 loss = 1.540760\n",
      "epoch 243 loss = 1.571852\n",
      "epoch 244 loss = 1.480623\n",
      "epoch 245 loss = 1.498726\n",
      "epoch 246 loss = 1.380115\n",
      "epoch 247 loss = 1.481330\n",
      "epoch 248 loss = 1.469301\n",
      "epoch 249 loss = 1.459529\n",
      "epoch 250 loss = 1.460829\n",
      "epoch 251 loss = 1.163325\n",
      "epoch 252 loss = 1.349368\n",
      "epoch 253 loss = 0.687517\n",
      "epoch 254 loss = 1.332862\n",
      "epoch 255 loss = 1.471752\n",
      "epoch 256 loss = 1.158007\n",
      "epoch 257 loss = 1.211850\n",
      "epoch 258 loss = 1.303138\n",
      "epoch 259 loss = 1.112089\n",
      "epoch 260 loss = 1.558703\n",
      "epoch 261 loss = 1.287645\n",
      "epoch 262 loss = 1.276083\n",
      "epoch 263 loss = 1.341416\n",
      "epoch 264 loss = 1.251100\n",
      "epoch 265 loss = 1.532947\n",
      "epoch 266 loss = 1.133309\n",
      "epoch 267 loss = 1.180886\n",
      "epoch 268 loss = 1.603004\n",
      "epoch 269 loss = 1.070106\n",
      "epoch 270 loss = 1.489393\n",
      "epoch 271 loss = 1.425463\n",
      "epoch 272 loss = 1.251696\n",
      "epoch 273 loss = 1.282589\n",
      "epoch 274 loss = 1.306433\n",
      "epoch 275 loss = 1.323002\n",
      "epoch 276 loss = 1.115816\n",
      "epoch 277 loss = 1.486897\n",
      "epoch 278 loss = 1.381981\n",
      "epoch 279 loss = 1.563327\n",
      "epoch 280 loss = 1.174590\n",
      "epoch 281 loss = 1.222303\n",
      "epoch 282 loss = 1.395897\n",
      "epoch 283 loss = 1.182021\n",
      "epoch 284 loss = 1.227712\n",
      "epoch 285 loss = 1.068786\n",
      "epoch 286 loss = 1.356476\n",
      "epoch 287 loss = 1.500595\n",
      "epoch 288 loss = 0.963592\n",
      "epoch 289 loss = 1.247479\n",
      "epoch 290 loss = 1.333023\n",
      "epoch 291 loss = 1.230851\n",
      "epoch 292 loss = 1.488575\n",
      "epoch 293 loss = 1.205832\n",
      "epoch 294 loss = 1.535878\n",
      "epoch 295 loss = 1.526485\n",
      "epoch 296 loss = 1.646277\n",
      "epoch 297 loss = 1.080784\n",
      "epoch 298 loss = 1.325824\n",
      "epoch 299 loss = 1.306150\n",
      "epoch 300 loss = 1.714164\n",
      "epoch 301 loss = 1.289755\n",
      "epoch 302 loss = 1.159738\n",
      "epoch 303 loss = 1.374573\n",
      "epoch 304 loss = 1.354855\n",
      "epoch 305 loss = 1.207466\n",
      "epoch 306 loss = 1.255250\n",
      "epoch 307 loss = 1.421654\n",
      "epoch 308 loss = 1.261180\n",
      "epoch 309 loss = 1.178071\n",
      "epoch 310 loss = 1.385610\n",
      "epoch 311 loss = 1.346421\n",
      "epoch 312 loss = 1.222505\n",
      "epoch 313 loss = 1.684053\n",
      "epoch 314 loss = 1.590156\n",
      "epoch 315 loss = 1.498552\n",
      "epoch 316 loss = 1.585620\n",
      "epoch 317 loss = 1.537260\n",
      "epoch 318 loss = 1.152945\n",
      "epoch 319 loss = 1.540806\n",
      "epoch 320 loss = 1.402049\n",
      "epoch 321 loss = 1.422457\n",
      "epoch 322 loss = 1.183650\n",
      "epoch 323 loss = 1.414944\n",
      "epoch 324 loss = 1.216658\n",
      "epoch 325 loss = 1.205583\n",
      "epoch 326 loss = 1.422061\n",
      "epoch 327 loss = 1.300288\n",
      "epoch 328 loss = 1.530506\n",
      "epoch 329 loss = 1.646694\n",
      "epoch 330 loss = 1.455370\n",
      "epoch 331 loss = 1.562256\n",
      "epoch 332 loss = 1.375471\n",
      "epoch 333 loss = 1.365101\n",
      "epoch 334 loss = 1.352332\n",
      "epoch 335 loss = 1.152764\n",
      "epoch 336 loss = 1.531117\n",
      "epoch 337 loss = 1.562501\n",
      "epoch 338 loss = 1.447733\n",
      "epoch 339 loss = 1.510450\n",
      "epoch 340 loss = 1.568218\n",
      "epoch 341 loss = 1.325757\n",
      "epoch 342 loss = 1.293597\n",
      "epoch 343 loss = 1.407530\n",
      "epoch 344 loss = 1.442919\n",
      "epoch 345 loss = 1.030410\n",
      "epoch 346 loss = 1.370123\n",
      "epoch 347 loss = 1.301965\n",
      "epoch 348 loss = 1.480515\n",
      "epoch 349 loss = 1.471428\n",
      "epoch 350 loss = 1.382517\n",
      "epoch 351 loss = 1.432395\n",
      "epoch 352 loss = 1.321716\n",
      "epoch 353 loss = 1.404332\n",
      "epoch 354 loss = 1.390729\n",
      "epoch 355 loss = 1.157484\n",
      "epoch 356 loss = 1.038041\n",
      "epoch 357 loss = 1.402731\n",
      "epoch 358 loss = 1.502771\n",
      "epoch 359 loss = 1.232756\n",
      "epoch 360 loss = 1.421479\n",
      "epoch 361 loss = 1.331239\n",
      "epoch 362 loss = 1.242450\n",
      "epoch 363 loss = 1.279434\n",
      "epoch 364 loss = 1.526388\n",
      "epoch 365 loss = 1.213406\n",
      "epoch 366 loss = 1.334418\n",
      "epoch 367 loss = 1.375707\n",
      "epoch 368 loss = 1.420754\n",
      "epoch 369 loss = 1.046118\n",
      "epoch 370 loss = 1.278934\n",
      "epoch 371 loss = 1.465128\n",
      "epoch 372 loss = 1.490216\n",
      "epoch 373 loss = 1.447549\n",
      "epoch 374 loss = 1.532972\n",
      "epoch 375 loss = 1.251671\n",
      "epoch 376 loss = 1.529788\n",
      "epoch 377 loss = 1.644768\n",
      "epoch 378 loss = 1.243678\n",
      "epoch 379 loss = 1.248576\n",
      "epoch 380 loss = 1.387846\n",
      "epoch 381 loss = 1.613262\n",
      "epoch 382 loss = 1.291057\n",
      "epoch 383 loss = 1.226599\n",
      "epoch 384 loss = 1.165954\n",
      "epoch 385 loss = 1.585178\n",
      "epoch 386 loss = 1.095967\n",
      "epoch 387 loss = 1.338110\n",
      "epoch 388 loss = 1.363148\n",
      "epoch 389 loss = 1.565456\n",
      "epoch 390 loss = 1.102093\n",
      "epoch 391 loss = 1.251790\n",
      "epoch 392 loss = 1.193230\n",
      "epoch 393 loss = 1.136783\n",
      "epoch 394 loss = 1.298123\n",
      "epoch 395 loss = 1.446355\n",
      "epoch 396 loss = 1.424120\n",
      "epoch 397 loss = 1.337822\n",
      "epoch 398 loss = 1.181434\n",
      "epoch 399 loss = 1.041416\n",
      "epoch 400 loss = 0.927991\n",
      "epoch 401 loss = 1.681491\n",
      "epoch 402 loss = 1.482590\n",
      "epoch 403 loss = 1.561599\n",
      "epoch 404 loss = 1.364103\n",
      "epoch 405 loss = 1.283264\n",
      "epoch 406 loss = 1.738646\n",
      "epoch 407 loss = 1.266604\n",
      "epoch 408 loss = 1.283198\n",
      "epoch 409 loss = 1.500846\n",
      "epoch 410 loss = 1.400558\n",
      "epoch 411 loss = 1.114775\n",
      "epoch 412 loss = 1.574514\n",
      "epoch 413 loss = 1.468076\n",
      "epoch 414 loss = 1.471600\n",
      "epoch 415 loss = 1.418842\n",
      "epoch 416 loss = 1.284789\n",
      "epoch 417 loss = 1.511203\n",
      "epoch 418 loss = 1.311555\n",
      "epoch 419 loss = 1.590546\n",
      "epoch 420 loss = 1.127258\n",
      "epoch 421 loss = 1.442127\n",
      "epoch 422 loss = 1.473495\n",
      "epoch 423 loss = 1.196691\n",
      "epoch 424 loss = 1.346400\n",
      "epoch 425 loss = 1.445582\n",
      "epoch 426 loss = 0.966815\n",
      "epoch 427 loss = 1.391861\n",
      "epoch 428 loss = 1.568093\n",
      "epoch 429 loss = 1.277444\n",
      "epoch 430 loss = 1.380143\n",
      "epoch 431 loss = 1.346731\n",
      "epoch 432 loss = 1.680091\n",
      "epoch 433 loss = 1.434404\n",
      "epoch 434 loss = 1.439727\n",
      "epoch 435 loss = 1.342627\n",
      "epoch 436 loss = 1.208427\n",
      "epoch 437 loss = 1.205923\n",
      "epoch 438 loss = 1.187068\n",
      "epoch 439 loss = 1.217771\n",
      "epoch 440 loss = 1.377544\n",
      "epoch 441 loss = 1.499975\n",
      "epoch 442 loss = 1.335025\n",
      "epoch 443 loss = 1.604373\n",
      "epoch 444 loss = 1.509448\n",
      "epoch 445 loss = 1.344885\n",
      "epoch 446 loss = 1.352933\n",
      "epoch 447 loss = 1.155686\n",
      "epoch 448 loss = 1.212043\n",
      "epoch 449 loss = 1.602704\n",
      "epoch 450 loss = 1.218109\n",
      "epoch 451 loss = 1.442612\n",
      "epoch 452 loss = 1.069941\n",
      "epoch 453 loss = 1.122841\n",
      "epoch 454 loss = 1.702993\n",
      "epoch 455 loss = 1.153270\n",
      "epoch 456 loss = 1.456977\n",
      "epoch 457 loss = 1.258899\n",
      "epoch 458 loss = 1.507681\n",
      "epoch 459 loss = 1.229460\n",
      "epoch 460 loss = 1.108963\n",
      "epoch 461 loss = 1.724273\n",
      "epoch 462 loss = 1.254936\n",
      "epoch 463 loss = 1.063833\n",
      "epoch 464 loss = 1.410596\n",
      "epoch 465 loss = 1.398139\n",
      "epoch 466 loss = 1.502664\n",
      "epoch 467 loss = 1.424965\n",
      "epoch 468 loss = 1.128007\n",
      "epoch 469 loss = 1.280650\n",
      "epoch 470 loss = 1.237223\n",
      "epoch 471 loss = 1.595881\n",
      "epoch 472 loss = 1.557954\n",
      "epoch 473 loss = 1.360849\n",
      "epoch 474 loss = 1.229289\n",
      "epoch 475 loss = 1.606385\n",
      "epoch 476 loss = 1.439523\n",
      "epoch 477 loss = 1.056437\n",
      "epoch 478 loss = 1.153536\n",
      "epoch 479 loss = 1.380798\n",
      "epoch 480 loss = 1.198498\n",
      "epoch 481 loss = 1.483092\n",
      "epoch 482 loss = 1.175279\n",
      "epoch 483 loss = 1.263900\n",
      "epoch 484 loss = 1.245906\n",
      "epoch 485 loss = 1.461771\n",
      "epoch 486 loss = 1.202615\n",
      "epoch 487 loss = 1.256526\n",
      "epoch 488 loss = 1.268486\n",
      "epoch 489 loss = 1.567755\n",
      "epoch 490 loss = 1.217601\n",
      "epoch 491 loss = 1.310647\n",
      "epoch 492 loss = 1.272297\n",
      "epoch 493 loss = 1.355421\n",
      "epoch 494 loss = 1.293284\n",
      "epoch 495 loss = 1.280863\n",
      "epoch 496 loss = 1.522750\n",
      "epoch 497 loss = 1.513596\n",
      "epoch 498 loss = 1.295747\n",
      "epoch 499 loss = 1.184292\n",
      "final loss = 1.184292\n",
      "3 test runs...\n",
      "accuracy_mc = tensor(0.7144, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.7592, device='cuda:0')\n",
      "\n",
      "10 test runs...\n",
      "accuracy_mc = tensor(0.6858, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.7592, device='cuda:0')\n",
      "\n",
      "30 test runs...\n",
      "accuracy_mc = tensor(0.6892, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.7592, device='cuda:0')\n",
      "\n",
      "100 test runs...\n",
      "accuracy_mc = tensor(0.6969, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.7592, device='cuda:0')\n",
      "\n",
      "300 test runs...\n",
      "accuracy_mc = tensor(0.6908, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.7592, device='cuda:0')\n",
      "\n",
      "1000 test runs...\n",
      "accuracy_mc = tensor(0.6891, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.7592, device='cuda:0')\n",
      "\n",
      "training time = 1721.2701551914215 seconds\n",
      "testing time (last run) = 17.98946452140808 seconds\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (50000, 32, 32, 3)\n",
      "training set size = 40000\n",
      "test set size = 10000\n",
      "SimpleCIFAR10MCDropout(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc1_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (fc3_dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Starting subset 1.000000, dropout_rate 0.500000, reg_strength 0.000500\n",
      "epoch 0 loss = 1.951360\n",
      "epoch 1 loss = 1.909133\n",
      "epoch 2 loss = 1.959417\n",
      "epoch 3 loss = 1.837639\n",
      "epoch 4 loss = 1.877281\n",
      "epoch 5 loss = 1.940529\n",
      "epoch 6 loss = 1.785727\n",
      "epoch 7 loss = 1.874642\n",
      "epoch 8 loss = 1.842594\n",
      "epoch 9 loss = 1.773869\n",
      "epoch 10 loss = 1.957866\n",
      "epoch 11 loss = 1.830638\n",
      "epoch 12 loss = 1.848161\n",
      "epoch 13 loss = 1.855011\n",
      "epoch 14 loss = 1.772515\n",
      "epoch 15 loss = 1.740872\n",
      "epoch 16 loss = 1.825854\n",
      "epoch 17 loss = 1.993367\n",
      "epoch 18 loss = 1.770196\n",
      "epoch 19 loss = 1.810855\n",
      "epoch 20 loss = 1.747064\n",
      "epoch 21 loss = 1.854930\n",
      "epoch 22 loss = 1.641090\n",
      "epoch 23 loss = 1.852633\n",
      "epoch 24 loss = 1.703482\n",
      "epoch 25 loss = 1.707327\n",
      "epoch 26 loss = 1.757416\n",
      "epoch 27 loss = 1.853808\n",
      "epoch 28 loss = 1.851918\n",
      "epoch 29 loss = 1.801418\n",
      "epoch 30 loss = 1.651425\n",
      "epoch 31 loss = 1.846313\n",
      "epoch 32 loss = 1.742421\n",
      "epoch 33 loss = 1.726124\n",
      "epoch 34 loss = 1.922382\n",
      "epoch 35 loss = 1.707128\n",
      "epoch 36 loss = 1.813111\n",
      "epoch 37 loss = 1.813152\n",
      "epoch 38 loss = 1.794681\n",
      "epoch 39 loss = 1.649543\n",
      "epoch 40 loss = 1.892046\n",
      "epoch 41 loss = 1.554955\n",
      "epoch 42 loss = 1.559106\n",
      "epoch 43 loss = 1.464986\n",
      "epoch 44 loss = 1.619526\n",
      "epoch 45 loss = 1.658364\n",
      "epoch 46 loss = 1.554871\n",
      "epoch 47 loss = 1.639013\n",
      "epoch 48 loss = 1.748698\n",
      "epoch 49 loss = 1.758195\n",
      "epoch 50 loss = 1.753576\n",
      "epoch 51 loss = 2.104468\n",
      "epoch 52 loss = 1.581493\n",
      "epoch 53 loss = 1.571815\n",
      "epoch 54 loss = 1.654362\n",
      "epoch 55 loss = 1.461096\n",
      "epoch 56 loss = 1.713387\n",
      "epoch 57 loss = 1.627687\n",
      "epoch 58 loss = 1.575067\n",
      "epoch 59 loss = 1.775293\n",
      "epoch 60 loss = 1.737532\n",
      "epoch 61 loss = 1.629027\n",
      "epoch 62 loss = 1.589158\n",
      "epoch 63 loss = 1.656463\n",
      "epoch 64 loss = 1.764092\n",
      "epoch 65 loss = 1.543099\n",
      "epoch 66 loss = 1.569137\n",
      "epoch 67 loss = 1.565125\n",
      "epoch 68 loss = 1.882663\n",
      "epoch 69 loss = 1.579471\n",
      "epoch 70 loss = 1.489413\n",
      "epoch 71 loss = 1.634431\n",
      "epoch 72 loss = 1.761281\n",
      "epoch 73 loss = 1.578114\n",
      "epoch 74 loss = 1.526554\n",
      "epoch 75 loss = 1.631382\n",
      "epoch 76 loss = 1.650066\n",
      "epoch 77 loss = 1.592922\n",
      "epoch 78 loss = 1.627411\n",
      "epoch 79 loss = 1.564376\n",
      "epoch 80 loss = 1.611120\n",
      "epoch 81 loss = 1.671931\n",
      "epoch 82 loss = 1.623720\n",
      "epoch 83 loss = 1.682817\n",
      "epoch 84 loss = 1.535685\n",
      "epoch 85 loss = 1.614024\n",
      "epoch 86 loss = 1.438625\n",
      "epoch 87 loss = 1.802542\n",
      "epoch 88 loss = 1.495863\n",
      "epoch 89 loss = 1.538616\n",
      "epoch 90 loss = 1.672009\n",
      "epoch 91 loss = 1.549149\n",
      "epoch 92 loss = 1.582501\n",
      "epoch 93 loss = 1.528868\n",
      "epoch 94 loss = 1.410724\n",
      "epoch 95 loss = 1.706909\n",
      "epoch 96 loss = 1.617169\n",
      "epoch 97 loss = 1.697373\n",
      "epoch 98 loss = 1.519103\n",
      "epoch 99 loss = 1.612230\n",
      "epoch 100 loss = 1.594052\n",
      "epoch 101 loss = 1.520395\n",
      "epoch 102 loss = 1.750928\n",
      "epoch 103 loss = 1.713939\n",
      "epoch 104 loss = 1.836380\n",
      "epoch 105 loss = 1.679887\n",
      "epoch 106 loss = 1.482828\n",
      "epoch 107 loss = 1.521719\n",
      "epoch 108 loss = 1.615926\n",
      "epoch 109 loss = 1.618150\n",
      "epoch 110 loss = 1.584192\n",
      "epoch 111 loss = 1.745563\n",
      "epoch 112 loss = 1.621308\n",
      "epoch 113 loss = 1.638889\n",
      "epoch 114 loss = 1.737737\n",
      "epoch 115 loss = 1.515766\n",
      "epoch 116 loss = 1.417175\n",
      "epoch 117 loss = 1.554423\n",
      "epoch 118 loss = 1.660388\n",
      "epoch 119 loss = 1.482656\n",
      "epoch 120 loss = 1.763607\n",
      "epoch 121 loss = 1.604846\n",
      "epoch 122 loss = 1.715753\n",
      "epoch 123 loss = 1.500958\n",
      "epoch 124 loss = 1.512224\n",
      "epoch 125 loss = 1.608400\n",
      "epoch 126 loss = 1.672642\n",
      "epoch 127 loss = 1.773781\n",
      "epoch 128 loss = 1.685975\n",
      "epoch 129 loss = 1.586244\n",
      "epoch 130 loss = 1.629005\n",
      "epoch 131 loss = 1.479214\n",
      "epoch 132 loss = 1.495602\n",
      "epoch 133 loss = 1.744705\n",
      "epoch 134 loss = 1.591494\n",
      "epoch 135 loss = 1.341004\n",
      "epoch 136 loss = 1.475356\n",
      "epoch 137 loss = 1.729352\n",
      "epoch 138 loss = 1.505474\n",
      "epoch 139 loss = 1.447321\n",
      "epoch 140 loss = 1.686684\n",
      "epoch 141 loss = 1.653984\n",
      "epoch 142 loss = 1.565664\n",
      "epoch 143 loss = 1.538290\n",
      "epoch 144 loss = 1.618939\n",
      "epoch 145 loss = 1.698557\n",
      "epoch 146 loss = 1.744748\n",
      "epoch 147 loss = 1.580963\n",
      "epoch 148 loss = 1.516824\n",
      "epoch 149 loss = 1.544613\n",
      "epoch 150 loss = 1.555527\n",
      "epoch 151 loss = 1.641626\n",
      "epoch 152 loss = 1.585644\n",
      "epoch 153 loss = 1.368227\n",
      "epoch 154 loss = 1.714136\n",
      "epoch 155 loss = 1.730104\n",
      "epoch 156 loss = 1.698046\n",
      "epoch 157 loss = 1.577002\n",
      "epoch 158 loss = 1.504629\n",
      "epoch 159 loss = 1.562286\n",
      "epoch 160 loss = 1.584904\n",
      "epoch 161 loss = 1.598894\n",
      "epoch 162 loss = 1.390414\n",
      "epoch 163 loss = 1.485630\n",
      "epoch 164 loss = 1.491790\n",
      "epoch 165 loss = 1.506448\n",
      "epoch 166 loss = 1.730403\n",
      "epoch 167 loss = 1.844705\n",
      "epoch 168 loss = 1.669578\n",
      "epoch 169 loss = 1.740408\n",
      "epoch 170 loss = 1.632087\n",
      "epoch 171 loss = 1.519897\n",
      "epoch 172 loss = 1.663167\n",
      "epoch 173 loss = 1.754900\n",
      "epoch 174 loss = 1.622383\n",
      "epoch 175 loss = 1.703000\n",
      "epoch 176 loss = 1.479257\n",
      "epoch 177 loss = 1.554893\n",
      "epoch 178 loss = 1.638413\n",
      "epoch 179 loss = 1.507120\n",
      "epoch 180 loss = 1.693567\n",
      "epoch 181 loss = 1.552856\n",
      "epoch 182 loss = 1.475546\n",
      "epoch 183 loss = 1.556471\n",
      "epoch 184 loss = 1.604422\n",
      "epoch 185 loss = 1.550418\n",
      "epoch 186 loss = 1.547416\n",
      "epoch 187 loss = 1.478620\n",
      "epoch 188 loss = 1.537732\n",
      "epoch 189 loss = 1.578145\n",
      "epoch 190 loss = 1.457137\n",
      "epoch 191 loss = 1.628443\n",
      "epoch 192 loss = 1.650543\n",
      "epoch 193 loss = 1.480228\n",
      "epoch 194 loss = 1.539906\n",
      "epoch 195 loss = 1.627005\n",
      "epoch 196 loss = 1.529899\n",
      "epoch 197 loss = 1.490997\n",
      "epoch 198 loss = 1.737039\n",
      "epoch 199 loss = 1.527185\n",
      "epoch 200 loss = 1.628000\n",
      "epoch 201 loss = 1.522492\n",
      "epoch 202 loss = 1.578282\n",
      "epoch 203 loss = 1.575978\n",
      "epoch 204 loss = 1.551543\n",
      "epoch 205 loss = 1.690694\n",
      "epoch 206 loss = 1.861891\n",
      "epoch 207 loss = 1.650774\n",
      "epoch 208 loss = 1.491259\n",
      "epoch 209 loss = 1.601924\n",
      "epoch 210 loss = 1.529145\n",
      "epoch 211 loss = 1.516913\n",
      "epoch 212 loss = 1.485416\n",
      "epoch 213 loss = 1.530629\n",
      "epoch 214 loss = 1.534420\n",
      "epoch 215 loss = 1.436387\n",
      "epoch 216 loss = 1.648287\n",
      "epoch 217 loss = 1.622880\n",
      "epoch 218 loss = 1.558167\n",
      "epoch 219 loss = 1.723324\n",
      "epoch 220 loss = 1.585119\n",
      "epoch 221 loss = 1.568186\n",
      "epoch 222 loss = 1.578963\n",
      "epoch 223 loss = 1.370220\n",
      "epoch 224 loss = 1.649053\n",
      "epoch 225 loss = 1.701824\n",
      "epoch 226 loss = 1.558986\n",
      "epoch 227 loss = 1.447872\n",
      "epoch 228 loss = 1.572447\n",
      "epoch 229 loss = 1.491754\n",
      "epoch 230 loss = 1.600763\n",
      "epoch 231 loss = 1.664515\n",
      "epoch 232 loss = 1.685574\n",
      "epoch 233 loss = 1.658858\n",
      "epoch 234 loss = 1.706098\n",
      "epoch 235 loss = 1.548087\n",
      "epoch 236 loss = 1.636953\n",
      "epoch 237 loss = 1.585549\n",
      "epoch 238 loss = 1.690429\n",
      "epoch 239 loss = 1.571029\n",
      "epoch 240 loss = 1.660088\n",
      "epoch 241 loss = 1.547243\n",
      "epoch 242 loss = 1.507342\n",
      "epoch 243 loss = 1.540005\n",
      "epoch 244 loss = 1.759133\n",
      "epoch 245 loss = 1.611347\n",
      "epoch 246 loss = 1.463410\n",
      "epoch 247 loss = 1.685589\n",
      "epoch 248 loss = 1.695591\n",
      "epoch 249 loss = 1.502189\n",
      "epoch 250 loss = 1.480719\n",
      "epoch 251 loss = 1.497370\n",
      "epoch 252 loss = 1.536955\n",
      "epoch 253 loss = 1.559281\n",
      "epoch 254 loss = 1.682981\n",
      "epoch 255 loss = 1.420047\n",
      "epoch 256 loss = 1.430323\n",
      "epoch 257 loss = 1.371001\n",
      "epoch 258 loss = 1.679382\n",
      "epoch 259 loss = 1.406384\n",
      "epoch 260 loss = 1.472205\n",
      "epoch 261 loss = 1.573092\n",
      "epoch 262 loss = 1.613565\n",
      "epoch 263 loss = 1.499434\n",
      "epoch 264 loss = 1.424884\n",
      "epoch 265 loss = 1.637335\n",
      "epoch 266 loss = 1.259801\n",
      "epoch 267 loss = 1.599348\n",
      "epoch 268 loss = 1.576414\n",
      "epoch 269 loss = 1.442662\n",
      "epoch 270 loss = 1.517797\n",
      "epoch 271 loss = 1.502479\n",
      "epoch 272 loss = 1.382200\n",
      "epoch 273 loss = 1.539641\n",
      "epoch 274 loss = 1.416855\n",
      "epoch 275 loss = 1.640968\n",
      "epoch 276 loss = 1.673038\n",
      "epoch 277 loss = 1.439503\n",
      "epoch 278 loss = 1.521678\n",
      "epoch 279 loss = 1.552031\n",
      "epoch 280 loss = 1.485934\n",
      "epoch 281 loss = 1.533263\n",
      "epoch 282 loss = 1.332679\n",
      "epoch 283 loss = 1.648248\n",
      "epoch 284 loss = 1.431080\n",
      "epoch 285 loss = 1.459340\n",
      "epoch 286 loss = 1.650373\n",
      "epoch 287 loss = 1.523318\n",
      "epoch 288 loss = 1.605205\n",
      "epoch 289 loss = 1.373824\n",
      "epoch 290 loss = 1.533131\n",
      "epoch 291 loss = 1.355435\n",
      "epoch 292 loss = 1.665627\n",
      "epoch 293 loss = 1.661893\n",
      "epoch 294 loss = 1.436961\n",
      "epoch 295 loss = 1.395459\n",
      "epoch 296 loss = 1.562980\n",
      "epoch 297 loss = 1.543112\n",
      "epoch 298 loss = 1.733101\n",
      "epoch 299 loss = 1.458293\n",
      "epoch 300 loss = 1.485367\n",
      "epoch 301 loss = 1.371587\n",
      "epoch 302 loss = 1.530647\n",
      "epoch 303 loss = 1.542786\n",
      "epoch 304 loss = 1.661581\n",
      "epoch 305 loss = 1.551150\n",
      "epoch 306 loss = 1.602614\n",
      "epoch 307 loss = 1.466365\n",
      "epoch 308 loss = 1.462779\n",
      "epoch 309 loss = 1.488878\n",
      "epoch 310 loss = 1.674942\n",
      "epoch 311 loss = 1.457024\n",
      "epoch 312 loss = 1.451700\n",
      "epoch 313 loss = 1.554715\n",
      "epoch 314 loss = 1.522122\n",
      "epoch 315 loss = 1.710137\n",
      "epoch 316 loss = 1.379835\n",
      "epoch 317 loss = 1.588915\n",
      "epoch 318 loss = 1.528126\n",
      "epoch 319 loss = 1.511301\n",
      "epoch 320 loss = 1.388023\n",
      "epoch 321 loss = 1.632744\n",
      "epoch 322 loss = 1.540285\n",
      "epoch 323 loss = 1.720300\n",
      "epoch 324 loss = 1.506996\n",
      "epoch 325 loss = 1.428355\n",
      "epoch 326 loss = 1.703845\n",
      "epoch 327 loss = 1.500639\n",
      "epoch 328 loss = 1.512375\n",
      "epoch 329 loss = 1.383361\n",
      "epoch 330 loss = 1.547763\n",
      "epoch 331 loss = 1.738652\n",
      "epoch 332 loss = 1.637118\n",
      "epoch 333 loss = 1.524772\n",
      "epoch 334 loss = 1.592278\n",
      "epoch 335 loss = 1.498097\n",
      "epoch 336 loss = 1.586745\n",
      "epoch 337 loss = 1.744432\n",
      "epoch 338 loss = 1.396230\n",
      "epoch 339 loss = 1.592377\n",
      "epoch 340 loss = 1.573632\n",
      "epoch 341 loss = 1.805639\n",
      "epoch 342 loss = 1.319649\n",
      "epoch 343 loss = 1.522258\n",
      "epoch 344 loss = 1.575223\n",
      "epoch 345 loss = 1.626953\n",
      "epoch 346 loss = 1.451689\n",
      "epoch 347 loss = 1.458290\n",
      "epoch 348 loss = 1.632124\n",
      "epoch 349 loss = 1.819263\n",
      "epoch 350 loss = 1.537850\n",
      "epoch 351 loss = 1.445899\n",
      "epoch 352 loss = 1.506531\n",
      "epoch 353 loss = 1.506164\n",
      "epoch 354 loss = 1.453474\n",
      "epoch 355 loss = 1.650151\n",
      "epoch 356 loss = 1.484687\n",
      "epoch 357 loss = 1.627463\n",
      "epoch 358 loss = 1.451600\n",
      "epoch 359 loss = 1.464218\n",
      "epoch 360 loss = 1.744069\n",
      "epoch 361 loss = 1.443310\n",
      "epoch 362 loss = 1.522073\n",
      "epoch 363 loss = 1.759973\n",
      "epoch 364 loss = 1.406918\n",
      "epoch 365 loss = 1.228697\n",
      "epoch 366 loss = 1.722040\n",
      "epoch 367 loss = 1.447542\n",
      "epoch 368 loss = 1.522863\n",
      "epoch 369 loss = 1.490882\n",
      "epoch 370 loss = 1.520382\n",
      "epoch 371 loss = 1.500667\n",
      "epoch 372 loss = 1.660539\n",
      "epoch 373 loss = 1.604907\n",
      "epoch 374 loss = 1.586445\n",
      "epoch 375 loss = 1.593237\n",
      "epoch 376 loss = 1.404199\n",
      "epoch 377 loss = 1.290989\n",
      "epoch 378 loss = 1.485475\n",
      "epoch 379 loss = 1.444495\n",
      "epoch 380 loss = 1.357733\n",
      "epoch 381 loss = 1.517835\n",
      "epoch 382 loss = 1.593956\n",
      "epoch 383 loss = 1.357331\n",
      "epoch 384 loss = 1.509043\n",
      "epoch 385 loss = 1.711862\n",
      "epoch 386 loss = 1.490638\n",
      "epoch 387 loss = 1.195085\n",
      "epoch 388 loss = 1.459846\n",
      "epoch 389 loss = 1.596133\n",
      "epoch 390 loss = 1.462805\n",
      "epoch 391 loss = 1.441251\n",
      "epoch 392 loss = 1.584182\n",
      "epoch 393 loss = 1.495282\n",
      "epoch 394 loss = 1.571928\n",
      "epoch 395 loss = 1.456180\n",
      "epoch 396 loss = 1.621278\n",
      "epoch 397 loss = 1.491556\n",
      "epoch 398 loss = 1.652154\n",
      "epoch 399 loss = 1.670833\n",
      "epoch 400 loss = 1.756250\n",
      "epoch 401 loss = 1.494303\n",
      "epoch 402 loss = 1.673252\n",
      "epoch 403 loss = 1.459385\n",
      "epoch 404 loss = 1.501802\n",
      "epoch 405 loss = 1.683650\n",
      "epoch 406 loss = 1.513849\n",
      "epoch 407 loss = 1.399145\n",
      "epoch 408 loss = 1.623838\n",
      "epoch 409 loss = 1.673291\n",
      "epoch 410 loss = 1.356250\n",
      "epoch 411 loss = 1.418824\n",
      "epoch 412 loss = 1.632199\n",
      "epoch 413 loss = 1.371773\n",
      "epoch 414 loss = 1.440771\n",
      "epoch 415 loss = 1.364374\n",
      "epoch 416 loss = 1.527285\n",
      "epoch 417 loss = 1.557354\n",
      "epoch 418 loss = 1.464856\n",
      "epoch 419 loss = 1.465692\n",
      "epoch 420 loss = 1.522813\n",
      "epoch 421 loss = 1.456825\n",
      "epoch 422 loss = 1.466550\n",
      "epoch 423 loss = 1.554634\n",
      "epoch 424 loss = 1.383060\n",
      "epoch 425 loss = 1.387203\n",
      "epoch 426 loss = 1.648111\n",
      "epoch 427 loss = 1.437895\n",
      "epoch 428 loss = 1.663550\n",
      "epoch 429 loss = 1.729518\n",
      "epoch 430 loss = 1.621510\n",
      "epoch 431 loss = 1.481402\n",
      "epoch 432 loss = 1.313123\n",
      "epoch 433 loss = 1.444913\n",
      "epoch 434 loss = 1.535960\n",
      "epoch 435 loss = 1.444104\n",
      "epoch 436 loss = 1.440847\n",
      "epoch 437 loss = 1.696106\n",
      "epoch 438 loss = 1.466416\n",
      "epoch 439 loss = 1.478412\n",
      "epoch 440 loss = 1.486603\n",
      "epoch 441 loss = 1.538765\n",
      "epoch 442 loss = 1.434635\n",
      "epoch 443 loss = 1.551962\n",
      "epoch 444 loss = 1.298057\n",
      "epoch 445 loss = 1.471135\n",
      "epoch 446 loss = 1.560740\n",
      "epoch 447 loss = 1.692158\n",
      "epoch 448 loss = 1.390054\n",
      "epoch 449 loss = 1.459988\n",
      "epoch 450 loss = 1.596898\n",
      "epoch 451 loss = 1.480159\n",
      "epoch 452 loss = 1.513270\n",
      "epoch 453 loss = 1.533535\n",
      "epoch 454 loss = 1.590133\n",
      "epoch 455 loss = 1.442538\n",
      "epoch 456 loss = 1.405299\n",
      "epoch 457 loss = 1.403757\n",
      "epoch 458 loss = 1.499035\n",
      "epoch 459 loss = 1.439112\n",
      "epoch 460 loss = 1.514770\n",
      "epoch 461 loss = 1.534997\n",
      "epoch 462 loss = 1.557299\n",
      "epoch 463 loss = 1.306241\n",
      "epoch 464 loss = 1.414738\n",
      "epoch 465 loss = 1.416280\n",
      "epoch 466 loss = 1.454587\n",
      "epoch 467 loss = 1.419748\n",
      "epoch 468 loss = 1.401172\n",
      "epoch 469 loss = 1.318398\n",
      "epoch 470 loss = 1.492775\n",
      "epoch 471 loss = 1.508524\n",
      "epoch 472 loss = 1.512885\n",
      "epoch 473 loss = 1.400598\n",
      "epoch 474 loss = 1.515447\n",
      "epoch 475 loss = 1.603094\n",
      "epoch 476 loss = 1.467499\n",
      "epoch 477 loss = 1.410571\n",
      "epoch 478 loss = 1.354374\n",
      "epoch 479 loss = 1.414615\n",
      "epoch 480 loss = 1.536121\n",
      "epoch 481 loss = 1.439568\n",
      "epoch 482 loss = 1.516339\n",
      "epoch 483 loss = 1.634426\n",
      "epoch 484 loss = 1.586291\n",
      "epoch 485 loss = 1.522501\n",
      "epoch 486 loss = 1.584188\n",
      "epoch 487 loss = 1.480725\n",
      "epoch 488 loss = 1.569688\n",
      "epoch 489 loss = 1.738663\n",
      "epoch 490 loss = 1.397705\n",
      "epoch 491 loss = 1.611329\n",
      "epoch 492 loss = 1.411456\n",
      "epoch 493 loss = 1.461982\n",
      "epoch 494 loss = 1.653472\n",
      "epoch 495 loss = 1.735022\n",
      "epoch 496 loss = 1.303847\n",
      "epoch 497 loss = 1.608889\n",
      "epoch 498 loss = 1.636213\n",
      "epoch 499 loss = 1.599123\n",
      "final loss = 1.599123\n",
      "3 test runs...\n",
      "accuracy_mc = tensor(0.6895, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.6531, device='cuda:0')\n",
      "\n",
      "10 test runs...\n",
      "accuracy_mc = tensor(0.6779, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.6531, device='cuda:0')\n",
      "\n",
      "30 test runs...\n",
      "accuracy_mc = tensor(0.6459, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.6531, device='cuda:0')\n",
      "\n",
      "100 test runs...\n",
      "accuracy_mc = tensor(0.6701, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.6531, device='cuda:0')\n",
      "\n",
      "300 test runs...\n",
      "accuracy_mc = tensor(0.7117, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.6531, device='cuda:0')\n",
      "\n",
      "1000 test runs...\n",
      "accuracy_mc = tensor(0.7136, device='cuda:0')\n",
      "\n",
      "accuracy_non_mc = tensor(0.6531, device='cuda:0')\n",
      "\n",
      "training time = 3427.932810306549 seconds\n",
      "testing time (last run) = 37.11993217468262 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subset_prop, dropout_rate, reg_strength in itertools.product(\n",
    "    subset_proportions,\n",
    "    dropout_rates, reg_strengths,\n",
    "):\n",
    "\n",
    "    # Reset the random number generator for each method (to produce identical results)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    pyro.set_rng_seed(random_seed)\n",
    "\n",
    "    \"\"\"\n",
    "    Results file storage\n",
    "    \"\"\"\n",
    " \n",
    "    # Create directory to store results for the current test configuration\n",
    "    test_results_path = os.path.join(\n",
    "        './test_results',\n",
    "        'number_of_test_predictions_2',\n",
    "        'CIFAR-10',\n",
    "        test_start_time,\n",
    "        (\n",
    "            str(subset_prop)\n",
    "            + '_' + str(dropout_rate) \n",
    "            + '_' + str(reg_strength)),\n",
    "    )\n",
    "    \n",
    "    os.makedirs(test_results_path, exist_ok=True)\n",
    "    \n",
    "    test_results_accuracy_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_mc.txt\"\n",
    "    )\n",
    "    \n",
    "    test_results_accuracy_non_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_non_mc.txt\"\n",
    "    )\n",
    "\n",
    "    # Prepare new subset of the original dataset\n",
    "    subset = datasets.CIFAR10(\n",
    "        root='./datasets_files', limit_size=subset_prop, transform=transform, download=True)\n",
    "\n",
    "    # Determine sizes of training and testing set\n",
    "    train_size = int(train_prop * len(subset))\n",
    "    test_size = len(subset) - train_size\n",
    "    \n",
    "    # Print the size of the subset\n",
    "    print(\"subset size = \" + str(subset.data.shape))\n",
    "    print(\"training set size = %d\" % train_size)\n",
    "    print(\"test set size = %d\" % test_size)\n",
    "    \n",
    "    train, test = random_split(subset, lengths=[train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "    # Prepare network\n",
    "    network = models.SimpleCIFAR10MCDropout(\n",
    "        dropout_rate=dropout_rate,\n",
    "        dropout_type='bernoulli',\n",
    "    )\n",
    "    \n",
    "    # Send the whole model to the selected torch.device\n",
    "    network.to(torch_device)\n",
    "\n",
    "    # Print the network structure\n",
    "    print(network)\n",
    "    \n",
    "    # Model to train mode\n",
    "    network.train()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    # https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam\n",
    "    # NOTE: Need to set L2 regularization from here\n",
    "    optimizer = optim.Adam(\n",
    "        network.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=reg_strength, # L2 regularization\n",
    "    )\n",
    "\n",
    "    print()\n",
    "\n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        \"Starting subset %f, dropout_rate %f, reg_strength %f\"\n",
    "        % (subset_prop, dropout_rate, reg_strength))\n",
    "\n",
    "    # Record training start time (for this split)\n",
    "    tic = time.time()\n",
    "\n",
    "    for epoch in range(n_epoch): # loop over the dataset multiple times\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, targets = data\n",
    "\n",
    "            # Store the batch to torch_device's memory\n",
    "            inputs = inputs.to(torch_device)\n",
    "            targets = targets.to(torch_device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = network(inputs)\n",
    "\n",
    "            loss = objective(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"epoch %d loss = %f\" % (epoch, loss.item()))\n",
    "            \n",
    "    # Record training end time\n",
    "    toc = time.time()\n",
    "\n",
    "    # Report the final loss\n",
    "    print(\"final loss = %f\" % (loss.item()))\n",
    "\n",
    "    \"\"\"\n",
    "    Testing\n",
    "    \"\"\"\n",
    "\n",
    "    # Model to eval mode\n",
    "    network.eval()\n",
    "\n",
    "    for n_predictions in prediction_runs:\n",
    "        print(str(n_predictions) + \" test runs...\")\n",
    "\n",
    "        # Get the test data\n",
    "        test_loader = DataLoader(test, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "        # Record testing start time (for this split)\n",
    "        tic_testing = time.time()\n",
    "\n",
    "        _, mean, metrics = network.predict_dist(test_loader, n_predictions, torch_device)\n",
    "\n",
    "        # Record testing end time\n",
    "        toc_testing = time.time()\n",
    "\n",
    "        # store additional metrics\n",
    "        if len(metrics) > 0:\n",
    "\n",
    "            for key, value in metrics.items():\n",
    "                print(str(key) + \" = \" + str(value))\n",
    "\n",
    "                if key == 'accuracy_mc':\n",
    "                    with open(test_results_accuracy_mc_path, 'a+') as accuracy_mc_file:\n",
    "                        accuracy_mc_file.write('%d %f \\n' % (n_predictions, value))\n",
    "\n",
    "                elif key == 'accuracy_non_mc':\n",
    "                    with open(test_results_accuracy_non_mc_path, 'a+') as accuracy_non_mc_file:\n",
    "                        accuracy_non_mc_file.write('%d %f \\n' % (n_predictions, value))\n",
    "                print()\n",
    "            \n",
    "    # Report the total training time\n",
    "    print(\"training time = \" + str(toc - tic) + \" seconds\")\n",
    "    \n",
    "    # Report the total testing time\n",
    "    print(\"testing time (last run) = \" + str(toc_testing - tic_testing) + \" seconds\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDxkRM5aVrdf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment_nn_capacity_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
