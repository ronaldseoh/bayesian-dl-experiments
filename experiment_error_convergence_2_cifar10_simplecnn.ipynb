{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bka_bK83VFHh"
   },
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D5NSPs_cJZe"
   },
   "source": [
    "### Random seed / PyTorch / CUDA related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7497,
     "status": "ok",
     "timestamp": 1575014963660,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "pHbfpytEVFHu",
    "outputId": "d7b77d77-5833-40d3-f21a-986ead165d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.4)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.3.1)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1)\n",
      "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.39.0)\n",
      "/content/drive/My Drive/Colab Notebooks/bayesian-dl-experiments\n",
      "datasets_files\t\t\t experiment_convergence_2.ipynb  README.md\n",
      "experiment_comparison_toy.ipynb  experiment_nn_capacity_1.ipynb  ronald_bdl\n",
      "experiment_convergence_1.ipynb\t LICENSE\t\t\t test_results\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # If there's a package I need to install separately, do it here\n",
    "    !pip install pyro-ppl\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd 'drive/My Drive/Colab Notebooks/bayesian-dl-experiments'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls\n",
    "\n",
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seeds\n",
    "# Based on https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random_seed = 682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqlpuws9Y-8U"
   },
   "source": [
    "### Third party libraries (NumPy, PyTorch, Pyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7691,
     "status": "ok",
     "timestamp": 1575014963874,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "2zNVvKmZY-8X",
    "outputId": "6c8c564e-79d0-49ec-9090-ddd214e570f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.8 (default, Oct  7 2019, 12:59:55) \n",
      "[GCC 8.3.0]\n",
      "NumPy Version: 1.17.4\n",
      "PyTorch Version: 1.3.1\n",
      "Pyro Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Third party libraries import\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print version information\n",
    "print(\"Python Version: \" + sys.version)\n",
    "print(\"NumPy Version: \" + np.__version__)\n",
    "print(\"PyTorch Version: \" + torch.__version__)\n",
    "print(\"Pyro Version: \" + pyro.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8313,
     "status": "ok",
     "timestamp": 1575014964514,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "uyRIfCC5Y-8g",
    "outputId": "6bdb3406-46fc-4ff6-bd0b-a25ae2c2fbee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 10.1.243\n",
      "cuDNN Version: 7603\n",
      "CUDA Device Name: Tesla K80\n",
      "CUDA Capabilities: (3, 7)\n"
     ]
    }
   ],
   "source": [
    "# More imports...\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader, RandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pyro.infer import SVI, Trace_ELBO, HMC, MCMC\n",
    "\n",
    "# Import model and dataset classes from ronald_bdl\n",
    "from ronald_bdl import models, datasets\n",
    "\n",
    "# pyplot setting\n",
    "%matplotlib inline\n",
    "\n",
    "# torch.device / CUDA Setup\n",
    "use_cuda = True\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Disable 'benchmark' mode\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    use_pin_memory = True # Faster Host to GPU copies with page-locked memory\n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIFRoH3AcJZn"
   },
   "source": [
    "### Variable settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGRg2u0Q_I3n"
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2gc_i7T_HVw"
   },
   "outputs": [],
   "source": [
    "# CIFAR10 data transformation setting\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Set the proportion of the original dataset to be available as a whole\n",
    "subset_proportions = [0.1, 0.5, 1]\n",
    "\n",
    "# Proportion of the dataset to be used for training\n",
    "dataset_train_size = 0.8\n",
    "\n",
    "# Number of dataset splits\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzxIZiUcA8D8"
   },
   "source": [
    "#### NN settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_pzGq1_cJZp"
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "dropout_rates = [0.01]\n",
    "\n",
    "# Regularization strengths\n",
    "reg_strengths = [0.0005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuTnXABzVFKI"
   },
   "source": [
    "\n",
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p19qFgSAVFKS"
   },
   "outputs": [],
   "source": [
    "# Epochs\n",
    "n_epochs = [10, 100, 1000]\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 0.001 # PyTorch default value is 0.001\n",
    "\n",
    "# Training data batch sizes\n",
    "n_training_batch = 128\n",
    "\n",
    "# Number of test predictions (for each data point)\n",
    "n_prediction = 10\n",
    "\n",
    "# Cross Entropy to minimize\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test start time\n",
    "test_start_time = datetime.datetime.today().strftime('%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PpzPMI8VFKE"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 200123,
     "status": "error",
     "timestamp": 1574977642893,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "m4kavCiTVFKf",
    "outputId": "f377a355-ba19-4b3b-b856-7e12f5fb6886",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset 0.100000, dropout_rate 0.010000, reg_strength 0.000500\n",
      "n_epoch 10\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (5000, 32, 32, 3)\n",
      "training set size = 4000\n",
      "test set size = 1000\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.007522\n",
      "epoch 1 loss = 1.848649\n",
      "epoch 2 loss = 1.701818\n",
      "epoch 3 loss = 1.565948\n",
      "epoch 4 loss = 1.474506\n",
      "epoch 5 loss = 1.376433\n",
      "epoch 6 loss = 1.310120\n",
      "epoch 7 loss = 1.230645\n",
      "epoch 8 loss = 1.173191\n",
      "epoch 9 loss = 1.096413\n",
      "final loss = 1.096413\n",
      "accuracy_mc = tensor(0.4556, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4534, device='cuda:0')\n",
      "training time = 11.017351865768433 seconds\n",
      "testing time = 0.3115875720977783 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.166982\n",
      "epoch 1 loss = 2.111107\n",
      "epoch 2 loss = 2.000000\n",
      "epoch 3 loss = 1.836208\n",
      "epoch 4 loss = 1.714867\n",
      "epoch 5 loss = 1.645934\n",
      "epoch 6 loss = 1.574343\n",
      "epoch 7 loss = 1.475113\n",
      "epoch 8 loss = 1.388012\n",
      "epoch 9 loss = 1.312057\n",
      "final loss = 1.312057\n",
      "accuracy_mc = tensor(0.4367, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4255, device='cuda:0')\n",
      "training time = 10.976114988327026 seconds\n",
      "testing time = 0.3536536693572998 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.208648\n",
      "epoch 1 loss = 1.956620\n",
      "epoch 2 loss = 1.814933\n",
      "epoch 3 loss = 1.742200\n",
      "epoch 4 loss = 1.698318\n",
      "epoch 5 loss = 1.675129\n",
      "epoch 6 loss = 1.611568\n",
      "epoch 7 loss = 1.511987\n",
      "epoch 8 loss = 1.423535\n",
      "epoch 9 loss = 1.337789\n",
      "final loss = 1.337789\n",
      "accuracy_mc = tensor(0.4446, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4323, device='cuda:0')\n",
      "training time = 10.925546646118164 seconds\n",
      "testing time = 0.32520294189453125 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.012994\n",
      "epoch 1 loss = 1.853031\n",
      "epoch 2 loss = 1.733534\n",
      "epoch 3 loss = 1.599650\n",
      "epoch 4 loss = 1.504861\n",
      "epoch 5 loss = 1.410583\n",
      "epoch 6 loss = 1.301413\n",
      "epoch 7 loss = 1.200444\n",
      "epoch 8 loss = 1.127155\n",
      "epoch 9 loss = 1.026649\n",
      "final loss = 1.026649\n",
      "accuracy_mc = tensor(0.4282, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4250, device='cuda:0')\n",
      "training time = 10.956621885299683 seconds\n",
      "testing time = 0.3273200988769531 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 1.929375\n",
      "epoch 1 loss = 1.620895\n",
      "epoch 2 loss = 1.462381\n",
      "epoch 3 loss = 1.332220\n",
      "epoch 4 loss = 1.286363\n",
      "epoch 5 loss = 1.221328\n",
      "epoch 6 loss = 1.181704\n",
      "epoch 7 loss = 1.147438\n",
      "epoch 8 loss = 1.090059\n",
      "epoch 9 loss = 1.008747\n",
      "final loss = 1.008747\n",
      "accuracy_mc = tensor(0.4484, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4511, device='cuda:0')\n",
      "training time = 10.891776323318481 seconds\n",
      "testing time = 0.3336827754974365 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.190454\n",
      "epoch 1 loss = 2.097644\n",
      "epoch 2 loss = 1.863615\n",
      "epoch 3 loss = 1.755175\n",
      "epoch 4 loss = 1.651729\n",
      "epoch 5 loss = 1.496847\n",
      "epoch 6 loss = 1.393669\n",
      "epoch 7 loss = 1.283250\n",
      "epoch 8 loss = 1.178692\n",
      "epoch 9 loss = 1.124007\n",
      "final loss = 1.124007\n",
      "accuracy_mc = tensor(0.4465, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4472, device='cuda:0')\n",
      "training time = 10.98387885093689 seconds\n",
      "testing time = 0.3568251132965088 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.044582\n",
      "epoch 1 loss = 1.860411\n",
      "epoch 2 loss = 1.694890\n",
      "epoch 3 loss = 1.554933\n",
      "epoch 4 loss = 1.406744\n",
      "epoch 5 loss = 1.257527\n",
      "epoch 6 loss = 1.204510\n",
      "epoch 7 loss = 1.109581\n",
      "epoch 8 loss = 1.026531\n",
      "epoch 9 loss = 0.923939\n",
      "final loss = 0.923939\n",
      "accuracy_mc = tensor(0.4233, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4251, device='cuda:0')\n",
      "training time = 10.895861387252808 seconds\n",
      "testing time = 0.3324558734893799 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.044919\n",
      "epoch 1 loss = 1.927095\n",
      "epoch 2 loss = 1.849471\n",
      "epoch 3 loss = 1.734227\n",
      "epoch 4 loss = 1.615824\n",
      "epoch 5 loss = 1.523171\n",
      "epoch 6 loss = 1.455090\n",
      "epoch 7 loss = 1.399448\n",
      "epoch 8 loss = 1.333978\n",
      "epoch 9 loss = 1.300451\n",
      "final loss = 1.300451\n",
      "accuracy_mc = tensor(0.3742, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3741, device='cuda:0')\n",
      "training time = 10.913340330123901 seconds\n",
      "testing time = 0.32535624504089355 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.222099\n",
      "epoch 1 loss = 2.010202\n",
      "epoch 2 loss = 1.835550\n",
      "epoch 3 loss = 1.761904\n",
      "epoch 4 loss = 1.658805\n",
      "epoch 5 loss = 1.560929\n",
      "epoch 6 loss = 1.501074\n",
      "epoch 7 loss = 1.401486\n",
      "epoch 8 loss = 1.298377\n",
      "epoch 9 loss = 1.223662\n",
      "final loss = 1.223662\n",
      "accuracy_mc = tensor(0.4194, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4210, device='cuda:0')\n",
      "training time = 10.933140516281128 seconds\n",
      "testing time = 0.3286280632019043 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 1.912113\n",
      "epoch 1 loss = 1.730344\n",
      "epoch 2 loss = 1.709066\n",
      "epoch 3 loss = 1.637321\n",
      "epoch 4 loss = 1.482409\n",
      "epoch 5 loss = 1.377127\n",
      "epoch 6 loss = 1.270653\n",
      "epoch 7 loss = 1.198631\n",
      "epoch 8 loss = 1.165424\n",
      "epoch 9 loss = 1.084764\n",
      "final loss = 1.084764\n",
      "accuracy_mc = tensor(0.4433, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4513, device='cuda:0')\n",
      "training time = 10.867115020751953 seconds\n",
      "testing time = 0.3153362274169922 seconds\n",
      "\n",
      "subset 0.100000, dropout_rate 0.010000, reg_strength 0.000500\n",
      "n_epoch 100\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (5000, 32, 32, 3)\n",
      "training set size = 4000\n",
      "test set size = 1000\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.007522\n",
      "epoch 1 loss = 1.848649\n",
      "epoch 2 loss = 1.701818\n",
      "epoch 3 loss = 1.565948\n",
      "epoch 4 loss = 1.474506\n",
      "epoch 5 loss = 1.376433\n",
      "epoch 6 loss = 1.310120\n",
      "epoch 7 loss = 1.230645\n",
      "epoch 8 loss = 1.173191\n",
      "epoch 9 loss = 1.096413\n",
      "epoch 10 loss = 1.030534\n",
      "epoch 11 loss = 0.989403\n",
      "epoch 12 loss = 0.901955\n",
      "epoch 13 loss = 0.850276\n",
      "epoch 14 loss = 0.785589\n",
      "epoch 15 loss = 0.778718\n",
      "epoch 16 loss = 0.694760\n",
      "epoch 17 loss = 0.631250\n",
      "epoch 18 loss = 0.571635\n",
      "epoch 19 loss = 0.516169\n",
      "epoch 20 loss = 0.494126\n",
      "epoch 21 loss = 0.440147\n",
      "epoch 22 loss = 0.432464\n",
      "epoch 23 loss = 0.386815\n",
      "epoch 24 loss = 0.377977\n",
      "epoch 25 loss = 0.336058\n",
      "epoch 26 loss = 0.302527\n",
      "epoch 27 loss = 0.294952\n",
      "epoch 28 loss = 0.223958\n",
      "epoch 29 loss = 0.227835\n",
      "epoch 30 loss = 0.218307\n",
      "epoch 31 loss = 0.177856\n",
      "epoch 32 loss = 0.162937\n",
      "epoch 33 loss = 0.209624\n",
      "epoch 34 loss = 0.291009\n",
      "epoch 35 loss = 0.215138\n",
      "epoch 36 loss = 0.166426\n",
      "epoch 37 loss = 0.127003\n",
      "epoch 38 loss = 0.131915\n",
      "epoch 39 loss = 0.114660\n",
      "epoch 40 loss = 0.299287\n",
      "epoch 41 loss = 0.089610\n",
      "epoch 42 loss = 0.070329\n",
      "epoch 43 loss = 0.085628\n",
      "epoch 44 loss = 0.055035\n",
      "epoch 45 loss = 0.070957\n",
      "epoch 46 loss = 0.054293\n",
      "epoch 47 loss = 0.037977\n",
      "epoch 48 loss = 0.056948\n",
      "epoch 49 loss = 0.058334\n",
      "epoch 50 loss = 0.042333\n",
      "epoch 51 loss = 0.055725\n",
      "epoch 52 loss = 0.033264\n",
      "epoch 53 loss = 0.076448\n",
      "epoch 54 loss = 0.026770\n",
      "epoch 55 loss = 0.065061\n",
      "epoch 56 loss = 0.076873\n",
      "epoch 57 loss = 0.170298\n",
      "epoch 58 loss = 0.116309\n",
      "epoch 59 loss = 0.081912\n",
      "epoch 60 loss = 0.048306\n",
      "epoch 61 loss = 0.124194\n",
      "epoch 62 loss = 0.085072\n",
      "epoch 63 loss = 0.057298\n",
      "epoch 64 loss = 0.030733\n",
      "epoch 65 loss = 0.025241\n",
      "epoch 66 loss = 0.016451\n",
      "epoch 67 loss = 0.019005\n",
      "epoch 68 loss = 0.041544\n",
      "epoch 69 loss = 0.055003\n",
      "epoch 70 loss = 0.030969\n",
      "epoch 71 loss = 0.023929\n",
      "epoch 72 loss = 0.027077\n",
      "epoch 73 loss = 0.142771\n",
      "epoch 74 loss = 0.015707\n",
      "epoch 75 loss = 0.023527\n",
      "epoch 76 loss = 0.012858\n",
      "epoch 77 loss = 0.048383\n",
      "epoch 78 loss = 0.024754\n",
      "epoch 79 loss = 0.017553\n",
      "epoch 80 loss = 0.114607\n",
      "epoch 81 loss = 0.010593\n",
      "epoch 82 loss = 0.015276\n",
      "epoch 83 loss = 0.010971\n",
      "epoch 84 loss = 0.055344\n",
      "epoch 85 loss = 0.005884\n",
      "epoch 86 loss = 0.027806\n",
      "epoch 87 loss = 0.006325\n",
      "epoch 88 loss = 0.005682\n",
      "epoch 89 loss = 0.005291\n",
      "epoch 90 loss = 0.004748\n",
      "epoch 91 loss = 0.012356\n",
      "epoch 92 loss = 0.062005\n",
      "epoch 93 loss = 0.004599\n",
      "epoch 94 loss = 0.008573\n",
      "epoch 95 loss = 0.007607\n",
      "epoch 96 loss = 0.026518\n",
      "epoch 97 loss = 0.015369\n",
      "epoch 98 loss = 0.003727\n",
      "epoch 99 loss = 0.004112\n",
      "final loss = 0.004112\n",
      "accuracy_mc = tensor(0.5371, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.5334, device='cuda:0')\n",
      "training time = 109.41723418235779 seconds\n",
      "testing time = 0.33993053436279297 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.007379\n",
      "epoch 1 loss = 1.998133\n",
      "epoch 2 loss = 1.936377\n",
      "epoch 3 loss = 1.833960\n",
      "epoch 4 loss = 1.732620\n",
      "epoch 5 loss = 1.635646\n",
      "epoch 6 loss = 1.539682\n",
      "epoch 7 loss = 1.495848\n",
      "epoch 8 loss = 1.433142\n",
      "epoch 9 loss = 1.392820\n",
      "epoch 10 loss = 1.300746\n",
      "epoch 11 loss = 1.226914\n",
      "epoch 12 loss = 1.217170\n",
      "epoch 13 loss = 1.116402\n",
      "epoch 14 loss = 1.081452\n",
      "epoch 15 loss = 0.994601\n",
      "epoch 16 loss = 0.963985\n",
      "epoch 17 loss = 0.937080\n",
      "epoch 18 loss = 0.882729\n",
      "epoch 19 loss = 0.838926\n",
      "epoch 20 loss = 0.773958\n",
      "epoch 21 loss = 0.737404\n",
      "epoch 22 loss = 0.674919\n",
      "epoch 23 loss = 0.654566\n",
      "epoch 24 loss = 0.587472\n",
      "epoch 25 loss = 0.567552\n",
      "epoch 26 loss = 0.555493\n",
      "epoch 27 loss = 0.481102\n",
      "epoch 28 loss = 0.551013\n",
      "epoch 29 loss = 0.518192\n",
      "epoch 30 loss = 0.460322\n",
      "epoch 31 loss = 0.354770\n",
      "epoch 32 loss = 0.302380\n",
      "epoch 33 loss = 0.324967\n",
      "epoch 34 loss = 0.287278\n",
      "epoch 35 loss = 0.312625\n",
      "epoch 36 loss = 0.258386\n",
      "epoch 37 loss = 0.214117\n",
      "epoch 38 loss = 0.178426\n",
      "epoch 39 loss = 0.200955\n",
      "epoch 40 loss = 0.232217\n",
      "epoch 41 loss = 0.139182\n",
      "epoch 42 loss = 0.112947\n",
      "epoch 43 loss = 0.108655\n",
      "epoch 44 loss = 0.119469\n",
      "epoch 45 loss = 0.092774\n",
      "epoch 46 loss = 0.146624\n",
      "epoch 47 loss = 0.169874\n",
      "epoch 48 loss = 0.139146\n",
      "epoch 49 loss = 0.222078\n",
      "epoch 50 loss = 0.115021\n",
      "epoch 51 loss = 0.086671\n",
      "epoch 52 loss = 0.080950\n",
      "epoch 53 loss = 0.059684\n",
      "epoch 54 loss = 0.117301\n",
      "epoch 55 loss = 0.124860\n",
      "epoch 56 loss = 0.181040\n",
      "epoch 57 loss = 0.129258\n",
      "epoch 58 loss = 0.174899\n",
      "epoch 59 loss = 0.112294\n",
      "epoch 60 loss = 0.077421\n",
      "epoch 61 loss = 0.086337\n",
      "epoch 62 loss = 0.059031\n",
      "epoch 63 loss = 0.083203\n",
      "epoch 64 loss = 0.048509\n",
      "epoch 65 loss = 0.051696\n",
      "epoch 66 loss = 0.022654\n",
      "epoch 67 loss = 0.042169\n",
      "epoch 68 loss = 0.083207\n",
      "epoch 69 loss = 0.036911\n",
      "epoch 70 loss = 0.184540\n",
      "epoch 71 loss = 0.162552\n",
      "epoch 72 loss = 0.044746\n",
      "epoch 73 loss = 0.094189\n",
      "epoch 74 loss = 0.025380\n",
      "epoch 75 loss = 0.058447\n",
      "epoch 76 loss = 0.025959\n",
      "epoch 77 loss = 0.056530\n",
      "epoch 78 loss = 0.132398\n",
      "epoch 79 loss = 0.032600\n",
      "epoch 80 loss = 0.025491\n",
      "epoch 81 loss = 0.081789\n",
      "epoch 82 loss = 0.035880\n",
      "epoch 83 loss = 0.045478\n",
      "epoch 84 loss = 0.036283\n",
      "epoch 85 loss = 0.012344\n",
      "epoch 86 loss = 0.014407\n",
      "epoch 87 loss = 0.018721\n",
      "epoch 88 loss = 0.013464\n",
      "epoch 89 loss = 0.008666\n",
      "epoch 90 loss = 0.008125\n",
      "epoch 91 loss = 0.010226\n",
      "epoch 92 loss = 0.010908\n",
      "epoch 93 loss = 0.015667\n",
      "epoch 94 loss = 0.003741\n",
      "epoch 95 loss = 0.012485\n",
      "epoch 96 loss = 0.024112\n",
      "epoch 97 loss = 0.008377\n",
      "epoch 98 loss = 0.012622\n",
      "epoch 99 loss = 0.010081\n",
      "final loss = 0.010081\n",
      "accuracy_mc = tensor(0.4901, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4889, device='cuda:0')\n",
      "training time = 109.10828256607056 seconds\n",
      "testing time = 0.31062984466552734 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.224866\n",
      "epoch 1 loss = 2.057986\n",
      "epoch 2 loss = 1.917831\n",
      "epoch 3 loss = 1.790767\n",
      "epoch 4 loss = 1.706232\n",
      "epoch 5 loss = 1.593357\n",
      "epoch 6 loss = 1.488014\n",
      "epoch 7 loss = 1.370089\n",
      "epoch 8 loss = 1.298356\n",
      "epoch 9 loss = 1.181366\n",
      "epoch 10 loss = 1.123754\n",
      "epoch 11 loss = 1.043563\n",
      "epoch 12 loss = 0.950421\n",
      "epoch 13 loss = 0.927360\n",
      "epoch 14 loss = 0.860582\n",
      "epoch 15 loss = 0.747921\n",
      "epoch 16 loss = 0.704221\n",
      "epoch 17 loss = 0.617668\n",
      "epoch 18 loss = 0.549303\n",
      "epoch 19 loss = 0.515767\n",
      "epoch 20 loss = 0.467255\n",
      "epoch 21 loss = 0.415609\n",
      "epoch 22 loss = 0.364003\n",
      "epoch 23 loss = 0.368069\n",
      "epoch 24 loss = 0.293030\n",
      "epoch 25 loss = 0.283359\n",
      "epoch 26 loss = 0.256966\n",
      "epoch 27 loss = 0.289725\n",
      "epoch 28 loss = 0.214168\n",
      "epoch 29 loss = 0.200709\n",
      "epoch 30 loss = 0.174720\n",
      "epoch 31 loss = 0.190162\n",
      "epoch 32 loss = 0.153338\n",
      "epoch 33 loss = 0.160002\n",
      "epoch 34 loss = 0.162085\n",
      "epoch 35 loss = 0.112477\n",
      "epoch 36 loss = 0.081925\n",
      "epoch 37 loss = 0.067155\n",
      "epoch 38 loss = 0.067475\n",
      "epoch 39 loss = 0.082186\n",
      "epoch 40 loss = 0.064370\n",
      "epoch 41 loss = 0.068835\n",
      "epoch 42 loss = 0.140461\n",
      "epoch 43 loss = 0.087140\n",
      "epoch 44 loss = 0.055359\n",
      "epoch 45 loss = 0.091912\n",
      "epoch 46 loss = 0.038851\n",
      "epoch 47 loss = 0.045972\n",
      "epoch 48 loss = 0.049240\n",
      "epoch 49 loss = 0.054516\n",
      "epoch 50 loss = 0.033029\n",
      "epoch 51 loss = 0.045839\n",
      "epoch 52 loss = 0.047178\n",
      "epoch 53 loss = 0.076126\n",
      "epoch 54 loss = 0.054401\n",
      "epoch 55 loss = 0.090945\n",
      "epoch 56 loss = 0.064959\n",
      "epoch 57 loss = 0.103006\n",
      "epoch 58 loss = 0.042675\n",
      "epoch 59 loss = 0.033956\n",
      "epoch 60 loss = 0.046935\n",
      "epoch 61 loss = 0.020735\n",
      "epoch 62 loss = 0.039274\n",
      "epoch 63 loss = 0.028815\n",
      "epoch 64 loss = 0.085511\n",
      "epoch 65 loss = 0.057729\n",
      "epoch 66 loss = 0.111752\n",
      "epoch 67 loss = 0.039736\n",
      "epoch 68 loss = 0.086360\n",
      "epoch 69 loss = 0.112814\n",
      "epoch 70 loss = 0.198718\n",
      "epoch 71 loss = 0.011760\n",
      "epoch 72 loss = 0.009163\n",
      "epoch 73 loss = 0.049199\n",
      "epoch 74 loss = 0.015601\n",
      "epoch 75 loss = 0.008880\n",
      "epoch 76 loss = 0.013029\n",
      "epoch 77 loss = 0.051527\n",
      "epoch 78 loss = 0.020946\n",
      "epoch 79 loss = 0.004111\n",
      "epoch 80 loss = 0.010847\n",
      "epoch 81 loss = 0.012653\n",
      "epoch 82 loss = 0.023194\n",
      "epoch 83 loss = 0.011581\n",
      "epoch 84 loss = 0.005476\n",
      "epoch 85 loss = 0.015316\n",
      "epoch 86 loss = 0.009944\n",
      "epoch 87 loss = 0.017472\n",
      "epoch 88 loss = 0.004713\n",
      "epoch 89 loss = 0.007161\n",
      "epoch 90 loss = 0.004124\n",
      "epoch 91 loss = 0.011466\n",
      "epoch 92 loss = 0.002993\n",
      "epoch 93 loss = 0.005651\n",
      "epoch 94 loss = 0.019969\n",
      "epoch 95 loss = 0.002465\n",
      "epoch 96 loss = 0.008474\n",
      "epoch 97 loss = 0.002970\n",
      "epoch 98 loss = 0.008851\n",
      "epoch 99 loss = 0.001598\n",
      "final loss = 0.001598\n",
      "accuracy_mc = tensor(0.4557, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4566, device='cuda:0')\n",
      "training time = 109.11407446861267 seconds\n",
      "testing time = 0.3210585117340088 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.027561\n",
      "epoch 1 loss = 1.854929\n",
      "epoch 2 loss = 1.772897\n",
      "epoch 3 loss = 1.619551\n",
      "epoch 4 loss = 1.532757\n",
      "epoch 5 loss = 1.443484\n",
      "epoch 6 loss = 1.332899\n",
      "epoch 7 loss = 1.226323\n",
      "epoch 8 loss = 1.178318\n",
      "epoch 9 loss = 1.093532\n",
      "epoch 10 loss = 1.030244\n",
      "epoch 11 loss = 1.001049\n",
      "epoch 12 loss = 0.866320\n",
      "epoch 13 loss = 0.826360\n",
      "epoch 14 loss = 0.754932\n",
      "epoch 15 loss = 0.704346\n",
      "epoch 16 loss = 0.616413\n",
      "epoch 17 loss = 0.621553\n",
      "epoch 18 loss = 0.548612\n",
      "epoch 19 loss = 0.499058\n",
      "epoch 20 loss = 0.466010\n",
      "epoch 21 loss = 0.459357\n",
      "epoch 22 loss = 0.442331\n",
      "epoch 23 loss = 0.410475\n",
      "epoch 24 loss = 0.372884\n",
      "epoch 25 loss = 0.360512\n",
      "epoch 26 loss = 0.323217\n",
      "epoch 27 loss = 0.298196\n",
      "epoch 28 loss = 0.249792\n",
      "epoch 29 loss = 0.250052\n",
      "epoch 30 loss = 0.211479\n",
      "epoch 31 loss = 0.201210\n",
      "epoch 32 loss = 0.162765\n",
      "epoch 33 loss = 0.199410\n",
      "epoch 34 loss = 0.208295\n",
      "epoch 35 loss = 0.162272\n",
      "epoch 36 loss = 0.121730\n",
      "epoch 37 loss = 0.154481\n",
      "epoch 38 loss = 0.143438\n",
      "epoch 39 loss = 0.199104\n",
      "epoch 40 loss = 0.104255\n",
      "epoch 41 loss = 0.088915\n",
      "epoch 42 loss = 0.206907\n",
      "epoch 43 loss = 0.156131\n",
      "epoch 44 loss = 0.084186\n",
      "epoch 45 loss = 0.182410\n",
      "epoch 46 loss = 0.057778\n",
      "epoch 47 loss = 0.050743\n",
      "epoch 48 loss = 0.058000\n",
      "epoch 49 loss = 0.046973\n",
      "epoch 50 loss = 0.038911\n",
      "epoch 51 loss = 0.049303\n",
      "epoch 52 loss = 0.047230\n",
      "epoch 53 loss = 0.101073\n",
      "epoch 54 loss = 0.081292\n",
      "epoch 55 loss = 0.045275\n",
      "epoch 56 loss = 0.027494\n",
      "epoch 57 loss = 0.041453\n",
      "epoch 58 loss = 0.063815\n",
      "epoch 59 loss = 0.055849\n",
      "epoch 60 loss = 0.063217\n",
      "epoch 61 loss = 0.089723\n",
      "epoch 62 loss = 0.104156\n",
      "epoch 63 loss = 0.043178\n",
      "epoch 64 loss = 0.133666\n",
      "epoch 65 loss = 0.043097\n",
      "epoch 66 loss = 0.055652\n",
      "epoch 67 loss = 0.150773\n",
      "epoch 68 loss = 0.031980\n",
      "epoch 69 loss = 0.017815\n",
      "epoch 70 loss = 0.012382\n",
      "epoch 71 loss = 0.045426\n",
      "epoch 72 loss = 0.023829\n",
      "epoch 73 loss = 0.019587\n",
      "epoch 74 loss = 0.038411\n",
      "epoch 75 loss = 0.009295\n",
      "epoch 76 loss = 0.028013\n",
      "epoch 77 loss = 0.035420\n",
      "epoch 78 loss = 0.023728\n",
      "epoch 79 loss = 0.026353\n",
      "epoch 80 loss = 0.017038\n",
      "epoch 81 loss = 0.006124\n",
      "epoch 82 loss = 0.036811\n",
      "epoch 83 loss = 0.013851\n",
      "epoch 84 loss = 0.008969\n",
      "epoch 85 loss = 0.005780\n",
      "epoch 86 loss = 0.003438\n",
      "epoch 87 loss = 0.054631\n",
      "epoch 88 loss = 0.006426\n",
      "epoch 89 loss = 0.004421\n",
      "epoch 90 loss = 0.002333\n",
      "epoch 91 loss = 0.009917\n",
      "epoch 92 loss = 0.004094\n",
      "epoch 93 loss = 0.090894\n",
      "epoch 94 loss = 0.001486\n",
      "epoch 95 loss = 0.018741\n",
      "epoch 96 loss = 0.001651\n",
      "epoch 97 loss = 0.117187\n",
      "epoch 98 loss = 0.002844\n",
      "epoch 99 loss = 0.002543\n",
      "final loss = 0.002543\n",
      "accuracy_mc = tensor(0.5172, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.5243, device='cuda:0')\n",
      "training time = 108.97451090812683 seconds\n",
      "testing time = 0.31719493865966797 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 1.984667\n",
      "epoch 1 loss = 1.764212\n",
      "epoch 2 loss = 1.617373\n",
      "epoch 3 loss = 1.507104\n",
      "epoch 4 loss = 1.399102\n",
      "epoch 5 loss = 1.287209\n",
      "epoch 6 loss = 1.224457\n",
      "epoch 7 loss = 1.133748\n",
      "epoch 8 loss = 1.018648\n",
      "epoch 9 loss = 0.966728\n",
      "epoch 10 loss = 0.892194\n",
      "epoch 11 loss = 0.773227\n",
      "epoch 12 loss = 0.769675\n",
      "epoch 13 loss = 0.716829\n",
      "epoch 14 loss = 0.605196\n",
      "epoch 15 loss = 0.562820\n",
      "epoch 16 loss = 0.519528\n",
      "epoch 17 loss = 0.497566\n",
      "epoch 18 loss = 0.502463\n",
      "epoch 19 loss = 0.368966\n",
      "epoch 20 loss = 0.386443\n",
      "epoch 21 loss = 0.346567\n",
      "epoch 22 loss = 0.332286\n",
      "epoch 23 loss = 0.298401\n",
      "epoch 24 loss = 0.249793\n",
      "epoch 25 loss = 0.218346\n",
      "epoch 26 loss = 0.201780\n",
      "epoch 27 loss = 0.176274\n",
      "epoch 28 loss = 0.313547\n",
      "epoch 29 loss = 0.147695\n",
      "epoch 30 loss = 0.110583\n",
      "epoch 31 loss = 0.101720\n",
      "epoch 32 loss = 0.087797\n",
      "epoch 33 loss = 0.080602\n",
      "epoch 34 loss = 0.151242\n",
      "epoch 35 loss = 0.125239\n",
      "epoch 36 loss = 0.079419\n",
      "epoch 37 loss = 0.100177\n",
      "epoch 38 loss = 0.073845\n",
      "epoch 39 loss = 0.128591\n",
      "epoch 40 loss = 0.080726\n",
      "epoch 41 loss = 0.119508\n",
      "epoch 42 loss = 0.151128\n",
      "epoch 43 loss = 0.102655\n",
      "epoch 44 loss = 0.153618\n",
      "epoch 45 loss = 0.079431\n",
      "epoch 46 loss = 0.052535\n",
      "epoch 47 loss = 0.041773\n",
      "epoch 48 loss = 0.035317\n",
      "epoch 49 loss = 0.118664\n",
      "epoch 50 loss = 0.061974\n",
      "epoch 51 loss = 0.046949\n",
      "epoch 52 loss = 0.046243\n",
      "epoch 53 loss = 0.031657\n",
      "epoch 54 loss = 0.025298\n",
      "epoch 55 loss = 0.030953\n",
      "epoch 56 loss = 0.022426\n",
      "epoch 57 loss = 0.049092\n",
      "epoch 58 loss = 0.030783\n",
      "epoch 59 loss = 0.085451\n",
      "epoch 60 loss = 0.013649\n",
      "epoch 61 loss = 0.107968\n",
      "epoch 62 loss = 0.134616\n",
      "epoch 63 loss = 0.032132\n",
      "epoch 64 loss = 0.017520\n",
      "epoch 65 loss = 0.093993\n",
      "epoch 66 loss = 0.093662\n",
      "epoch 67 loss = 0.007887\n",
      "epoch 68 loss = 0.011700\n",
      "epoch 69 loss = 0.005861\n",
      "epoch 70 loss = 0.009178\n",
      "epoch 71 loss = 0.075903\n",
      "epoch 72 loss = 0.003456\n",
      "epoch 73 loss = 0.007996\n",
      "epoch 74 loss = 0.011800\n",
      "epoch 75 loss = 0.026651\n",
      "epoch 76 loss = 0.062773\n",
      "epoch 77 loss = 0.115048\n",
      "epoch 78 loss = 0.030179\n",
      "epoch 79 loss = 0.008803\n",
      "epoch 80 loss = 0.003308\n",
      "epoch 81 loss = 0.002601\n",
      "epoch 82 loss = 0.003953\n",
      "epoch 83 loss = 0.003004\n",
      "epoch 84 loss = 0.003084\n",
      "epoch 85 loss = 0.003741\n",
      "epoch 86 loss = 0.004404\n",
      "epoch 87 loss = 0.001275\n",
      "epoch 88 loss = 0.012932\n",
      "epoch 89 loss = 0.001034\n",
      "epoch 90 loss = 0.000455\n",
      "epoch 91 loss = 0.001237\n",
      "epoch 92 loss = 0.016915\n",
      "epoch 93 loss = 0.001861\n",
      "epoch 94 loss = 0.003986\n",
      "epoch 95 loss = 0.002238\n",
      "epoch 96 loss = 0.001039\n",
      "epoch 97 loss = 0.001481\n",
      "epoch 98 loss = 0.005425\n",
      "epoch 99 loss = 0.002995\n",
      "final loss = 0.002995\n",
      "accuracy_mc = tensor(0.5089, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.5052, device='cuda:0')\n",
      "training time = 108.94979882240295 seconds\n",
      "testing time = 0.3311800956726074 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.136504\n",
      "epoch 1 loss = 2.031358\n",
      "epoch 2 loss = 1.876112\n",
      "epoch 3 loss = 1.683101\n",
      "epoch 4 loss = 1.491077\n",
      "epoch 5 loss = 1.399276\n",
      "epoch 6 loss = 1.327256\n",
      "epoch 7 loss = 1.258507\n",
      "epoch 8 loss = 1.180316\n",
      "epoch 9 loss = 1.104214\n",
      "epoch 10 loss = 1.037428\n",
      "epoch 11 loss = 0.969021\n",
      "epoch 12 loss = 0.942193\n",
      "epoch 13 loss = 0.870975\n",
      "epoch 14 loss = 0.802245\n",
      "epoch 15 loss = 0.778072\n",
      "epoch 16 loss = 0.736591\n",
      "epoch 17 loss = 0.657359\n",
      "epoch 18 loss = 0.602702\n",
      "epoch 19 loss = 0.545948\n",
      "epoch 20 loss = 0.507681\n",
      "epoch 21 loss = 0.470720\n",
      "epoch 22 loss = 0.411258\n",
      "epoch 23 loss = 0.378062\n",
      "epoch 24 loss = 0.353399\n",
      "epoch 25 loss = 0.309150\n",
      "epoch 26 loss = 0.259217\n",
      "epoch 27 loss = 0.260156\n",
      "epoch 28 loss = 0.219977\n",
      "epoch 29 loss = 0.221659\n",
      "epoch 30 loss = 0.189064\n",
      "epoch 31 loss = 0.203121\n",
      "epoch 32 loss = 0.278742\n",
      "epoch 33 loss = 0.185165\n",
      "epoch 34 loss = 0.181456\n",
      "epoch 35 loss = 0.191166\n",
      "epoch 36 loss = 0.169282\n",
      "epoch 37 loss = 0.140235\n",
      "epoch 38 loss = 0.128570\n",
      "epoch 39 loss = 0.122109\n",
      "epoch 40 loss = 0.104226\n",
      "epoch 41 loss = 0.078805\n",
      "epoch 42 loss = 0.051388\n",
      "epoch 43 loss = 0.040935\n",
      "epoch 44 loss = 0.060722\n",
      "epoch 45 loss = 0.106879\n",
      "epoch 46 loss = 0.183683\n",
      "epoch 47 loss = 0.046024\n",
      "epoch 48 loss = 0.039542\n",
      "epoch 49 loss = 0.209114\n",
      "epoch 50 loss = 0.096799\n",
      "epoch 51 loss = 0.046460\n",
      "epoch 52 loss = 0.044545\n",
      "epoch 53 loss = 0.079106\n",
      "epoch 54 loss = 0.042977\n",
      "epoch 55 loss = 0.043814\n",
      "epoch 56 loss = 0.346604\n",
      "epoch 57 loss = 0.049960\n",
      "epoch 58 loss = 0.024318\n",
      "epoch 59 loss = 0.016544\n",
      "epoch 60 loss = 0.072803\n",
      "epoch 61 loss = 0.013445\n",
      "epoch 62 loss = 0.017816\n",
      "epoch 63 loss = 0.031818\n",
      "epoch 64 loss = 0.027521\n",
      "epoch 65 loss = 0.095895\n",
      "epoch 66 loss = 0.075704\n",
      "epoch 67 loss = 0.048379\n",
      "epoch 68 loss = 0.024230\n",
      "epoch 69 loss = 0.022889\n",
      "epoch 70 loss = 0.017608\n",
      "epoch 71 loss = 0.009466\n",
      "epoch 72 loss = 0.083664\n",
      "epoch 73 loss = 0.014983\n",
      "epoch 74 loss = 0.066216\n",
      "epoch 75 loss = 0.006309\n",
      "epoch 76 loss = 0.004007\n",
      "epoch 77 loss = 0.006473\n",
      "epoch 78 loss = 0.007198\n",
      "epoch 79 loss = 0.005629\n",
      "epoch 80 loss = 0.002760\n",
      "epoch 81 loss = 0.007546\n",
      "epoch 82 loss = 0.003152\n",
      "epoch 83 loss = 0.013859\n",
      "epoch 84 loss = 0.003576\n",
      "epoch 85 loss = 0.007962\n",
      "epoch 86 loss = 0.016787\n",
      "epoch 87 loss = 0.069063\n",
      "epoch 88 loss = 0.024273\n",
      "epoch 89 loss = 0.028200\n",
      "epoch 90 loss = 0.010830\n",
      "epoch 91 loss = 0.005404\n",
      "epoch 92 loss = 0.009873\n",
      "epoch 93 loss = 0.050520\n",
      "epoch 94 loss = 0.010274\n",
      "epoch 95 loss = 0.054597\n",
      "epoch 96 loss = 0.008993\n",
      "epoch 97 loss = 0.007877\n",
      "epoch 98 loss = 0.007630\n",
      "epoch 99 loss = 0.106088\n",
      "final loss = 0.106088\n",
      "accuracy_mc = tensor(0.4550, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4555, device='cuda:0')\n",
      "training time = 109.18862438201904 seconds\n",
      "testing time = 0.3153674602508545 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.039819\n",
      "epoch 1 loss = 1.792802\n",
      "epoch 2 loss = 1.683617\n",
      "epoch 3 loss = 1.537699\n",
      "epoch 4 loss = 1.469195\n",
      "epoch 5 loss = 1.366541\n",
      "epoch 6 loss = 1.257762\n",
      "epoch 7 loss = 1.180556\n",
      "epoch 8 loss = 1.117728\n",
      "epoch 9 loss = 1.060088\n",
      "epoch 10 loss = 1.029493\n",
      "epoch 11 loss = 0.930528\n",
      "epoch 12 loss = 0.857463\n",
      "epoch 13 loss = 0.838630\n",
      "epoch 14 loss = 0.777835\n",
      "epoch 15 loss = 0.676510\n",
      "epoch 16 loss = 0.669700\n",
      "epoch 17 loss = 0.646316\n",
      "epoch 18 loss = 0.578709\n",
      "epoch 19 loss = 0.553245\n",
      "epoch 20 loss = 0.560132\n",
      "epoch 21 loss = 0.509760\n",
      "epoch 22 loss = 0.438430\n",
      "epoch 23 loss = 0.445618\n",
      "epoch 24 loss = 0.374771\n",
      "epoch 25 loss = 0.355122\n",
      "epoch 26 loss = 0.345049\n",
      "epoch 27 loss = 0.322147\n",
      "epoch 28 loss = 0.284181\n",
      "epoch 29 loss = 0.262387\n",
      "epoch 30 loss = 0.240362\n",
      "epoch 31 loss = 0.222663\n",
      "epoch 32 loss = 0.198225\n",
      "epoch 33 loss = 0.195377\n",
      "epoch 34 loss = 0.225667\n",
      "epoch 35 loss = 0.158828\n",
      "epoch 36 loss = 0.149551\n",
      "epoch 37 loss = 0.131269\n",
      "epoch 38 loss = 0.154002\n",
      "epoch 39 loss = 0.147744\n",
      "epoch 40 loss = 0.118718\n",
      "epoch 41 loss = 0.197282\n",
      "epoch 42 loss = 0.163843\n",
      "epoch 43 loss = 0.184652\n",
      "epoch 44 loss = 0.101798\n",
      "epoch 45 loss = 0.114030\n",
      "epoch 46 loss = 0.094599\n",
      "epoch 47 loss = 0.179833\n",
      "epoch 48 loss = 0.088033\n",
      "epoch 49 loss = 0.064908\n",
      "epoch 50 loss = 0.069380\n",
      "epoch 51 loss = 0.058268\n",
      "epoch 52 loss = 0.043778\n",
      "epoch 53 loss = 0.068372\n",
      "epoch 54 loss = 0.044840\n",
      "epoch 55 loss = 0.088706\n",
      "epoch 56 loss = 0.043927\n",
      "epoch 57 loss = 0.153871\n",
      "epoch 58 loss = 0.075109\n",
      "epoch 59 loss = 0.042363\n",
      "epoch 60 loss = 0.105228\n",
      "epoch 61 loss = 0.057478\n",
      "epoch 62 loss = 0.068892\n",
      "epoch 63 loss = 0.056908\n",
      "epoch 64 loss = 0.062654\n",
      "epoch 65 loss = 0.098379\n",
      "epoch 66 loss = 0.066452\n",
      "epoch 67 loss = 0.039881\n",
      "epoch 68 loss = 0.026703\n",
      "epoch 69 loss = 0.052550\n",
      "epoch 70 loss = 0.036565\n",
      "epoch 71 loss = 0.029402\n",
      "epoch 72 loss = 0.010863\n",
      "epoch 73 loss = 0.011739\n",
      "epoch 74 loss = 0.013407\n",
      "epoch 75 loss = 0.011277\n",
      "epoch 76 loss = 0.018822\n",
      "epoch 77 loss = 0.008848\n",
      "epoch 78 loss = 0.015998\n",
      "epoch 79 loss = 0.058755\n",
      "epoch 80 loss = 0.011838\n",
      "epoch 81 loss = 0.013718\n",
      "epoch 82 loss = 0.027903\n",
      "epoch 83 loss = 0.003907\n",
      "epoch 84 loss = 0.010745\n",
      "epoch 85 loss = 0.037219\n",
      "epoch 86 loss = 0.003884\n",
      "epoch 87 loss = 0.011620\n",
      "epoch 88 loss = 0.002239\n",
      "epoch 89 loss = 0.040169\n",
      "epoch 90 loss = 0.007709\n",
      "epoch 91 loss = 0.008001\n",
      "epoch 92 loss = 0.009783\n",
      "epoch 93 loss = 0.010137\n",
      "epoch 94 loss = 0.016720\n",
      "epoch 95 loss = 0.013108\n",
      "epoch 96 loss = 0.005208\n",
      "epoch 97 loss = 0.015581\n",
      "epoch 98 loss = 0.003683\n",
      "epoch 99 loss = 0.264207\n",
      "final loss = 0.264207\n",
      "accuracy_mc = tensor(0.4319, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4384, device='cuda:0')\n",
      "training time = 109.78066563606262 seconds\n",
      "testing time = 0.31038594245910645 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.194172\n",
      "epoch 1 loss = 2.022043\n",
      "epoch 2 loss = 1.964907\n",
      "epoch 3 loss = 1.862702\n",
      "epoch 4 loss = 1.749181\n",
      "epoch 5 loss = 1.637319\n",
      "epoch 6 loss = 1.485727\n",
      "epoch 7 loss = 1.325882\n",
      "epoch 8 loss = 1.195504\n",
      "epoch 9 loss = 1.073389\n",
      "epoch 10 loss = 1.005284\n",
      "epoch 11 loss = 0.951204\n",
      "epoch 12 loss = 0.869060\n",
      "epoch 13 loss = 0.784989\n",
      "epoch 14 loss = 0.809822\n",
      "epoch 15 loss = 0.779379\n",
      "epoch 16 loss = 0.701026\n",
      "epoch 17 loss = 0.634161\n",
      "epoch 18 loss = 0.580873\n",
      "epoch 19 loss = 0.556438\n",
      "epoch 20 loss = 0.490407\n",
      "epoch 21 loss = 0.439011\n",
      "epoch 22 loss = 0.392448\n",
      "epoch 23 loss = 0.386708\n",
      "epoch 24 loss = 0.412449\n",
      "epoch 25 loss = 0.390499\n",
      "epoch 26 loss = 0.341828\n",
      "epoch 27 loss = 0.314248\n",
      "epoch 28 loss = 0.257834\n",
      "epoch 29 loss = 0.251285\n",
      "epoch 30 loss = 0.198538\n",
      "epoch 31 loss = 0.216626\n",
      "epoch 32 loss = 0.224430\n",
      "epoch 33 loss = 0.230724\n",
      "epoch 34 loss = 0.235765\n",
      "epoch 35 loss = 0.206345\n",
      "epoch 36 loss = 0.194870\n",
      "epoch 37 loss = 0.107783\n",
      "epoch 38 loss = 0.115612\n",
      "epoch 39 loss = 0.166721\n",
      "epoch 40 loss = 0.079190\n",
      "epoch 41 loss = 0.095379\n",
      "epoch 42 loss = 0.150876\n",
      "epoch 43 loss = 0.066166\n",
      "epoch 44 loss = 0.096798\n",
      "epoch 45 loss = 0.050180\n",
      "epoch 46 loss = 0.053104\n",
      "epoch 47 loss = 0.050311\n",
      "epoch 48 loss = 0.052245\n",
      "epoch 49 loss = 0.108613\n",
      "epoch 50 loss = 0.042217\n",
      "epoch 51 loss = 0.031871\n",
      "epoch 52 loss = 0.035307\n",
      "epoch 53 loss = 0.029571\n",
      "epoch 54 loss = 0.042578\n",
      "epoch 55 loss = 0.015616\n",
      "epoch 56 loss = 0.026646\n",
      "epoch 57 loss = 0.028740\n",
      "epoch 58 loss = 0.031679\n",
      "epoch 59 loss = 0.040232\n",
      "epoch 60 loss = 0.031713\n",
      "epoch 61 loss = 0.014111\n",
      "epoch 62 loss = 0.024831\n",
      "epoch 63 loss = 0.009679\n",
      "epoch 64 loss = 0.021417\n",
      "epoch 65 loss = 0.015355\n",
      "epoch 66 loss = 0.057881\n",
      "epoch 67 loss = 0.029973\n",
      "epoch 68 loss = 0.052146\n",
      "epoch 69 loss = 0.064352\n",
      "epoch 70 loss = 0.026610\n",
      "epoch 71 loss = 0.010903\n",
      "epoch 72 loss = 0.084348\n",
      "epoch 73 loss = 0.138125\n",
      "epoch 74 loss = 0.043696\n",
      "epoch 75 loss = 0.004358\n",
      "epoch 76 loss = 0.036491\n",
      "epoch 77 loss = 0.006818\n",
      "epoch 78 loss = 0.010971\n",
      "epoch 79 loss = 0.018144\n",
      "epoch 80 loss = 0.009319\n",
      "epoch 81 loss = 0.049428\n",
      "epoch 82 loss = 0.012599\n",
      "epoch 83 loss = 0.012815\n",
      "epoch 84 loss = 0.025546\n",
      "epoch 85 loss = 0.008413\n",
      "epoch 86 loss = 0.004250\n",
      "epoch 87 loss = 0.022254\n",
      "epoch 88 loss = 0.195421\n",
      "epoch 89 loss = 0.008341\n",
      "epoch 90 loss = 0.004154\n",
      "epoch 91 loss = 0.005077\n",
      "epoch 92 loss = 0.002132\n",
      "epoch 93 loss = 0.004863\n",
      "epoch 94 loss = 0.004494\n",
      "epoch 95 loss = 0.005910\n",
      "epoch 96 loss = 0.011685\n",
      "epoch 97 loss = 0.006246\n",
      "epoch 98 loss = 0.002074\n",
      "epoch 99 loss = 0.004496\n",
      "final loss = 0.004496\n",
      "accuracy_mc = tensor(0.4661, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4713, device='cuda:0')\n",
      "training time = 109.32038116455078 seconds\n",
      "testing time = 0.358288049697876 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.198614\n",
      "epoch 1 loss = 1.906018\n",
      "epoch 2 loss = 1.716211\n",
      "epoch 3 loss = 1.544024\n",
      "epoch 4 loss = 1.414286\n",
      "epoch 5 loss = 1.294735\n",
      "epoch 6 loss = 1.191396\n",
      "epoch 7 loss = 1.100353\n",
      "epoch 8 loss = 0.980507\n",
      "epoch 9 loss = 0.889379\n",
      "epoch 10 loss = 0.821704\n",
      "epoch 11 loss = 0.805400\n",
      "epoch 12 loss = 0.696904\n",
      "epoch 13 loss = 0.654328\n",
      "epoch 14 loss = 0.580667\n",
      "epoch 15 loss = 0.521655\n",
      "epoch 16 loss = 0.484697\n",
      "epoch 17 loss = 0.429757\n",
      "epoch 18 loss = 0.373827\n",
      "epoch 19 loss = 0.368285\n",
      "epoch 20 loss = 0.337127\n",
      "epoch 21 loss = 0.288041\n",
      "epoch 22 loss = 0.274667\n",
      "epoch 23 loss = 0.231430\n",
      "epoch 24 loss = 0.215344\n",
      "epoch 25 loss = 0.188644\n",
      "epoch 26 loss = 0.185473\n",
      "epoch 27 loss = 0.190873\n",
      "epoch 28 loss = 0.175836\n",
      "epoch 29 loss = 0.157486\n",
      "epoch 30 loss = 0.113613\n",
      "epoch 31 loss = 0.169898\n",
      "epoch 32 loss = 0.162870\n",
      "epoch 33 loss = 0.095812\n",
      "epoch 34 loss = 0.079237\n",
      "epoch 35 loss = 0.060209\n",
      "epoch 36 loss = 0.070365\n",
      "epoch 37 loss = 0.110254\n",
      "epoch 38 loss = 0.080328\n",
      "epoch 39 loss = 0.166177\n",
      "epoch 40 loss = 0.104095\n",
      "epoch 41 loss = 0.046741\n",
      "epoch 42 loss = 0.060048\n",
      "epoch 43 loss = 0.045824\n",
      "epoch 44 loss = 0.042388\n",
      "epoch 45 loss = 0.070318\n",
      "epoch 46 loss = 0.045987\n",
      "epoch 47 loss = 0.101415\n",
      "epoch 48 loss = 0.077654\n",
      "epoch 49 loss = 0.036860\n",
      "epoch 50 loss = 0.042648\n",
      "epoch 51 loss = 0.050050\n",
      "epoch 52 loss = 0.038225\n",
      "epoch 53 loss = 0.064940\n",
      "epoch 54 loss = 0.009226\n",
      "epoch 55 loss = 0.032132\n",
      "epoch 56 loss = 0.013282\n",
      "epoch 57 loss = 0.009985\n",
      "epoch 58 loss = 0.010098\n",
      "epoch 59 loss = 0.005463\n",
      "epoch 60 loss = 0.003525\n",
      "epoch 61 loss = 0.011285\n",
      "epoch 62 loss = 0.004300\n",
      "epoch 63 loss = 0.002155\n",
      "epoch 64 loss = 0.005419\n",
      "epoch 65 loss = 0.003388\n",
      "epoch 66 loss = 0.004665\n",
      "epoch 67 loss = 0.006504\n",
      "epoch 68 loss = 0.004295\n",
      "epoch 69 loss = 0.005064\n",
      "epoch 70 loss = 0.003692\n",
      "epoch 71 loss = 0.003758\n",
      "epoch 72 loss = 0.007813\n",
      "epoch 73 loss = 0.002198\n",
      "epoch 74 loss = 0.002690\n",
      "epoch 75 loss = 0.005937\n",
      "epoch 76 loss = 0.012479\n",
      "epoch 77 loss = 0.004190\n",
      "epoch 78 loss = 0.002545\n",
      "epoch 79 loss = 0.002896\n",
      "epoch 80 loss = 0.001288\n",
      "epoch 81 loss = 0.000828\n",
      "epoch 82 loss = 0.003979\n",
      "epoch 83 loss = 0.006031\n",
      "epoch 84 loss = 0.012901\n",
      "epoch 85 loss = 0.001720\n",
      "epoch 86 loss = 0.053325\n",
      "epoch 87 loss = 0.002264\n",
      "epoch 88 loss = 0.059986\n",
      "epoch 89 loss = 0.004454\n",
      "epoch 90 loss = 0.002046\n",
      "epoch 91 loss = 0.004030\n",
      "epoch 92 loss = 0.006096\n",
      "epoch 93 loss = 0.005541\n",
      "epoch 94 loss = 0.004610\n",
      "epoch 95 loss = 0.013199\n",
      "epoch 96 loss = 0.011650\n",
      "epoch 97 loss = 0.002531\n",
      "epoch 98 loss = 0.002288\n",
      "epoch 99 loss = 0.065701\n",
      "final loss = 0.065701\n",
      "accuracy_mc = tensor(0.4229, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4388, device='cuda:0')\n",
      "training time = 109.137211561203 seconds\n",
      "testing time = 0.32418155670166016 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 1.984628\n",
      "epoch 1 loss = 1.969018\n",
      "epoch 2 loss = 1.836321\n",
      "epoch 3 loss = 1.758470\n",
      "epoch 4 loss = 1.614523\n",
      "epoch 5 loss = 1.548989\n",
      "epoch 6 loss = 1.424079\n",
      "epoch 7 loss = 1.332023\n",
      "epoch 8 loss = 1.280376\n",
      "epoch 9 loss = 1.177946\n",
      "epoch 10 loss = 1.128097\n",
      "epoch 11 loss = 1.058751\n",
      "epoch 12 loss = 0.964579\n",
      "epoch 13 loss = 0.878615\n",
      "epoch 14 loss = 0.836103\n",
      "epoch 15 loss = 0.766270\n",
      "epoch 16 loss = 0.677182\n",
      "epoch 17 loss = 0.648487\n",
      "epoch 18 loss = 0.617181\n",
      "epoch 19 loss = 0.558761\n",
      "epoch 20 loss = 0.505626\n",
      "epoch 21 loss = 0.434314\n",
      "epoch 22 loss = 0.426686\n",
      "epoch 23 loss = 0.397353\n",
      "epoch 24 loss = 0.394893\n",
      "epoch 25 loss = 0.355287\n",
      "epoch 26 loss = 0.371081\n",
      "epoch 27 loss = 0.349200\n",
      "epoch 28 loss = 0.317260\n",
      "epoch 29 loss = 0.306682\n",
      "epoch 30 loss = 0.242592\n",
      "epoch 31 loss = 0.236951\n",
      "epoch 32 loss = 0.204636\n",
      "epoch 33 loss = 0.176658\n",
      "epoch 34 loss = 0.148285\n",
      "epoch 35 loss = 0.249831\n",
      "epoch 36 loss = 0.173764\n",
      "epoch 37 loss = 0.173386\n",
      "epoch 38 loss = 0.148873\n",
      "epoch 39 loss = 0.179138\n",
      "epoch 40 loss = 0.202890\n",
      "epoch 41 loss = 0.178035\n",
      "epoch 42 loss = 0.110034\n",
      "epoch 43 loss = 0.098350\n",
      "epoch 44 loss = 0.086869\n",
      "epoch 45 loss = 0.093592\n",
      "epoch 46 loss = 0.123311\n",
      "epoch 47 loss = 0.122491\n",
      "epoch 48 loss = 0.079348\n",
      "epoch 49 loss = 0.115379\n",
      "epoch 50 loss = 0.105876\n",
      "epoch 51 loss = 0.073250\n",
      "epoch 52 loss = 0.042059\n",
      "epoch 53 loss = 0.065831\n",
      "epoch 54 loss = 0.049360\n",
      "epoch 55 loss = 0.068050\n",
      "epoch 56 loss = 0.059022\n",
      "epoch 57 loss = 0.091404\n",
      "epoch 58 loss = 0.096746\n",
      "epoch 59 loss = 0.085506\n",
      "epoch 60 loss = 0.050003\n",
      "epoch 61 loss = 0.019877\n",
      "epoch 62 loss = 0.019991\n",
      "epoch 63 loss = 0.034733\n",
      "epoch 64 loss = 0.016624\n",
      "epoch 65 loss = 0.038864\n",
      "epoch 66 loss = 0.018952\n",
      "epoch 67 loss = 0.045214\n",
      "epoch 68 loss = 0.139949\n",
      "epoch 69 loss = 0.016146\n",
      "epoch 70 loss = 0.051772\n",
      "epoch 71 loss = 0.016384\n",
      "epoch 72 loss = 0.033802\n",
      "epoch 73 loss = 0.020847\n",
      "epoch 74 loss = 0.015163\n",
      "epoch 75 loss = 0.051117\n",
      "epoch 76 loss = 0.009726\n",
      "epoch 77 loss = 0.006117\n",
      "epoch 78 loss = 0.006167\n",
      "epoch 79 loss = 0.018557\n",
      "epoch 80 loss = 0.010417\n",
      "epoch 81 loss = 0.002034\n",
      "epoch 82 loss = 0.002995\n",
      "epoch 83 loss = 0.003875\n",
      "epoch 84 loss = 0.001774\n",
      "epoch 85 loss = 0.003065\n",
      "epoch 86 loss = 0.002822\n",
      "epoch 87 loss = 0.009066\n",
      "epoch 88 loss = 0.004841\n",
      "epoch 89 loss = 0.002093\n",
      "epoch 90 loss = 0.002348\n",
      "epoch 91 loss = 0.005267\n",
      "epoch 92 loss = 0.006336\n",
      "epoch 93 loss = 0.055650\n",
      "epoch 94 loss = 0.112560\n",
      "epoch 95 loss = 0.014787\n",
      "epoch 96 loss = 0.037592\n",
      "epoch 97 loss = 0.005137\n",
      "epoch 98 loss = 0.005753\n",
      "epoch 99 loss = 0.002174\n",
      "final loss = 0.002174\n",
      "accuracy_mc = tensor(0.4752, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4779, device='cuda:0')\n",
      "training time = 109.32315468788147 seconds\n",
      "testing time = 0.3212106227874756 seconds\n",
      "\n",
      "subset 0.100000, dropout_rate 0.010000, reg_strength 0.000500\n",
      "n_epoch 1000\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (5000, 32, 32, 3)\n",
      "training set size = 4000\n",
      "test set size = 1000\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.007522\n",
      "epoch 1 loss = 1.848649\n",
      "epoch 2 loss = 1.701818\n",
      "epoch 3 loss = 1.565948\n",
      "epoch 4 loss = 1.474506\n",
      "epoch 5 loss = 1.376433\n",
      "epoch 6 loss = 1.310120\n",
      "epoch 7 loss = 1.230645\n",
      "epoch 8 loss = 1.173191\n",
      "epoch 9 loss = 1.096413\n",
      "epoch 10 loss = 1.030534\n",
      "epoch 11 loss = 0.989403\n",
      "epoch 12 loss = 0.901955\n",
      "epoch 13 loss = 0.850276\n",
      "epoch 14 loss = 0.785589\n",
      "epoch 15 loss = 0.778718\n",
      "epoch 16 loss = 0.694760\n",
      "epoch 17 loss = 0.631250\n",
      "epoch 18 loss = 0.571635\n",
      "epoch 19 loss = 0.516169\n",
      "epoch 20 loss = 0.494126\n",
      "epoch 21 loss = 0.440147\n",
      "epoch 22 loss = 0.432464\n",
      "epoch 23 loss = 0.386815\n",
      "epoch 24 loss = 0.377977\n",
      "epoch 25 loss = 0.336058\n",
      "epoch 26 loss = 0.302527\n",
      "epoch 27 loss = 0.294952\n",
      "epoch 28 loss = 0.223958\n",
      "epoch 29 loss = 0.227835\n",
      "epoch 30 loss = 0.218307\n",
      "epoch 31 loss = 0.177856\n",
      "epoch 32 loss = 0.162937\n",
      "epoch 33 loss = 0.209624\n",
      "epoch 34 loss = 0.291009\n",
      "epoch 35 loss = 0.215138\n",
      "epoch 36 loss = 0.166426\n",
      "epoch 37 loss = 0.127003\n",
      "epoch 38 loss = 0.131915\n",
      "epoch 39 loss = 0.114660\n",
      "epoch 40 loss = 0.299287\n",
      "epoch 41 loss = 0.089610\n",
      "epoch 42 loss = 0.070329\n",
      "epoch 43 loss = 0.085628\n",
      "epoch 44 loss = 0.055035\n",
      "epoch 45 loss = 0.070957\n",
      "epoch 46 loss = 0.054293\n",
      "epoch 47 loss = 0.037977\n",
      "epoch 48 loss = 0.056948\n",
      "epoch 49 loss = 0.058334\n",
      "epoch 50 loss = 0.042333\n",
      "epoch 51 loss = 0.055725\n",
      "epoch 52 loss = 0.033264\n",
      "epoch 53 loss = 0.076448\n",
      "epoch 54 loss = 0.026770\n",
      "epoch 55 loss = 0.065061\n",
      "epoch 56 loss = 0.076873\n",
      "epoch 57 loss = 0.170298\n",
      "epoch 58 loss = 0.116309\n",
      "epoch 59 loss = 0.081912\n",
      "epoch 60 loss = 0.048306\n",
      "epoch 61 loss = 0.124194\n",
      "epoch 62 loss = 0.085072\n",
      "epoch 63 loss = 0.057298\n",
      "epoch 64 loss = 0.030733\n",
      "epoch 65 loss = 0.025241\n",
      "epoch 66 loss = 0.016451\n",
      "epoch 67 loss = 0.019005\n",
      "epoch 68 loss = 0.041544\n",
      "epoch 69 loss = 0.055003\n",
      "epoch 70 loss = 0.030969\n",
      "epoch 71 loss = 0.023929\n",
      "epoch 72 loss = 0.027077\n",
      "epoch 73 loss = 0.142771\n",
      "epoch 74 loss = 0.015707\n",
      "epoch 75 loss = 0.023527\n",
      "epoch 76 loss = 0.012858\n",
      "epoch 77 loss = 0.048383\n",
      "epoch 78 loss = 0.024754\n",
      "epoch 79 loss = 0.017553\n",
      "epoch 80 loss = 0.114607\n",
      "epoch 81 loss = 0.010593\n",
      "epoch 82 loss = 0.015276\n",
      "epoch 83 loss = 0.010971\n",
      "epoch 84 loss = 0.055344\n",
      "epoch 85 loss = 0.005884\n",
      "epoch 86 loss = 0.027806\n",
      "epoch 87 loss = 0.006325\n",
      "epoch 88 loss = 0.005682\n",
      "epoch 89 loss = 0.005291\n",
      "epoch 90 loss = 0.004748\n",
      "epoch 91 loss = 0.012356\n",
      "epoch 92 loss = 0.062005\n",
      "epoch 93 loss = 0.004599\n",
      "epoch 94 loss = 0.008573\n",
      "epoch 95 loss = 0.007607\n",
      "epoch 96 loss = 0.026518\n",
      "epoch 97 loss = 0.015369\n",
      "epoch 98 loss = 0.003727\n",
      "epoch 99 loss = 0.004112\n",
      "epoch 100 loss = 0.015470\n",
      "epoch 101 loss = 0.006364\n",
      "epoch 102 loss = 0.003529\n",
      "epoch 103 loss = 0.014114\n",
      "epoch 104 loss = 0.004004\n",
      "epoch 105 loss = 0.006983\n",
      "epoch 106 loss = 0.006497\n",
      "epoch 107 loss = 0.010388\n",
      "epoch 108 loss = 0.003306\n",
      "epoch 109 loss = 0.007447\n",
      "epoch 110 loss = 0.005398\n",
      "epoch 111 loss = 0.002156\n",
      "epoch 112 loss = 0.019334\n",
      "epoch 113 loss = 0.010186\n",
      "epoch 114 loss = 0.069787\n",
      "epoch 115 loss = 0.004910\n",
      "epoch 116 loss = 0.002708\n",
      "epoch 117 loss = 0.002052\n",
      "epoch 118 loss = 0.003398\n",
      "epoch 119 loss = 0.003847\n",
      "epoch 120 loss = 0.002594\n",
      "epoch 121 loss = 0.002787\n",
      "epoch 122 loss = 0.006768\n",
      "epoch 123 loss = 0.012936\n",
      "epoch 124 loss = 0.047835\n",
      "epoch 125 loss = 0.006781\n",
      "epoch 126 loss = 0.057522\n",
      "epoch 127 loss = 0.004918\n",
      "epoch 128 loss = 0.002878\n",
      "epoch 129 loss = 0.033915\n",
      "epoch 130 loss = 0.002589\n",
      "epoch 131 loss = 0.003044\n",
      "epoch 132 loss = 0.005263\n",
      "epoch 133 loss = 0.002834\n",
      "epoch 134 loss = 0.001603\n",
      "epoch 135 loss = 0.001912\n",
      "epoch 136 loss = 0.003919\n",
      "epoch 137 loss = 0.000932\n",
      "epoch 138 loss = 0.001773\n",
      "epoch 139 loss = 0.007211\n",
      "epoch 140 loss = 0.002815\n",
      "epoch 141 loss = 0.007985\n",
      "epoch 142 loss = 0.005312\n",
      "epoch 143 loss = 0.009826\n",
      "epoch 144 loss = 0.002971\n",
      "epoch 145 loss = 0.006202\n",
      "epoch 146 loss = 0.078680\n",
      "epoch 147 loss = 0.005813\n",
      "epoch 148 loss = 0.003785\n",
      "epoch 149 loss = 0.007341\n",
      "epoch 150 loss = 0.008952\n",
      "epoch 151 loss = 0.002940\n",
      "epoch 152 loss = 0.002325\n",
      "epoch 153 loss = 0.000999\n",
      "epoch 154 loss = 0.007340\n",
      "epoch 155 loss = 0.035882\n",
      "epoch 156 loss = 0.002514\n",
      "epoch 157 loss = 0.001905\n",
      "epoch 158 loss = 0.000914\n",
      "epoch 159 loss = 0.009447\n",
      "epoch 160 loss = 0.000956\n",
      "epoch 161 loss = 0.001104\n",
      "epoch 162 loss = 0.004234\n",
      "epoch 163 loss = 0.024556\n",
      "epoch 164 loss = 0.001404\n",
      "epoch 165 loss = 0.001605\n",
      "epoch 166 loss = 0.001636\n",
      "epoch 167 loss = 0.000722\n",
      "epoch 168 loss = 0.002640\n",
      "epoch 169 loss = 0.013207\n",
      "epoch 170 loss = 0.011831\n",
      "epoch 171 loss = 0.000951\n",
      "epoch 172 loss = 0.007693\n",
      "epoch 173 loss = 0.004782\n",
      "epoch 174 loss = 0.003495\n",
      "epoch 175 loss = 0.005627\n",
      "epoch 176 loss = 0.003200\n",
      "epoch 177 loss = 0.002633\n",
      "epoch 178 loss = 0.002709\n",
      "epoch 179 loss = 0.006867\n",
      "epoch 180 loss = 0.001025\n",
      "epoch 181 loss = 0.003407\n",
      "epoch 182 loss = 0.004963\n",
      "epoch 183 loss = 0.002177\n",
      "epoch 184 loss = 0.000715\n",
      "epoch 185 loss = 0.002371\n",
      "epoch 186 loss = 0.001625\n",
      "epoch 187 loss = 0.003075\n",
      "epoch 188 loss = 0.006312\n",
      "epoch 189 loss = 0.000779\n",
      "epoch 190 loss = 0.016917\n",
      "epoch 191 loss = 0.005353\n",
      "epoch 192 loss = 0.002855\n",
      "epoch 193 loss = 0.002090\n",
      "epoch 194 loss = 0.001857\n",
      "epoch 195 loss = 0.006386\n",
      "epoch 196 loss = 0.001785\n",
      "epoch 197 loss = 0.009553\n",
      "epoch 198 loss = 0.001058\n",
      "epoch 199 loss = 0.018772\n",
      "epoch 200 loss = 0.001918\n",
      "epoch 201 loss = 0.003689\n",
      "epoch 202 loss = 0.013445\n",
      "epoch 203 loss = 0.004786\n",
      "epoch 204 loss = 0.009768\n",
      "epoch 205 loss = 0.000829\n",
      "epoch 206 loss = 0.005215\n",
      "epoch 207 loss = 0.001263\n",
      "epoch 208 loss = 0.008071\n",
      "epoch 209 loss = 0.001189\n",
      "epoch 210 loss = 0.001312\n",
      "epoch 211 loss = 0.007037\n",
      "epoch 212 loss = 0.011609\n",
      "epoch 213 loss = 0.014473\n",
      "epoch 214 loss = 0.130384\n",
      "epoch 215 loss = 0.004987\n",
      "epoch 216 loss = 0.001933\n",
      "epoch 217 loss = 0.004457\n",
      "epoch 218 loss = 0.005184\n",
      "epoch 219 loss = 0.231681\n",
      "epoch 220 loss = 0.005016\n",
      "epoch 221 loss = 0.006807\n",
      "epoch 222 loss = 0.002855\n",
      "epoch 223 loss = 0.009483\n",
      "epoch 224 loss = 0.003021\n",
      "epoch 225 loss = 0.000756\n",
      "epoch 226 loss = 0.003768\n",
      "epoch 227 loss = 0.005620\n",
      "epoch 228 loss = 0.033260\n",
      "epoch 229 loss = 0.000589\n",
      "epoch 230 loss = 0.001282\n",
      "epoch 231 loss = 0.003935\n",
      "epoch 232 loss = 0.005554\n",
      "epoch 233 loss = 0.002632\n",
      "epoch 234 loss = 0.008274\n",
      "epoch 235 loss = 0.000827\n",
      "epoch 236 loss = 0.000555\n",
      "epoch 237 loss = 0.004029\n",
      "epoch 238 loss = 0.001051\n",
      "epoch 239 loss = 0.001037\n",
      "epoch 240 loss = 0.034115\n",
      "epoch 241 loss = 0.008940\n",
      "epoch 242 loss = 0.001005\n",
      "epoch 243 loss = 0.005001\n",
      "epoch 244 loss = 0.000457\n",
      "epoch 245 loss = 0.000359\n",
      "epoch 246 loss = 0.000476\n",
      "epoch 247 loss = 0.003533\n",
      "epoch 248 loss = 0.005036\n",
      "epoch 249 loss = 0.005305\n",
      "epoch 250 loss = 0.000472\n",
      "epoch 251 loss = 0.006542\n",
      "epoch 252 loss = 0.003703\n",
      "epoch 253 loss = 0.008822\n",
      "epoch 254 loss = 0.003877\n",
      "epoch 255 loss = 0.003065\n",
      "epoch 256 loss = 0.001970\n",
      "epoch 257 loss = 0.002088\n",
      "epoch 258 loss = 0.003474\n",
      "epoch 259 loss = 0.000991\n",
      "epoch 260 loss = 0.090068\n",
      "epoch 261 loss = 0.001035\n",
      "epoch 262 loss = 0.001879\n",
      "epoch 263 loss = 0.057086\n",
      "epoch 264 loss = 0.004338\n",
      "epoch 265 loss = 0.001310\n",
      "epoch 266 loss = 0.000650\n",
      "epoch 267 loss = 0.031436\n",
      "epoch 268 loss = 0.001702\n",
      "epoch 269 loss = 0.000742\n",
      "epoch 270 loss = 0.000301\n",
      "epoch 271 loss = 0.001216\n",
      "epoch 272 loss = 0.002794\n",
      "epoch 273 loss = 0.005463\n",
      "epoch 274 loss = 0.000462\n",
      "epoch 275 loss = 0.001451\n",
      "epoch 276 loss = 0.001124\n",
      "epoch 277 loss = 0.001066\n",
      "epoch 278 loss = 0.000754\n",
      "epoch 279 loss = 0.002212\n",
      "epoch 280 loss = 0.002611\n",
      "epoch 281 loss = 0.002834\n",
      "epoch 282 loss = 0.003235\n",
      "epoch 283 loss = 0.001529\n",
      "epoch 284 loss = 0.001838\n",
      "epoch 285 loss = 0.001649\n",
      "epoch 286 loss = 0.000396\n",
      "epoch 287 loss = 0.002072\n",
      "epoch 288 loss = 0.009123\n",
      "epoch 289 loss = 0.002152\n",
      "epoch 290 loss = 0.004496\n",
      "epoch 291 loss = 0.001033\n",
      "epoch 292 loss = 0.009081\n",
      "epoch 293 loss = 0.004198\n",
      "epoch 294 loss = 0.007055\n",
      "epoch 295 loss = 0.000561\n",
      "epoch 296 loss = 0.001954\n",
      "epoch 297 loss = 0.000920\n",
      "epoch 298 loss = 0.000908\n",
      "epoch 299 loss = 0.000836\n",
      "epoch 300 loss = 0.002208\n",
      "epoch 301 loss = 0.000765\n",
      "epoch 302 loss = 0.001124\n",
      "epoch 303 loss = 0.000716\n",
      "epoch 304 loss = 0.006080\n",
      "epoch 305 loss = 0.002259\n",
      "epoch 306 loss = 0.021758\n",
      "epoch 307 loss = 0.005388\n",
      "epoch 308 loss = 0.006603\n",
      "epoch 309 loss = 0.014726\n",
      "epoch 310 loss = 0.001044\n",
      "epoch 311 loss = 0.001875\n",
      "epoch 312 loss = 0.006468\n",
      "epoch 313 loss = 0.001961\n",
      "epoch 314 loss = 0.001183\n",
      "epoch 315 loss = 0.003735\n",
      "epoch 316 loss = 0.000294\n",
      "epoch 317 loss = 0.001342\n",
      "epoch 318 loss = 0.000506\n",
      "epoch 319 loss = 0.003954\n",
      "epoch 320 loss = 0.002503\n",
      "epoch 321 loss = 0.001831\n",
      "epoch 322 loss = 0.002924\n",
      "epoch 323 loss = 0.002451\n",
      "epoch 324 loss = 0.036165\n",
      "epoch 325 loss = 0.000536\n",
      "epoch 326 loss = 0.001231\n",
      "epoch 327 loss = 0.003375\n",
      "epoch 328 loss = 0.001549\n",
      "epoch 329 loss = 0.003480\n",
      "epoch 330 loss = 0.000543\n",
      "epoch 331 loss = 0.001107\n",
      "epoch 332 loss = 0.000306\n",
      "epoch 333 loss = 0.001717\n",
      "epoch 334 loss = 0.000465\n",
      "epoch 335 loss = 0.000895\n",
      "epoch 336 loss = 0.002219\n",
      "epoch 337 loss = 0.003106\n",
      "epoch 338 loss = 0.000492\n",
      "epoch 339 loss = 0.007964\n",
      "epoch 340 loss = 0.004205\n",
      "epoch 341 loss = 0.002234\n",
      "epoch 342 loss = 0.002239\n",
      "epoch 343 loss = 0.009908\n",
      "epoch 344 loss = 0.008993\n",
      "epoch 345 loss = 0.001616\n",
      "epoch 346 loss = 0.001212\n",
      "epoch 347 loss = 0.001017\n",
      "epoch 348 loss = 0.000807\n",
      "epoch 349 loss = 0.000784\n",
      "epoch 350 loss = 0.000505\n",
      "epoch 351 loss = 0.001886\n",
      "epoch 352 loss = 0.005380\n",
      "epoch 353 loss = 0.005083\n",
      "epoch 354 loss = 0.003074\n",
      "epoch 355 loss = 0.002227\n",
      "epoch 356 loss = 0.001609\n",
      "epoch 357 loss = 0.000444\n",
      "epoch 358 loss = 0.003403\n",
      "epoch 359 loss = 0.001997\n",
      "epoch 360 loss = 0.003237\n",
      "epoch 361 loss = 0.000606\n",
      "epoch 362 loss = 0.000866\n",
      "epoch 363 loss = 0.001094\n",
      "epoch 364 loss = 0.011141\n",
      "epoch 365 loss = 0.000603\n",
      "epoch 366 loss = 0.001783\n",
      "epoch 367 loss = 0.001165\n",
      "epoch 368 loss = 0.000896\n",
      "epoch 369 loss = 0.002155\n",
      "epoch 370 loss = 0.003516\n",
      "epoch 371 loss = 0.068354\n",
      "epoch 372 loss = 0.002892\n",
      "epoch 373 loss = 0.011942\n",
      "epoch 374 loss = 0.010864\n",
      "epoch 375 loss = 0.002397\n",
      "epoch 376 loss = 0.004505\n",
      "epoch 377 loss = 0.202804\n",
      "epoch 378 loss = 0.034295\n",
      "epoch 379 loss = 0.007056\n",
      "epoch 380 loss = 0.000863\n",
      "epoch 381 loss = 0.003454\n",
      "epoch 382 loss = 0.000870\n",
      "epoch 383 loss = 0.041259\n",
      "epoch 384 loss = 0.000973\n",
      "epoch 385 loss = 0.000437\n",
      "epoch 386 loss = 0.002584\n",
      "epoch 387 loss = 0.001284\n",
      "epoch 388 loss = 0.001943\n",
      "epoch 389 loss = 0.000423\n",
      "epoch 390 loss = 0.001110\n",
      "epoch 391 loss = 0.007405\n",
      "epoch 392 loss = 0.000871\n",
      "epoch 393 loss = 0.000898\n",
      "epoch 394 loss = 0.001122\n",
      "epoch 395 loss = 0.000926\n",
      "epoch 396 loss = 0.001348\n",
      "epoch 397 loss = 0.003035\n",
      "epoch 398 loss = 0.015769\n",
      "epoch 399 loss = 0.005641\n",
      "epoch 400 loss = 0.006678\n",
      "epoch 401 loss = 0.001120\n",
      "epoch 402 loss = 0.003699\n",
      "epoch 403 loss = 0.000618\n",
      "epoch 404 loss = 0.000595\n",
      "epoch 405 loss = 0.004613\n",
      "epoch 406 loss = 0.000474\n",
      "epoch 407 loss = 0.023678\n",
      "epoch 408 loss = 0.005790\n",
      "epoch 409 loss = 0.000475\n",
      "epoch 410 loss = 0.005830\n",
      "epoch 411 loss = 0.000840\n",
      "epoch 412 loss = 0.023010\n",
      "epoch 413 loss = 0.004415\n",
      "epoch 414 loss = 0.021611\n",
      "epoch 415 loss = 0.002728\n",
      "epoch 416 loss = 0.001905\n",
      "epoch 417 loss = 0.001391\n",
      "epoch 418 loss = 0.011680\n",
      "epoch 419 loss = 0.003715\n",
      "epoch 420 loss = 0.000217\n",
      "epoch 421 loss = 0.000607\n",
      "epoch 422 loss = 0.001940\n",
      "epoch 423 loss = 0.001123\n",
      "epoch 424 loss = 0.000633\n",
      "epoch 425 loss = 0.001148\n",
      "epoch 426 loss = 0.000779\n",
      "epoch 427 loss = 0.005715\n",
      "epoch 428 loss = 0.002352\n",
      "epoch 429 loss = 0.000723\n",
      "epoch 430 loss = 0.001443\n",
      "epoch 431 loss = 0.000493\n",
      "epoch 432 loss = 0.000183\n",
      "epoch 433 loss = 0.000747\n",
      "epoch 434 loss = 0.000412\n",
      "epoch 435 loss = 0.001688\n",
      "epoch 436 loss = 0.000336\n",
      "epoch 437 loss = 0.000235\n",
      "epoch 438 loss = 0.000283\n",
      "epoch 439 loss = 0.000471\n",
      "epoch 440 loss = 0.001206\n",
      "epoch 441 loss = 0.001950\n",
      "epoch 442 loss = 0.003102\n",
      "epoch 443 loss = 0.001262\n",
      "epoch 444 loss = 0.000419\n",
      "epoch 445 loss = 0.000677\n",
      "epoch 446 loss = 0.003861\n",
      "epoch 447 loss = 0.001075\n",
      "epoch 448 loss = 0.000447\n",
      "epoch 449 loss = 0.007727\n",
      "epoch 450 loss = 0.023478\n",
      "epoch 451 loss = 0.006157\n",
      "epoch 452 loss = 0.005050\n",
      "epoch 453 loss = 0.005834\n",
      "epoch 454 loss = 0.002870\n",
      "epoch 455 loss = 0.001678\n",
      "epoch 456 loss = 0.001426\n",
      "epoch 457 loss = 0.000970\n",
      "epoch 458 loss = 0.002241\n",
      "epoch 459 loss = 0.001013\n",
      "epoch 460 loss = 0.001328\n",
      "epoch 461 loss = 0.003540\n",
      "epoch 462 loss = 0.000329\n",
      "epoch 463 loss = 0.004102\n",
      "epoch 464 loss = 0.001130\n",
      "epoch 465 loss = 0.001578\n",
      "epoch 466 loss = 0.000525\n",
      "epoch 467 loss = 0.001341\n",
      "epoch 468 loss = 0.001625\n",
      "epoch 469 loss = 0.000545\n",
      "epoch 470 loss = 0.001183\n",
      "epoch 471 loss = 0.000541\n",
      "epoch 472 loss = 0.002724\n",
      "epoch 473 loss = 0.003456\n",
      "epoch 474 loss = 0.001431\n",
      "epoch 475 loss = 0.009548\n",
      "epoch 476 loss = 0.001688\n",
      "epoch 477 loss = 0.000817\n",
      "epoch 478 loss = 0.002212\n",
      "epoch 479 loss = 0.001583\n",
      "epoch 480 loss = 0.003445\n",
      "epoch 481 loss = 0.001926\n",
      "epoch 482 loss = 0.000783\n",
      "epoch 483 loss = 0.003495\n",
      "epoch 484 loss = 0.000709\n",
      "epoch 485 loss = 0.000664\n",
      "epoch 486 loss = 0.072323\n",
      "epoch 487 loss = 0.003203\n",
      "epoch 488 loss = 0.001014\n",
      "epoch 489 loss = 0.000728\n",
      "epoch 490 loss = 0.000664\n",
      "epoch 491 loss = 0.084771\n",
      "epoch 492 loss = 0.011579\n",
      "epoch 493 loss = 0.001261\n",
      "epoch 494 loss = 0.000605\n",
      "epoch 495 loss = 0.000829\n",
      "epoch 496 loss = 0.000947\n",
      "epoch 497 loss = 0.000724\n",
      "epoch 498 loss = 0.001027\n",
      "epoch 499 loss = 0.002277\n",
      "epoch 500 loss = 0.003619\n",
      "epoch 501 loss = 0.000200\n",
      "epoch 502 loss = 0.001068\n",
      "epoch 503 loss = 0.000527\n",
      "epoch 504 loss = 0.011562\n",
      "epoch 505 loss = 0.003392\n",
      "epoch 506 loss = 0.002960\n",
      "epoch 507 loss = 0.001039\n",
      "epoch 508 loss = 0.000352\n",
      "epoch 509 loss = 0.000159\n",
      "epoch 510 loss = 0.000316\n",
      "epoch 511 loss = 0.000363\n",
      "epoch 512 loss = 0.000348\n",
      "epoch 513 loss = 0.000656\n",
      "epoch 514 loss = 0.000294\n",
      "epoch 515 loss = 0.000284\n",
      "epoch 516 loss = 0.001665\n",
      "epoch 517 loss = 0.001063\n",
      "epoch 518 loss = 0.000985\n",
      "epoch 519 loss = 0.001301\n",
      "epoch 520 loss = 0.018980\n",
      "epoch 521 loss = 0.005238\n",
      "epoch 522 loss = 0.002183\n",
      "epoch 523 loss = 0.001637\n",
      "epoch 524 loss = 0.003264\n",
      "epoch 525 loss = 0.002424\n",
      "epoch 526 loss = 0.019049\n",
      "epoch 527 loss = 0.002561\n",
      "epoch 528 loss = 0.001619\n",
      "epoch 529 loss = 0.000744\n",
      "epoch 530 loss = 0.001192\n",
      "epoch 531 loss = 0.000555\n",
      "epoch 532 loss = 0.000243\n",
      "epoch 533 loss = 0.002129\n",
      "epoch 534 loss = 0.006176\n",
      "epoch 535 loss = 0.000466\n",
      "epoch 536 loss = 0.010809\n",
      "epoch 537 loss = 0.000950\n",
      "epoch 538 loss = 0.000290\n",
      "epoch 539 loss = 0.000428\n",
      "epoch 540 loss = 0.000831\n",
      "epoch 541 loss = 0.000316\n",
      "epoch 542 loss = 0.003147\n",
      "epoch 543 loss = 0.000522\n",
      "epoch 544 loss = 0.002987\n",
      "epoch 545 loss = 0.000815\n",
      "epoch 546 loss = 0.000240\n",
      "epoch 547 loss = 0.000340\n",
      "epoch 548 loss = 0.001003\n",
      "epoch 549 loss = 0.000314\n",
      "epoch 550 loss = 0.000740\n",
      "epoch 551 loss = 0.000423\n",
      "epoch 552 loss = 0.000332\n",
      "epoch 553 loss = 0.000843\n",
      "epoch 554 loss = 0.023800\n",
      "epoch 555 loss = 0.002182\n",
      "epoch 556 loss = 0.015114\n",
      "epoch 557 loss = 0.024133\n",
      "epoch 558 loss = 0.001825\n",
      "epoch 559 loss = 0.003569\n",
      "epoch 560 loss = 0.060015\n",
      "epoch 561 loss = 0.001346\n",
      "epoch 562 loss = 0.005436\n",
      "epoch 563 loss = 0.001894\n",
      "epoch 564 loss = 0.000621\n",
      "epoch 565 loss = 0.000962\n",
      "epoch 566 loss = 0.003242\n",
      "epoch 567 loss = 0.000653\n",
      "epoch 568 loss = 0.000253\n",
      "epoch 569 loss = 0.000415\n",
      "epoch 570 loss = 0.009258\n",
      "epoch 571 loss = 0.000744\n",
      "epoch 572 loss = 0.000640\n",
      "epoch 573 loss = 0.000234\n",
      "epoch 574 loss = 0.002639\n",
      "epoch 575 loss = 0.000574\n",
      "epoch 576 loss = 0.003063\n",
      "epoch 577 loss = 0.001216\n",
      "epoch 578 loss = 0.004498\n",
      "epoch 579 loss = 0.000763\n",
      "epoch 580 loss = 0.003329\n",
      "epoch 581 loss = 0.001174\n",
      "epoch 582 loss = 0.001465\n",
      "epoch 583 loss = 0.002257\n",
      "epoch 584 loss = 0.002972\n",
      "epoch 585 loss = 0.001525\n",
      "epoch 586 loss = 0.002191\n",
      "epoch 587 loss = 0.000289\n",
      "epoch 588 loss = 0.000839\n",
      "epoch 589 loss = 0.000379\n",
      "epoch 590 loss = 0.008904\n",
      "epoch 591 loss = 0.002754\n",
      "epoch 592 loss = 0.000888\n",
      "epoch 593 loss = 0.003862\n",
      "epoch 594 loss = 0.000256\n",
      "epoch 595 loss = 0.000752\n",
      "epoch 596 loss = 0.008982\n",
      "epoch 597 loss = 0.002100\n",
      "epoch 598 loss = 0.004204\n",
      "epoch 599 loss = 0.004803\n",
      "epoch 600 loss = 0.019621\n",
      "epoch 601 loss = 0.002029\n",
      "epoch 602 loss = 0.009371\n",
      "epoch 603 loss = 0.001104\n",
      "epoch 604 loss = 0.017879\n",
      "epoch 605 loss = 0.007679\n",
      "epoch 606 loss = 0.001153\n",
      "epoch 607 loss = 0.001411\n",
      "epoch 608 loss = 0.000659\n",
      "epoch 609 loss = 0.000234\n",
      "epoch 610 loss = 0.000472\n",
      "epoch 611 loss = 0.000947\n",
      "epoch 612 loss = 0.001674\n",
      "epoch 613 loss = 0.001605\n",
      "epoch 614 loss = 0.001165\n",
      "epoch 615 loss = 0.000484\n",
      "epoch 616 loss = 0.000114\n",
      "epoch 617 loss = 0.000303\n",
      "epoch 618 loss = 0.005308\n",
      "epoch 619 loss = 0.000821\n",
      "epoch 620 loss = 0.001819\n",
      "epoch 621 loss = 0.000276\n",
      "epoch 622 loss = 0.000271\n",
      "epoch 623 loss = 0.002817\n",
      "epoch 624 loss = 0.000805\n",
      "epoch 625 loss = 0.000530\n",
      "epoch 626 loss = 0.005524\n",
      "epoch 627 loss = 0.000581\n",
      "epoch 628 loss = 0.001808\n",
      "epoch 629 loss = 0.000882\n",
      "epoch 630 loss = 0.001915\n",
      "epoch 631 loss = 0.003217\n",
      "epoch 632 loss = 0.000458\n",
      "epoch 633 loss = 0.002376\n",
      "epoch 634 loss = 0.000295\n",
      "epoch 635 loss = 0.000707\n",
      "epoch 636 loss = 0.000848\n",
      "epoch 637 loss = 0.001189\n",
      "epoch 638 loss = 0.016453\n",
      "epoch 639 loss = 0.001341\n",
      "epoch 640 loss = 0.001754\n",
      "epoch 641 loss = 0.004028\n",
      "epoch 642 loss = 0.002783\n",
      "epoch 643 loss = 0.002968\n",
      "epoch 644 loss = 0.004376\n",
      "epoch 645 loss = 0.028860\n",
      "epoch 646 loss = 0.005806\n",
      "epoch 647 loss = 0.000891\n",
      "epoch 648 loss = 0.002279\n",
      "epoch 649 loss = 0.004539\n",
      "epoch 650 loss = 0.000641\n",
      "epoch 651 loss = 0.001559\n",
      "epoch 652 loss = 0.002035\n",
      "epoch 653 loss = 0.005675\n",
      "epoch 654 loss = 0.000426\n",
      "epoch 655 loss = 0.000365\n",
      "epoch 656 loss = 0.002303\n",
      "epoch 657 loss = 0.003095\n",
      "epoch 658 loss = 0.000374\n",
      "epoch 659 loss = 0.000357\n",
      "epoch 660 loss = 0.001200\n",
      "epoch 661 loss = 0.001695\n",
      "epoch 662 loss = 0.005910\n",
      "epoch 663 loss = 0.000334\n",
      "epoch 664 loss = 0.000261\n",
      "epoch 665 loss = 0.001105\n",
      "epoch 666 loss = 0.000178\n",
      "epoch 667 loss = 0.000194\n",
      "epoch 668 loss = 0.000251\n",
      "epoch 669 loss = 0.004980\n",
      "epoch 670 loss = 0.004277\n",
      "epoch 671 loss = 0.009604\n",
      "epoch 672 loss = 0.001779\n",
      "epoch 673 loss = 0.003834\n",
      "epoch 674 loss = 0.015139\n",
      "epoch 675 loss = 0.002409\n",
      "epoch 676 loss = 0.000978\n",
      "epoch 677 loss = 0.001240\n",
      "epoch 678 loss = 0.003467\n",
      "epoch 679 loss = 0.004015\n",
      "epoch 680 loss = 0.004789\n",
      "epoch 681 loss = 0.000365\n",
      "epoch 682 loss = 0.001520\n",
      "epoch 683 loss = 0.001279\n",
      "epoch 684 loss = 0.000530\n",
      "epoch 685 loss = 0.000380\n",
      "epoch 686 loss = 0.001295\n",
      "epoch 687 loss = 0.000406\n",
      "epoch 688 loss = 0.000637\n",
      "epoch 689 loss = 0.001490\n",
      "epoch 690 loss = 0.000235\n",
      "epoch 691 loss = 0.000711\n",
      "epoch 692 loss = 0.001423\n",
      "epoch 693 loss = 0.003099\n",
      "epoch 694 loss = 0.009729\n",
      "epoch 695 loss = 0.001002\n",
      "epoch 696 loss = 0.056862\n",
      "epoch 697 loss = 0.005447\n",
      "epoch 698 loss = 0.001242\n",
      "epoch 699 loss = 0.001194\n",
      "epoch 700 loss = 0.024659\n",
      "epoch 701 loss = 0.002541\n",
      "epoch 702 loss = 0.001054\n",
      "epoch 703 loss = 0.000731\n",
      "epoch 704 loss = 0.001571\n",
      "epoch 705 loss = 0.000542\n",
      "epoch 706 loss = 0.000510\n",
      "epoch 707 loss = 0.000602\n",
      "epoch 708 loss = 0.000967\n",
      "epoch 709 loss = 0.000215\n",
      "epoch 710 loss = 0.000425\n",
      "epoch 711 loss = 0.002315\n",
      "epoch 712 loss = 0.075969\n",
      "epoch 713 loss = 0.000923\n",
      "epoch 714 loss = 0.000724\n",
      "epoch 715 loss = 0.000429\n",
      "epoch 716 loss = 0.001285\n",
      "epoch 717 loss = 0.005663\n",
      "epoch 718 loss = 0.000881\n",
      "epoch 719 loss = 0.002763\n",
      "epoch 720 loss = 0.000482\n",
      "epoch 721 loss = 0.000680\n",
      "epoch 722 loss = 0.000290\n",
      "epoch 723 loss = 0.000391\n",
      "epoch 724 loss = 0.000720\n",
      "epoch 725 loss = 0.000620\n",
      "epoch 726 loss = 0.000169\n",
      "epoch 727 loss = 0.001310\n",
      "epoch 728 loss = 0.000578\n",
      "epoch 729 loss = 0.002929\n",
      "epoch 730 loss = 0.000491\n",
      "epoch 731 loss = 0.000304\n",
      "epoch 732 loss = 0.000740\n",
      "epoch 733 loss = 0.000346\n",
      "epoch 734 loss = 0.001781\n",
      "epoch 735 loss = 0.002263\n",
      "epoch 736 loss = 0.001976\n",
      "epoch 737 loss = 0.003445\n",
      "epoch 738 loss = 0.007409\n",
      "epoch 739 loss = 0.007940\n",
      "epoch 740 loss = 0.000606\n",
      "epoch 741 loss = 0.005883\n",
      "epoch 742 loss = 0.002229\n",
      "epoch 743 loss = 0.003061\n",
      "epoch 744 loss = 0.001005\n",
      "epoch 745 loss = 0.000457\n",
      "epoch 746 loss = 0.001808\n",
      "epoch 747 loss = 0.001933\n",
      "epoch 748 loss = 0.000544\n",
      "epoch 749 loss = 0.000074\n",
      "epoch 750 loss = 0.000416\n",
      "epoch 751 loss = 0.000154\n",
      "epoch 752 loss = 0.000590\n",
      "epoch 753 loss = 0.000247\n",
      "epoch 754 loss = 0.000925\n",
      "epoch 755 loss = 0.001201\n",
      "epoch 756 loss = 0.008347\n",
      "epoch 757 loss = 0.008636\n",
      "epoch 758 loss = 0.001267\n",
      "epoch 759 loss = 0.002069\n",
      "epoch 760 loss = 0.001823\n",
      "epoch 761 loss = 0.002341\n",
      "epoch 762 loss = 0.001739\n",
      "epoch 763 loss = 0.000628\n",
      "epoch 764 loss = 0.000231\n",
      "epoch 765 loss = 0.000297\n",
      "epoch 766 loss = 0.000515\n",
      "epoch 767 loss = 0.239816\n",
      "epoch 768 loss = 0.011156\n",
      "epoch 769 loss = 0.002051\n",
      "epoch 770 loss = 0.002955\n",
      "epoch 771 loss = 0.000889\n",
      "epoch 772 loss = 0.000991\n",
      "epoch 773 loss = 0.000826\n",
      "epoch 774 loss = 0.000417\n",
      "epoch 775 loss = 0.000265\n",
      "epoch 776 loss = 0.002608\n",
      "epoch 777 loss = 0.005420\n",
      "epoch 778 loss = 0.000381\n",
      "epoch 779 loss = 0.001857\n",
      "epoch 780 loss = 0.000784\n",
      "epoch 781 loss = 0.000276\n",
      "epoch 782 loss = 0.000598\n",
      "epoch 783 loss = 0.000266\n",
      "epoch 784 loss = 0.000469\n",
      "epoch 785 loss = 0.002786\n",
      "epoch 786 loss = 0.000313\n",
      "epoch 787 loss = 0.000883\n",
      "epoch 788 loss = 0.001100\n",
      "epoch 789 loss = 0.001379\n",
      "epoch 790 loss = 0.001058\n",
      "epoch 791 loss = 0.001671\n",
      "epoch 792 loss = 0.000972\n",
      "epoch 793 loss = 0.000643\n",
      "epoch 794 loss = 0.000382\n",
      "epoch 795 loss = 0.000988\n",
      "epoch 796 loss = 0.017569\n",
      "epoch 797 loss = 0.007836\n",
      "epoch 798 loss = 0.002474\n",
      "epoch 799 loss = 0.004536\n",
      "epoch 800 loss = 0.002930\n",
      "epoch 801 loss = 0.008350\n",
      "epoch 802 loss = 0.002054\n",
      "epoch 803 loss = 0.023954\n",
      "epoch 804 loss = 0.003814\n",
      "epoch 805 loss = 0.000739\n",
      "epoch 806 loss = 0.000956\n",
      "epoch 807 loss = 0.000465\n",
      "epoch 808 loss = 0.000301\n",
      "epoch 809 loss = 0.000476\n",
      "epoch 810 loss = 0.001100\n",
      "epoch 811 loss = 0.001808\n",
      "epoch 812 loss = 0.000695\n",
      "epoch 813 loss = 0.000641\n",
      "epoch 814 loss = 0.000449\n",
      "epoch 815 loss = 0.000570\n",
      "epoch 816 loss = 0.000835\n",
      "epoch 817 loss = 0.000919\n",
      "epoch 818 loss = 0.001326\n",
      "epoch 819 loss = 0.003182\n",
      "epoch 820 loss = 0.000728\n",
      "epoch 821 loss = 0.001504\n",
      "epoch 822 loss = 0.004652\n",
      "epoch 823 loss = 0.001806\n",
      "epoch 824 loss = 0.003223\n",
      "epoch 825 loss = 0.000555\n",
      "epoch 826 loss = 0.002151\n",
      "epoch 827 loss = 0.001119\n",
      "epoch 828 loss = 0.000780\n",
      "epoch 829 loss = 0.000756\n",
      "epoch 830 loss = 0.003536\n",
      "epoch 831 loss = 0.002658\n",
      "epoch 832 loss = 0.000696\n",
      "epoch 833 loss = 0.004433\n",
      "epoch 834 loss = 0.003619\n",
      "epoch 835 loss = 0.002548\n",
      "epoch 836 loss = 0.013321\n",
      "epoch 837 loss = 0.003810\n",
      "epoch 838 loss = 0.070070\n",
      "epoch 839 loss = 0.003119\n",
      "epoch 840 loss = 0.032660\n",
      "epoch 841 loss = 0.023410\n",
      "epoch 842 loss = 0.002485\n",
      "epoch 843 loss = 0.000586\n",
      "epoch 844 loss = 0.002415\n",
      "epoch 845 loss = 0.000610\n",
      "epoch 846 loss = 0.002971\n",
      "epoch 847 loss = 0.000891\n",
      "epoch 848 loss = 0.004965\n",
      "epoch 849 loss = 0.002248\n",
      "epoch 850 loss = 0.000553\n",
      "epoch 851 loss = 0.002692\n",
      "epoch 852 loss = 0.001264\n",
      "epoch 853 loss = 0.022855\n",
      "epoch 854 loss = 0.000317\n",
      "epoch 855 loss = 0.000805\n",
      "epoch 856 loss = 0.000942\n",
      "epoch 857 loss = 0.005257\n",
      "epoch 858 loss = 0.001681\n",
      "epoch 859 loss = 0.000381\n",
      "epoch 860 loss = 0.015440\n",
      "epoch 861 loss = 0.003645\n",
      "epoch 862 loss = 0.000523\n",
      "epoch 863 loss = 0.000679\n",
      "epoch 864 loss = 0.000557\n",
      "epoch 865 loss = 0.002391\n",
      "epoch 866 loss = 0.003118\n",
      "epoch 867 loss = 0.011706\n",
      "epoch 868 loss = 0.000488\n",
      "epoch 869 loss = 0.003099\n",
      "epoch 870 loss = 0.000392\n",
      "epoch 871 loss = 0.000416\n",
      "epoch 872 loss = 0.002599\n",
      "epoch 873 loss = 0.000717\n",
      "epoch 874 loss = 0.015305\n",
      "epoch 875 loss = 0.001627\n",
      "epoch 876 loss = 0.001633\n",
      "epoch 877 loss = 0.000441\n",
      "epoch 878 loss = 0.013609\n",
      "epoch 879 loss = 0.000868\n",
      "epoch 880 loss = 0.000618\n",
      "epoch 881 loss = 0.000694\n",
      "epoch 882 loss = 0.005504\n",
      "epoch 883 loss = 0.010214\n",
      "epoch 884 loss = 0.000495\n",
      "epoch 885 loss = 0.000308\n",
      "epoch 886 loss = 0.000464\n",
      "epoch 887 loss = 0.000997\n",
      "epoch 888 loss = 0.001405\n",
      "epoch 889 loss = 0.000822\n",
      "epoch 890 loss = 0.002062\n",
      "epoch 891 loss = 0.005186\n",
      "epoch 892 loss = 0.001491\n",
      "epoch 893 loss = 0.001165\n",
      "epoch 894 loss = 0.000276\n",
      "epoch 895 loss = 0.000585\n",
      "epoch 896 loss = 0.000317\n",
      "epoch 897 loss = 0.000661\n",
      "epoch 898 loss = 0.000246\n",
      "epoch 899 loss = 0.000270\n",
      "epoch 900 loss = 0.000529\n",
      "epoch 901 loss = 0.000132\n",
      "epoch 902 loss = 0.000168\n",
      "epoch 903 loss = 0.000391\n",
      "epoch 904 loss = 0.000488\n",
      "epoch 905 loss = 0.000830\n",
      "epoch 906 loss = 0.004955\n",
      "epoch 907 loss = 0.000357\n",
      "epoch 908 loss = 0.001144\n",
      "epoch 909 loss = 0.000927\n",
      "epoch 910 loss = 0.005009\n",
      "epoch 911 loss = 0.001710\n",
      "epoch 912 loss = 0.003828\n",
      "epoch 913 loss = 0.010624\n",
      "epoch 914 loss = 0.016579\n",
      "epoch 915 loss = 0.010006\n",
      "epoch 916 loss = 0.001652\n",
      "epoch 917 loss = 0.001757\n",
      "epoch 918 loss = 0.001343\n",
      "epoch 919 loss = 0.003235\n",
      "epoch 920 loss = 0.000329\n",
      "epoch 921 loss = 0.000644\n",
      "epoch 922 loss = 0.000422\n",
      "epoch 923 loss = 0.000112\n",
      "epoch 924 loss = 0.000767\n",
      "epoch 925 loss = 0.016986\n",
      "epoch 926 loss = 0.000686\n",
      "epoch 927 loss = 0.001154\n",
      "epoch 928 loss = 0.000538\n",
      "epoch 929 loss = 0.000211\n",
      "epoch 930 loss = 0.000702\n",
      "epoch 931 loss = 0.000233\n",
      "epoch 932 loss = 0.000254\n",
      "epoch 933 loss = 0.001993\n",
      "epoch 934 loss = 0.000213\n",
      "epoch 935 loss = 0.000681\n",
      "epoch 936 loss = 0.000734\n",
      "epoch 937 loss = 0.002571\n",
      "epoch 938 loss = 0.000454\n",
      "epoch 939 loss = 0.006392\n",
      "epoch 940 loss = 0.000380\n",
      "epoch 941 loss = 0.001073\n",
      "epoch 942 loss = 0.001242\n",
      "epoch 943 loss = 0.000774\n",
      "epoch 944 loss = 0.000524\n",
      "epoch 945 loss = 0.000454\n",
      "epoch 946 loss = 0.000613\n",
      "epoch 947 loss = 0.000503\n",
      "epoch 948 loss = 0.022095\n",
      "epoch 949 loss = 0.003083\n",
      "epoch 950 loss = 0.002009\n",
      "epoch 951 loss = 0.001610\n",
      "epoch 952 loss = 0.003777\n",
      "epoch 953 loss = 0.003926\n",
      "epoch 954 loss = 0.000883\n",
      "epoch 955 loss = 0.000780\n",
      "epoch 956 loss = 0.005595\n",
      "epoch 957 loss = 0.000737\n",
      "epoch 958 loss = 0.000937\n",
      "epoch 959 loss = 0.003591\n",
      "epoch 960 loss = 0.002350\n",
      "epoch 961 loss = 0.000687\n",
      "epoch 962 loss = 0.003752\n",
      "epoch 963 loss = 0.011608\n",
      "epoch 964 loss = 0.001236\n",
      "epoch 965 loss = 0.001286\n",
      "epoch 966 loss = 0.000265\n",
      "epoch 967 loss = 0.000635\n",
      "epoch 968 loss = 0.004192\n",
      "epoch 969 loss = 0.012774\n",
      "epoch 970 loss = 0.002543\n",
      "epoch 971 loss = 0.001772\n",
      "epoch 972 loss = 0.000913\n",
      "epoch 973 loss = 0.000993\n",
      "epoch 974 loss = 0.000824\n",
      "epoch 975 loss = 0.001087\n",
      "epoch 976 loss = 0.015013\n",
      "epoch 977 loss = 0.002653\n",
      "epoch 978 loss = 0.001025\n",
      "epoch 979 loss = 0.002979\n",
      "epoch 980 loss = 0.004691\n",
      "epoch 981 loss = 0.009312\n",
      "epoch 982 loss = 0.002872\n",
      "epoch 983 loss = 0.001541\n",
      "epoch 984 loss = 0.028254\n",
      "epoch 985 loss = 0.001522\n",
      "epoch 986 loss = 0.000672\n",
      "epoch 987 loss = 0.000779\n",
      "epoch 988 loss = 0.003949\n",
      "epoch 989 loss = 0.001851\n",
      "epoch 990 loss = 0.000666\n",
      "epoch 991 loss = 0.001226\n",
      "epoch 992 loss = 0.000682\n",
      "epoch 993 loss = 0.001984\n",
      "epoch 994 loss = 0.001520\n",
      "epoch 995 loss = 0.000216\n",
      "epoch 996 loss = 0.000740\n",
      "epoch 997 loss = 0.000377\n",
      "epoch 998 loss = 0.002944\n",
      "epoch 999 loss = 0.000366\n",
      "final loss = 0.000366\n",
      "accuracy_mc = tensor(0.5127, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.5186, device='cuda:0')\n",
      "training time = 1084.6121234893799 seconds\n",
      "testing time = 0.31453394889831543 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.078600\n",
      "epoch 1 loss = 2.001798\n",
      "epoch 2 loss = 1.976221\n",
      "epoch 3 loss = 1.839749\n",
      "epoch 4 loss = 1.744136\n",
      "epoch 5 loss = 1.639323\n",
      "epoch 6 loss = 1.532778\n",
      "epoch 7 loss = 1.455901\n",
      "epoch 8 loss = 1.397124\n",
      "epoch 9 loss = 1.332218\n",
      "epoch 10 loss = 1.246116\n",
      "epoch 11 loss = 1.191498\n",
      "epoch 12 loss = 1.144328\n",
      "epoch 13 loss = 1.102763\n",
      "epoch 14 loss = 1.087079\n",
      "epoch 15 loss = 1.023208\n",
      "epoch 16 loss = 0.926594\n",
      "epoch 17 loss = 0.880722\n",
      "epoch 18 loss = 0.786300\n",
      "epoch 19 loss = 0.729702\n",
      "epoch 20 loss = 0.714956\n",
      "epoch 21 loss = 0.644209\n",
      "epoch 22 loss = 0.583839\n",
      "epoch 23 loss = 0.554255\n",
      "epoch 24 loss = 0.531690\n",
      "epoch 25 loss = 0.506005\n",
      "epoch 26 loss = 0.469930\n",
      "epoch 27 loss = 0.464161\n",
      "epoch 28 loss = 0.402544\n",
      "epoch 29 loss = 0.424871\n",
      "epoch 30 loss = 0.334397\n",
      "epoch 31 loss = 0.319309\n",
      "epoch 32 loss = 0.242927\n",
      "epoch 33 loss = 0.212983\n",
      "epoch 34 loss = 0.204382\n",
      "epoch 35 loss = 0.288229\n",
      "epoch 36 loss = 0.229353\n",
      "epoch 37 loss = 0.214554\n",
      "epoch 38 loss = 0.166867\n",
      "epoch 39 loss = 0.173147\n",
      "epoch 40 loss = 0.186348\n",
      "epoch 41 loss = 0.142689\n",
      "epoch 42 loss = 0.133057\n",
      "epoch 43 loss = 0.172916\n",
      "epoch 44 loss = 0.206481\n",
      "epoch 45 loss = 0.123769\n",
      "epoch 46 loss = 0.127452\n",
      "epoch 47 loss = 0.087617\n",
      "epoch 48 loss = 0.070479\n",
      "epoch 49 loss = 0.061699\n",
      "epoch 50 loss = 0.061320\n",
      "epoch 51 loss = 0.082286\n",
      "epoch 52 loss = 0.075503\n",
      "epoch 53 loss = 0.122326\n",
      "epoch 54 loss = 0.081467\n",
      "epoch 55 loss = 0.084230\n",
      "epoch 56 loss = 0.046546\n",
      "epoch 57 loss = 0.076864\n",
      "epoch 58 loss = 0.082905\n",
      "epoch 59 loss = 0.089434\n",
      "epoch 60 loss = 0.083916\n",
      "epoch 61 loss = 0.237778\n",
      "epoch 62 loss = 0.051592\n",
      "epoch 63 loss = 0.059691\n",
      "epoch 64 loss = 0.059361\n",
      "epoch 65 loss = 0.018579\n",
      "epoch 66 loss = 0.014577\n",
      "epoch 67 loss = 0.015388\n",
      "epoch 68 loss = 0.020741\n",
      "epoch 69 loss = 0.019912\n",
      "epoch 70 loss = 0.098364\n",
      "epoch 71 loss = 0.024929\n",
      "epoch 72 loss = 0.028939\n",
      "epoch 73 loss = 0.021139\n",
      "epoch 74 loss = 0.027211\n",
      "epoch 75 loss = 0.028307\n",
      "epoch 76 loss = 0.047906\n",
      "epoch 77 loss = 0.008193\n",
      "epoch 78 loss = 0.007236\n",
      "epoch 79 loss = 0.003935\n",
      "epoch 80 loss = 0.112050\n",
      "epoch 81 loss = 0.004503\n",
      "epoch 82 loss = 0.005457\n",
      "epoch 83 loss = 0.004331\n",
      "epoch 84 loss = 0.012855\n",
      "epoch 85 loss = 0.012137\n",
      "epoch 86 loss = 0.005953\n",
      "epoch 87 loss = 0.005843\n",
      "epoch 88 loss = 0.035305\n",
      "epoch 89 loss = 0.006863\n",
      "epoch 90 loss = 0.006046\n",
      "epoch 91 loss = 0.072370\n",
      "epoch 92 loss = 0.035640\n",
      "epoch 93 loss = 0.010317\n",
      "epoch 94 loss = 0.017306\n",
      "epoch 95 loss = 0.005792\n",
      "epoch 96 loss = 0.010580\n",
      "epoch 97 loss = 0.014239\n",
      "epoch 98 loss = 0.002694\n",
      "epoch 99 loss = 0.003858\n",
      "epoch 100 loss = 0.007547\n",
      "epoch 101 loss = 0.004302\n",
      "epoch 102 loss = 0.009007\n",
      "epoch 103 loss = 0.015987\n",
      "epoch 104 loss = 0.014211\n",
      "epoch 105 loss = 0.003800\n",
      "epoch 106 loss = 0.004106\n",
      "epoch 107 loss = 0.007509\n",
      "epoch 108 loss = 0.005706\n",
      "epoch 109 loss = 0.004388\n",
      "epoch 110 loss = 0.004160\n",
      "epoch 111 loss = 0.003807\n",
      "epoch 112 loss = 0.006484\n",
      "epoch 113 loss = 0.006948\n",
      "epoch 114 loss = 0.005517\n",
      "epoch 115 loss = 0.023299\n",
      "epoch 116 loss = 0.031816\n",
      "epoch 117 loss = 0.004451\n",
      "epoch 118 loss = 0.001559\n",
      "epoch 119 loss = 0.004797\n",
      "epoch 120 loss = 0.004426\n",
      "epoch 121 loss = 0.010011\n",
      "epoch 122 loss = 0.003975\n",
      "epoch 123 loss = 0.003009\n",
      "epoch 124 loss = 0.002741\n",
      "epoch 125 loss = 0.009107\n",
      "epoch 126 loss = 0.004952\n",
      "epoch 127 loss = 0.006463\n",
      "epoch 128 loss = 0.032131\n",
      "epoch 129 loss = 0.009255\n",
      "epoch 130 loss = 0.012941\n",
      "epoch 131 loss = 0.004081\n",
      "epoch 132 loss = 0.002743\n",
      "epoch 133 loss = 0.003486\n",
      "epoch 134 loss = 0.001345\n",
      "epoch 135 loss = 0.001729\n",
      "epoch 136 loss = 0.001401\n",
      "epoch 137 loss = 0.001683\n",
      "epoch 138 loss = 0.001599\n",
      "epoch 139 loss = 0.001824\n",
      "epoch 140 loss = 0.004988\n",
      "epoch 141 loss = 0.004038\n",
      "epoch 142 loss = 0.004226\n",
      "epoch 143 loss = 0.007754\n",
      "epoch 144 loss = 0.004375\n",
      "epoch 145 loss = 0.001906\n",
      "epoch 146 loss = 0.040415\n",
      "epoch 147 loss = 0.015889\n",
      "epoch 148 loss = 0.033802\n",
      "epoch 149 loss = 0.006945\n",
      "epoch 150 loss = 0.001222\n",
      "epoch 151 loss = 0.001685\n",
      "epoch 152 loss = 0.012500\n",
      "epoch 153 loss = 0.001624\n",
      "epoch 154 loss = 0.001712\n",
      "epoch 155 loss = 0.007632\n",
      "epoch 156 loss = 0.017211\n",
      "epoch 157 loss = 0.002679\n",
      "epoch 158 loss = 0.002027\n",
      "epoch 159 loss = 0.145795\n",
      "epoch 160 loss = 0.010375\n",
      "epoch 161 loss = 0.008162\n",
      "epoch 162 loss = 0.002143\n",
      "epoch 163 loss = 0.003693\n",
      "epoch 164 loss = 0.003860\n",
      "epoch 165 loss = 0.001507\n",
      "epoch 166 loss = 0.002516\n",
      "epoch 167 loss = 0.028386\n",
      "epoch 168 loss = 0.017034\n",
      "epoch 169 loss = 0.001355\n",
      "epoch 170 loss = 0.008447\n",
      "epoch 171 loss = 0.001663\n",
      "epoch 172 loss = 0.010032\n",
      "epoch 173 loss = 0.009520\n",
      "epoch 174 loss = 0.009061\n",
      "epoch 175 loss = 0.003816\n",
      "epoch 176 loss = 0.014068\n",
      "epoch 177 loss = 0.001736\n",
      "epoch 178 loss = 0.086649\n",
      "epoch 179 loss = 0.001862\n",
      "epoch 180 loss = 0.009799\n",
      "epoch 181 loss = 0.012601\n",
      "epoch 182 loss = 0.001022\n",
      "epoch 183 loss = 0.007874\n",
      "epoch 184 loss = 0.001579\n",
      "epoch 185 loss = 0.001612\n",
      "epoch 186 loss = 0.002247\n",
      "epoch 187 loss = 0.000525\n",
      "epoch 188 loss = 0.000838\n",
      "epoch 189 loss = 0.002614\n",
      "epoch 190 loss = 0.004048\n",
      "epoch 191 loss = 0.002938\n",
      "epoch 192 loss = 0.001495\n",
      "epoch 193 loss = 0.001047\n",
      "epoch 194 loss = 0.008092\n",
      "epoch 195 loss = 0.004915\n",
      "epoch 196 loss = 0.002063\n",
      "epoch 197 loss = 0.004045\n",
      "epoch 198 loss = 0.000522\n",
      "epoch 199 loss = 0.001644\n",
      "epoch 200 loss = 0.000691\n",
      "epoch 201 loss = 0.001186\n",
      "epoch 202 loss = 0.001483\n",
      "epoch 203 loss = 0.004270\n",
      "epoch 204 loss = 0.004548\n",
      "epoch 205 loss = 0.011266\n",
      "epoch 206 loss = 0.008762\n",
      "epoch 207 loss = 0.011118\n",
      "epoch 208 loss = 0.001780\n",
      "epoch 209 loss = 0.003214\n",
      "epoch 210 loss = 0.001772\n",
      "epoch 211 loss = 0.009083\n",
      "epoch 212 loss = 0.001281\n",
      "epoch 213 loss = 0.009751\n",
      "epoch 214 loss = 0.001517\n",
      "epoch 215 loss = 0.000906\n",
      "epoch 216 loss = 0.008481\n",
      "epoch 217 loss = 0.000589\n",
      "epoch 218 loss = 0.002340\n",
      "epoch 219 loss = 0.001966\n",
      "epoch 220 loss = 0.001738\n",
      "epoch 221 loss = 0.012754\n",
      "epoch 222 loss = 0.001698\n",
      "epoch 223 loss = 0.005156\n",
      "epoch 224 loss = 0.008977\n",
      "epoch 225 loss = 0.002009\n",
      "epoch 226 loss = 0.001893\n",
      "epoch 227 loss = 0.001611\n",
      "epoch 228 loss = 0.005169\n",
      "epoch 229 loss = 0.000732\n",
      "epoch 230 loss = 0.003350\n",
      "epoch 231 loss = 0.002727\n",
      "epoch 232 loss = 0.002919\n",
      "epoch 233 loss = 0.001009\n",
      "epoch 234 loss = 0.006274\n",
      "epoch 235 loss = 0.002010\n",
      "epoch 236 loss = 0.002617\n",
      "epoch 237 loss = 0.015378\n",
      "epoch 238 loss = 0.061406\n",
      "epoch 239 loss = 0.020739\n",
      "epoch 240 loss = 0.003678\n",
      "epoch 241 loss = 0.002482\n",
      "epoch 242 loss = 0.000699\n",
      "epoch 243 loss = 0.002189\n",
      "epoch 244 loss = 0.000503\n",
      "epoch 245 loss = 0.004474\n",
      "epoch 246 loss = 0.002231\n",
      "epoch 247 loss = 0.002086\n",
      "epoch 248 loss = 0.004106\n",
      "epoch 249 loss = 0.001885\n",
      "epoch 250 loss = 0.005129\n",
      "epoch 251 loss = 0.001329\n",
      "epoch 252 loss = 0.000901\n",
      "epoch 253 loss = 0.001132\n",
      "epoch 254 loss = 0.000491\n",
      "epoch 255 loss = 0.001621\n",
      "epoch 256 loss = 0.001364\n",
      "epoch 257 loss = 0.001417\n",
      "epoch 258 loss = 0.007883\n",
      "epoch 259 loss = 0.002014\n",
      "epoch 260 loss = 0.002016\n",
      "epoch 261 loss = 0.002821\n",
      "epoch 262 loss = 0.006114\n",
      "epoch 263 loss = 0.002338\n",
      "epoch 264 loss = 0.001046\n",
      "epoch 265 loss = 0.001532\n",
      "epoch 266 loss = 0.002140\n",
      "epoch 267 loss = 0.002482\n",
      "epoch 268 loss = 0.039354\n",
      "epoch 269 loss = 0.009649\n",
      "epoch 270 loss = 0.002178\n",
      "epoch 271 loss = 0.003265\n",
      "epoch 272 loss = 0.005346\n",
      "epoch 273 loss = 0.001388\n",
      "epoch 274 loss = 0.003124\n",
      "epoch 275 loss = 0.001708\n",
      "epoch 276 loss = 0.014824\n",
      "epoch 277 loss = 0.001402\n",
      "epoch 278 loss = 0.009565\n",
      "epoch 279 loss = 0.000627\n",
      "epoch 280 loss = 0.000978\n",
      "epoch 281 loss = 0.003514\n",
      "epoch 282 loss = 0.001880\n",
      "epoch 283 loss = 0.003228\n",
      "epoch 284 loss = 0.013966\n",
      "epoch 285 loss = 0.005759\n",
      "epoch 286 loss = 0.001698\n",
      "epoch 287 loss = 0.004977\n",
      "epoch 288 loss = 0.001922\n",
      "epoch 289 loss = 0.006589\n",
      "epoch 290 loss = 0.002370\n",
      "epoch 291 loss = 0.003007\n",
      "epoch 292 loss = 0.000712\n",
      "epoch 293 loss = 0.000387\n",
      "epoch 294 loss = 0.001067\n",
      "epoch 295 loss = 0.006656\n",
      "epoch 296 loss = 0.005643\n",
      "epoch 297 loss = 0.004354\n",
      "epoch 298 loss = 0.002883\n",
      "epoch 299 loss = 0.001296\n",
      "epoch 300 loss = 0.003240\n",
      "epoch 301 loss = 0.001863\n",
      "epoch 302 loss = 0.001121\n",
      "epoch 303 loss = 0.001510\n",
      "epoch 304 loss = 0.000712\n",
      "epoch 305 loss = 0.002718\n",
      "epoch 306 loss = 0.003424\n",
      "epoch 307 loss = 0.000439\n",
      "epoch 308 loss = 0.001840\n",
      "epoch 309 loss = 0.002387\n",
      "epoch 310 loss = 0.010797\n",
      "epoch 311 loss = 0.002352\n",
      "epoch 312 loss = 0.003483\n",
      "epoch 313 loss = 0.001516\n",
      "epoch 314 loss = 0.004295\n",
      "epoch 315 loss = 0.005871\n",
      "epoch 316 loss = 0.002553\n",
      "epoch 317 loss = 0.036894\n",
      "epoch 318 loss = 0.010132\n",
      "epoch 319 loss = 0.002049\n",
      "epoch 320 loss = 0.005584\n",
      "epoch 321 loss = 0.001337\n",
      "epoch 322 loss = 0.001060\n",
      "epoch 323 loss = 0.000656\n",
      "epoch 324 loss = 0.002745\n",
      "epoch 325 loss = 0.024588\n",
      "epoch 326 loss = 0.027585\n",
      "epoch 327 loss = 0.001001\n",
      "epoch 328 loss = 0.003389\n",
      "epoch 329 loss = 0.001040\n",
      "epoch 330 loss = 0.001412\n",
      "epoch 331 loss = 0.006584\n",
      "epoch 332 loss = 0.013602\n",
      "epoch 333 loss = 0.001111\n",
      "epoch 334 loss = 0.003271\n",
      "epoch 335 loss = 0.002043\n",
      "epoch 336 loss = 0.008702\n",
      "epoch 337 loss = 0.001840\n",
      "epoch 338 loss = 0.001009\n",
      "epoch 339 loss = 0.002320\n",
      "epoch 340 loss = 0.000562\n",
      "epoch 341 loss = 0.000444\n",
      "epoch 342 loss = 0.001309\n",
      "epoch 343 loss = 0.001350\n",
      "epoch 344 loss = 0.002833\n",
      "epoch 345 loss = 0.001435\n",
      "epoch 346 loss = 0.001858\n",
      "epoch 347 loss = 0.001140\n",
      "epoch 348 loss = 0.000926\n",
      "epoch 349 loss = 0.005997\n",
      "epoch 350 loss = 0.008370\n",
      "epoch 351 loss = 0.108322\n",
      "epoch 352 loss = 0.006113\n",
      "epoch 353 loss = 0.003643\n",
      "epoch 354 loss = 0.007760\n",
      "epoch 355 loss = 0.003416\n",
      "epoch 356 loss = 0.002948\n",
      "epoch 357 loss = 0.004133\n",
      "epoch 358 loss = 0.000920\n",
      "epoch 359 loss = 0.001250\n",
      "epoch 360 loss = 0.003667\n",
      "epoch 361 loss = 0.002187\n",
      "epoch 362 loss = 0.001879\n",
      "epoch 363 loss = 0.002326\n",
      "epoch 364 loss = 0.000400\n",
      "epoch 365 loss = 0.000863\n",
      "epoch 366 loss = 0.013758\n",
      "epoch 367 loss = 0.001421\n",
      "epoch 368 loss = 0.000667\n",
      "epoch 369 loss = 0.001272\n",
      "epoch 370 loss = 0.004099\n",
      "epoch 371 loss = 0.002487\n",
      "epoch 372 loss = 0.001091\n",
      "epoch 373 loss = 0.000894\n",
      "epoch 374 loss = 0.000592\n",
      "epoch 375 loss = 0.023296\n",
      "epoch 376 loss = 0.004921\n",
      "epoch 377 loss = 0.008657\n",
      "epoch 378 loss = 0.001618\n",
      "epoch 379 loss = 0.005441\n",
      "epoch 380 loss = 0.005342\n",
      "epoch 381 loss = 0.016983\n",
      "epoch 382 loss = 0.005083\n",
      "epoch 383 loss = 0.005566\n",
      "epoch 384 loss = 0.002498\n",
      "epoch 385 loss = 0.001523\n",
      "epoch 386 loss = 0.001869\n",
      "epoch 387 loss = 0.001503\n",
      "epoch 388 loss = 0.001494\n",
      "epoch 389 loss = 0.006270\n",
      "epoch 390 loss = 0.002855\n",
      "epoch 391 loss = 0.022817\n",
      "epoch 392 loss = 0.001812\n",
      "epoch 393 loss = 0.001570\n",
      "epoch 394 loss = 0.000690\n",
      "epoch 395 loss = 0.002416\n",
      "epoch 396 loss = 0.004676\n",
      "epoch 397 loss = 0.000711\n",
      "epoch 398 loss = 0.001489\n",
      "epoch 399 loss = 0.001096\n",
      "epoch 400 loss = 0.001484\n",
      "epoch 401 loss = 0.013330\n",
      "epoch 402 loss = 0.000839\n",
      "epoch 403 loss = 0.000628\n",
      "epoch 404 loss = 0.001363\n",
      "epoch 405 loss = 0.000879\n",
      "epoch 406 loss = 0.000743\n",
      "epoch 407 loss = 0.000798\n",
      "epoch 408 loss = 0.001102\n",
      "epoch 409 loss = 0.001066\n",
      "epoch 410 loss = 0.047421\n",
      "epoch 411 loss = 0.003704\n",
      "epoch 412 loss = 0.003426\n",
      "epoch 413 loss = 0.000540\n",
      "epoch 414 loss = 0.013937\n",
      "epoch 415 loss = 0.004784\n",
      "epoch 416 loss = 0.013019\n",
      "epoch 417 loss = 0.002449\n",
      "epoch 418 loss = 0.002448\n",
      "epoch 419 loss = 0.006898\n",
      "epoch 420 loss = 0.000315\n",
      "epoch 421 loss = 0.000973\n",
      "epoch 422 loss = 0.000411\n",
      "epoch 423 loss = 0.000803\n",
      "epoch 424 loss = 0.000572\n",
      "epoch 425 loss = 0.004675\n",
      "epoch 426 loss = 0.000440\n",
      "epoch 427 loss = 0.005919\n",
      "epoch 428 loss = 0.001209\n",
      "epoch 429 loss = 0.000234\n",
      "epoch 430 loss = 0.001102\n",
      "epoch 431 loss = 0.000370\n",
      "epoch 432 loss = 0.005762\n",
      "epoch 433 loss = 0.000799\n",
      "epoch 434 loss = 0.011423\n",
      "epoch 435 loss = 0.006317\n",
      "epoch 436 loss = 0.000621\n",
      "epoch 437 loss = 0.002501\n",
      "epoch 438 loss = 0.000985\n",
      "epoch 439 loss = 0.001418\n",
      "epoch 440 loss = 0.000973\n",
      "epoch 441 loss = 0.009845\n",
      "epoch 442 loss = 0.000973\n",
      "epoch 443 loss = 0.001600\n",
      "epoch 444 loss = 0.002708\n",
      "epoch 445 loss = 0.000939\n",
      "epoch 446 loss = 0.003950\n",
      "epoch 447 loss = 0.003410\n",
      "epoch 448 loss = 0.000737\n",
      "epoch 449 loss = 0.000405\n",
      "epoch 450 loss = 0.001856\n",
      "epoch 451 loss = 0.002107\n",
      "epoch 452 loss = 0.002153\n",
      "epoch 453 loss = 0.002018\n",
      "epoch 454 loss = 0.003973\n",
      "epoch 455 loss = 0.020680\n",
      "epoch 456 loss = 0.000543\n",
      "epoch 457 loss = 0.002975\n",
      "epoch 458 loss = 0.001055\n",
      "epoch 459 loss = 0.009581\n",
      "epoch 460 loss = 0.005531\n",
      "epoch 461 loss = 0.115427\n",
      "epoch 462 loss = 0.008665\n",
      "epoch 463 loss = 0.030866\n",
      "epoch 464 loss = 0.069012\n",
      "epoch 465 loss = 0.002199\n",
      "epoch 466 loss = 0.000753\n",
      "epoch 467 loss = 0.001026\n",
      "epoch 468 loss = 0.000637\n",
      "epoch 469 loss = 0.000707\n",
      "epoch 470 loss = 0.002536\n",
      "epoch 471 loss = 0.000715\n",
      "epoch 472 loss = 0.013567\n",
      "epoch 473 loss = 0.001075\n",
      "epoch 474 loss = 0.000789\n",
      "epoch 475 loss = 0.013803\n",
      "epoch 476 loss = 0.003433\n",
      "epoch 477 loss = 0.005481\n",
      "epoch 478 loss = 0.000787\n",
      "epoch 479 loss = 0.004744\n",
      "epoch 480 loss = 0.001687\n",
      "epoch 481 loss = 0.002224\n",
      "epoch 482 loss = 0.007024\n",
      "epoch 483 loss = 0.003397\n",
      "epoch 484 loss = 0.001445\n",
      "epoch 485 loss = 0.004407\n",
      "epoch 486 loss = 0.004981\n",
      "epoch 487 loss = 0.000844\n",
      "epoch 488 loss = 0.001116\n",
      "epoch 489 loss = 0.002911\n",
      "epoch 490 loss = 0.002075\n",
      "epoch 491 loss = 0.000469\n",
      "epoch 492 loss = 0.003166\n",
      "epoch 493 loss = 0.000387\n",
      "epoch 494 loss = 0.001413\n",
      "epoch 495 loss = 0.038470\n",
      "epoch 496 loss = 0.001042\n",
      "epoch 497 loss = 0.000994\n",
      "epoch 498 loss = 0.002300\n",
      "epoch 499 loss = 0.000854\n",
      "epoch 500 loss = 0.006887\n",
      "epoch 501 loss = 0.000687\n",
      "epoch 502 loss = 0.010593\n",
      "epoch 503 loss = 0.001405\n",
      "epoch 504 loss = 0.001102\n",
      "epoch 505 loss = 0.002777\n",
      "epoch 506 loss = 0.000528\n",
      "epoch 507 loss = 0.001475\n",
      "epoch 508 loss = 0.003690\n",
      "epoch 509 loss = 0.001725\n",
      "epoch 510 loss = 0.001258\n",
      "epoch 511 loss = 0.007737\n",
      "epoch 512 loss = 0.003109\n",
      "epoch 513 loss = 0.000927\n",
      "epoch 514 loss = 0.002254\n",
      "epoch 515 loss = 0.003029\n",
      "epoch 516 loss = 0.001656\n",
      "epoch 517 loss = 0.001139\n",
      "epoch 518 loss = 0.003541\n",
      "epoch 519 loss = 0.003444\n",
      "epoch 520 loss = 0.015171\n",
      "epoch 521 loss = 0.007220\n",
      "epoch 522 loss = 0.001356\n",
      "epoch 523 loss = 0.000608\n",
      "epoch 524 loss = 0.000388\n",
      "epoch 525 loss = 0.001156\n",
      "epoch 526 loss = 0.012236\n",
      "epoch 527 loss = 0.001704\n",
      "epoch 528 loss = 0.002534\n",
      "epoch 529 loss = 0.000684\n",
      "epoch 530 loss = 0.001238\n",
      "epoch 531 loss = 0.002216\n",
      "epoch 532 loss = 0.002849\n",
      "epoch 533 loss = 0.002537\n",
      "epoch 534 loss = 0.001255\n",
      "epoch 535 loss = 0.002392\n",
      "epoch 536 loss = 0.019901\n",
      "epoch 537 loss = 0.114711\n",
      "epoch 538 loss = 0.004543\n",
      "epoch 539 loss = 0.001441\n",
      "epoch 540 loss = 0.000647\n",
      "epoch 541 loss = 0.005076\n",
      "epoch 542 loss = 0.000678\n",
      "epoch 543 loss = 0.000674\n",
      "epoch 544 loss = 0.005478\n",
      "epoch 545 loss = 0.000615\n",
      "epoch 546 loss = 0.000361\n",
      "epoch 547 loss = 0.000942\n",
      "epoch 548 loss = 0.001709\n",
      "epoch 549 loss = 0.002161\n",
      "epoch 550 loss = 0.001021\n",
      "epoch 551 loss = 0.001048\n",
      "epoch 552 loss = 0.008421\n",
      "epoch 553 loss = 0.003403\n",
      "epoch 554 loss = 0.010115\n",
      "epoch 555 loss = 0.001537\n",
      "epoch 556 loss = 0.004951\n",
      "epoch 557 loss = 0.003466\n",
      "epoch 558 loss = 0.001088\n",
      "epoch 559 loss = 0.002923\n",
      "epoch 560 loss = 0.003744\n",
      "epoch 561 loss = 0.001057\n",
      "epoch 562 loss = 0.000345\n",
      "epoch 563 loss = 0.000247\n",
      "epoch 564 loss = 0.003198\n",
      "epoch 565 loss = 0.001137\n",
      "epoch 566 loss = 0.000434\n",
      "epoch 567 loss = 0.000533\n",
      "epoch 568 loss = 0.002160\n",
      "epoch 569 loss = 0.025073\n",
      "epoch 570 loss = 0.001817\n",
      "epoch 571 loss = 0.002702\n",
      "epoch 572 loss = 0.001184\n",
      "epoch 573 loss = 0.001027\n",
      "epoch 574 loss = 0.010077\n",
      "epoch 575 loss = 0.002669\n",
      "epoch 576 loss = 0.009923\n",
      "epoch 577 loss = 0.000910\n",
      "epoch 578 loss = 0.000798\n",
      "epoch 579 loss = 0.000562\n",
      "epoch 580 loss = 0.001243\n",
      "epoch 581 loss = 0.002904\n",
      "epoch 582 loss = 0.000359\n",
      "epoch 583 loss = 0.006706\n",
      "epoch 584 loss = 0.001532\n",
      "epoch 585 loss = 0.001389\n",
      "epoch 586 loss = 0.020194\n",
      "epoch 587 loss = 0.001428\n",
      "epoch 588 loss = 0.001402\n",
      "epoch 589 loss = 0.003399\n",
      "epoch 590 loss = 0.002265\n",
      "epoch 591 loss = 0.019328\n",
      "epoch 592 loss = 0.004482\n",
      "epoch 593 loss = 0.001133\n",
      "epoch 594 loss = 0.001731\n",
      "epoch 595 loss = 0.000960\n",
      "epoch 596 loss = 0.001636\n",
      "epoch 597 loss = 0.001562\n",
      "epoch 598 loss = 0.002587\n",
      "epoch 599 loss = 0.006244\n",
      "epoch 600 loss = 0.004066\n",
      "epoch 601 loss = 0.000823\n",
      "epoch 602 loss = 0.000453\n",
      "epoch 603 loss = 0.002428\n",
      "epoch 604 loss = 0.001686\n",
      "epoch 605 loss = 0.001069\n",
      "epoch 606 loss = 0.001177\n",
      "epoch 607 loss = 0.000574\n",
      "epoch 608 loss = 0.000590\n",
      "epoch 609 loss = 0.000667\n",
      "epoch 610 loss = 0.000198\n",
      "epoch 611 loss = 0.000616\n",
      "epoch 612 loss = 0.000582\n",
      "epoch 613 loss = 0.000386\n",
      "epoch 614 loss = 0.001679\n",
      "epoch 615 loss = 0.002077\n",
      "epoch 616 loss = 0.001360\n",
      "epoch 617 loss = 0.000570\n",
      "epoch 618 loss = 0.005209\n",
      "epoch 619 loss = 0.010773\n",
      "epoch 620 loss = 0.009197\n",
      "epoch 621 loss = 0.002855\n",
      "epoch 622 loss = 0.040252\n",
      "epoch 623 loss = 0.003808\n",
      "epoch 624 loss = 0.024823\n",
      "epoch 625 loss = 0.001736\n",
      "epoch 626 loss = 0.001010\n",
      "epoch 627 loss = 0.003526\n",
      "epoch 628 loss = 0.001423\n",
      "epoch 629 loss = 0.000485\n",
      "epoch 630 loss = 0.000798\n",
      "epoch 631 loss = 0.002173\n",
      "epoch 632 loss = 0.000665\n",
      "epoch 633 loss = 0.000607\n",
      "epoch 634 loss = 0.000281\n",
      "epoch 635 loss = 0.000408\n",
      "epoch 636 loss = 0.000868\n",
      "epoch 637 loss = 0.000822\n",
      "epoch 638 loss = 0.002911\n",
      "epoch 639 loss = 0.001191\n",
      "epoch 640 loss = 0.002813\n",
      "epoch 641 loss = 0.002931\n",
      "epoch 642 loss = 0.000631\n",
      "epoch 643 loss = 0.001326\n",
      "epoch 644 loss = 0.000827\n",
      "epoch 645 loss = 0.007021\n",
      "epoch 646 loss = 0.001278\n",
      "epoch 647 loss = 0.001446\n",
      "epoch 648 loss = 0.003849\n",
      "epoch 649 loss = 0.003363\n",
      "epoch 650 loss = 0.001406\n",
      "epoch 651 loss = 0.000331\n",
      "epoch 652 loss = 0.002133\n",
      "epoch 653 loss = 0.007428\n",
      "epoch 654 loss = 0.011531\n",
      "epoch 655 loss = 0.001466\n",
      "epoch 656 loss = 0.001564\n",
      "epoch 657 loss = 0.004803\n",
      "epoch 658 loss = 0.009432\n",
      "epoch 659 loss = 0.000856\n",
      "epoch 660 loss = 0.015743\n",
      "epoch 661 loss = 0.004681\n",
      "epoch 662 loss = 0.003278\n",
      "epoch 663 loss = 0.003026\n",
      "epoch 664 loss = 0.000883\n",
      "epoch 665 loss = 0.000765\n",
      "epoch 666 loss = 0.004425\n",
      "epoch 667 loss = 0.002400\n",
      "epoch 668 loss = 0.000509\n",
      "epoch 669 loss = 0.002370\n",
      "epoch 670 loss = 0.000701\n",
      "epoch 671 loss = 0.001490\n",
      "epoch 672 loss = 0.013901\n",
      "epoch 673 loss = 0.001674\n",
      "epoch 674 loss = 0.001111\n",
      "epoch 675 loss = 0.002197\n",
      "epoch 676 loss = 0.003518\n",
      "epoch 677 loss = 0.001308\n",
      "epoch 678 loss = 0.010762\n",
      "epoch 679 loss = 0.007072\n",
      "epoch 680 loss = 0.122690\n",
      "epoch 681 loss = 0.004009\n",
      "epoch 682 loss = 0.003581\n",
      "epoch 683 loss = 0.001247\n",
      "epoch 684 loss = 0.006092\n",
      "epoch 685 loss = 0.002510\n",
      "epoch 686 loss = 0.000976\n",
      "epoch 687 loss = 0.002404\n",
      "epoch 688 loss = 0.000969\n",
      "epoch 689 loss = 0.000620\n",
      "epoch 690 loss = 0.001212\n",
      "epoch 691 loss = 0.001382\n",
      "epoch 692 loss = 0.000862\n",
      "epoch 693 loss = 0.001205\n",
      "epoch 694 loss = 0.011535\n",
      "epoch 695 loss = 0.000958\n",
      "epoch 696 loss = 0.001231\n",
      "epoch 697 loss = 0.000712\n",
      "epoch 698 loss = 0.008321\n",
      "epoch 699 loss = 0.000806\n",
      "epoch 700 loss = 0.001212\n",
      "epoch 701 loss = 0.000442\n",
      "epoch 702 loss = 0.000300\n",
      "epoch 703 loss = 0.000597\n",
      "epoch 704 loss = 0.002054\n",
      "epoch 705 loss = 0.000426\n",
      "epoch 706 loss = 0.000282\n",
      "epoch 707 loss = 0.001402\n",
      "epoch 708 loss = 0.001407\n",
      "epoch 709 loss = 0.002535\n",
      "epoch 710 loss = 0.009703\n",
      "epoch 711 loss = 0.012716\n",
      "epoch 712 loss = 0.002533\n",
      "epoch 713 loss = 0.002964\n",
      "epoch 714 loss = 0.031379\n",
      "epoch 715 loss = 0.015408\n",
      "epoch 716 loss = 0.001476\n",
      "epoch 717 loss = 0.010998\n",
      "epoch 718 loss = 0.000887\n",
      "epoch 719 loss = 0.001180\n",
      "epoch 720 loss = 0.000679\n",
      "epoch 721 loss = 0.000632\n",
      "epoch 722 loss = 0.000290\n",
      "epoch 723 loss = 0.000199\n",
      "epoch 724 loss = 0.003511\n",
      "epoch 725 loss = 0.001459\n",
      "epoch 726 loss = 0.000289\n",
      "epoch 727 loss = 0.002414\n",
      "epoch 728 loss = 0.000463\n",
      "epoch 729 loss = 0.005076\n",
      "epoch 730 loss = 0.001134\n",
      "epoch 731 loss = 0.000321\n",
      "epoch 732 loss = 0.000307\n",
      "epoch 733 loss = 0.000477\n",
      "epoch 734 loss = 0.000342\n",
      "epoch 735 loss = 0.000643\n",
      "epoch 736 loss = 0.006086\n",
      "epoch 737 loss = 0.000742\n",
      "epoch 738 loss = 0.004623\n",
      "epoch 739 loss = 0.002260\n",
      "epoch 740 loss = 0.001140\n",
      "epoch 741 loss = 0.000646\n",
      "epoch 742 loss = 0.001846\n",
      "epoch 743 loss = 0.001267\n",
      "epoch 744 loss = 0.001928\n",
      "epoch 745 loss = 0.000639\n",
      "epoch 746 loss = 0.002267\n",
      "epoch 747 loss = 0.001383\n",
      "epoch 748 loss = 0.014626\n",
      "epoch 749 loss = 0.004972\n",
      "epoch 750 loss = 0.008994\n",
      "epoch 751 loss = 0.011174\n",
      "epoch 752 loss = 0.000380\n",
      "epoch 753 loss = 0.000593\n",
      "epoch 754 loss = 0.000595\n",
      "epoch 755 loss = 0.000888\n",
      "epoch 756 loss = 0.000613\n",
      "epoch 757 loss = 0.003156\n",
      "epoch 758 loss = 0.177174\n",
      "epoch 759 loss = 0.009321\n",
      "epoch 760 loss = 0.004047\n",
      "epoch 761 loss = 0.004993\n",
      "epoch 762 loss = 0.002921\n",
      "epoch 763 loss = 0.011375\n",
      "epoch 764 loss = 0.001970\n",
      "epoch 765 loss = 0.000691\n",
      "epoch 766 loss = 0.008676\n",
      "epoch 767 loss = 0.000273\n",
      "epoch 768 loss = 0.000482\n",
      "epoch 769 loss = 0.000669\n",
      "epoch 770 loss = 0.002253\n",
      "epoch 771 loss = 0.001169\n",
      "epoch 772 loss = 0.001487\n",
      "epoch 773 loss = 0.000982\n",
      "epoch 774 loss = 0.001409\n",
      "epoch 775 loss = 0.006715\n",
      "epoch 776 loss = 0.000673\n",
      "epoch 777 loss = 0.012993\n",
      "epoch 778 loss = 0.000699\n",
      "epoch 779 loss = 0.000535\n",
      "epoch 780 loss = 0.002766\n",
      "epoch 781 loss = 0.001536\n",
      "epoch 782 loss = 0.001695\n",
      "epoch 783 loss = 0.001305\n",
      "epoch 784 loss = 0.003012\n",
      "epoch 785 loss = 0.001753\n",
      "epoch 786 loss = 0.000971\n",
      "epoch 787 loss = 0.004252\n",
      "epoch 788 loss = 0.000862\n",
      "epoch 789 loss = 0.005626\n",
      "epoch 790 loss = 0.001184\n",
      "epoch 791 loss = 0.001851\n",
      "epoch 792 loss = 0.001849\n",
      "epoch 793 loss = 0.001534\n",
      "epoch 794 loss = 0.007521\n",
      "epoch 795 loss = 0.001462\n",
      "epoch 796 loss = 0.002733\n",
      "epoch 797 loss = 0.003071\n",
      "epoch 798 loss = 0.002649\n",
      "epoch 799 loss = 0.009563\n",
      "epoch 800 loss = 0.000539\n",
      "epoch 801 loss = 0.000752\n",
      "epoch 802 loss = 0.000206\n",
      "epoch 803 loss = 0.000420\n",
      "epoch 804 loss = 0.000647\n",
      "epoch 805 loss = 0.006651\n",
      "epoch 806 loss = 0.002538\n",
      "epoch 807 loss = 0.006500\n",
      "epoch 808 loss = 0.007757\n",
      "epoch 809 loss = 0.158827\n",
      "epoch 810 loss = 0.002170\n",
      "epoch 811 loss = 0.003595\n",
      "epoch 812 loss = 0.001340\n",
      "epoch 813 loss = 0.001738\n",
      "epoch 814 loss = 0.003561\n",
      "epoch 815 loss = 0.014295\n",
      "epoch 816 loss = 0.002449\n",
      "epoch 817 loss = 0.021763\n",
      "epoch 818 loss = 0.001188\n",
      "epoch 819 loss = 0.001244\n",
      "epoch 820 loss = 0.004190\n",
      "epoch 821 loss = 0.000912\n",
      "epoch 822 loss = 0.000547\n",
      "epoch 823 loss = 0.017791\n",
      "epoch 824 loss = 0.004396\n",
      "epoch 825 loss = 0.001245\n",
      "epoch 826 loss = 0.000514\n",
      "epoch 827 loss = 0.000711\n",
      "epoch 828 loss = 0.001544\n",
      "epoch 829 loss = 0.000688\n",
      "epoch 830 loss = 0.000951\n",
      "epoch 831 loss = 0.012263\n",
      "epoch 832 loss = 0.002451\n",
      "epoch 833 loss = 0.006107\n",
      "epoch 834 loss = 0.004897\n",
      "epoch 835 loss = 0.001550\n",
      "epoch 836 loss = 0.000659\n",
      "epoch 837 loss = 0.000419\n",
      "epoch 838 loss = 0.004314\n",
      "epoch 839 loss = 0.001126\n",
      "epoch 840 loss = 0.003814\n",
      "epoch 841 loss = 0.003887\n",
      "epoch 842 loss = 0.001018\n",
      "epoch 843 loss = 0.001704\n",
      "epoch 844 loss = 0.008773\n",
      "epoch 845 loss = 0.000701\n",
      "epoch 846 loss = 0.001266\n",
      "epoch 847 loss = 0.000526\n",
      "epoch 848 loss = 0.000732\n",
      "epoch 849 loss = 0.000897\n",
      "epoch 850 loss = 0.000545\n",
      "epoch 851 loss = 0.001625\n",
      "epoch 852 loss = 0.001860\n",
      "epoch 853 loss = 0.000386\n",
      "epoch 854 loss = 0.003321\n",
      "epoch 855 loss = 0.001432\n",
      "epoch 856 loss = 0.009095\n",
      "epoch 857 loss = 0.002655\n",
      "epoch 858 loss = 0.000769\n",
      "epoch 859 loss = 0.003509\n",
      "epoch 860 loss = 0.001136\n",
      "epoch 861 loss = 0.001511\n",
      "epoch 862 loss = 0.000548\n",
      "epoch 863 loss = 0.000947\n",
      "epoch 864 loss = 0.000510\n",
      "epoch 865 loss = 0.001495\n",
      "epoch 866 loss = 0.002217\n",
      "epoch 867 loss = 0.000567\n",
      "epoch 868 loss = 0.027658\n",
      "epoch 869 loss = 0.006558\n",
      "epoch 870 loss = 0.001791\n",
      "epoch 871 loss = 0.002510\n",
      "epoch 872 loss = 0.021142\n",
      "epoch 873 loss = 0.001399\n",
      "epoch 874 loss = 0.001050\n",
      "epoch 875 loss = 0.000423\n",
      "epoch 876 loss = 0.002292\n",
      "epoch 877 loss = 0.000615\n",
      "epoch 878 loss = 0.003092\n",
      "epoch 879 loss = 0.005561\n",
      "epoch 880 loss = 0.000617\n",
      "epoch 881 loss = 0.006349\n",
      "epoch 882 loss = 0.001042\n",
      "epoch 883 loss = 0.001590\n",
      "epoch 884 loss = 0.003301\n",
      "epoch 885 loss = 0.003034\n",
      "epoch 886 loss = 0.015709\n",
      "epoch 887 loss = 0.004205\n",
      "epoch 888 loss = 0.002118\n",
      "epoch 889 loss = 0.000775\n",
      "epoch 890 loss = 0.002771\n",
      "epoch 891 loss = 0.006369\n",
      "epoch 892 loss = 0.009492\n",
      "epoch 893 loss = 0.002834\n",
      "epoch 894 loss = 0.002110\n",
      "epoch 895 loss = 0.004909\n",
      "epoch 896 loss = 0.003896\n",
      "epoch 897 loss = 0.028069\n",
      "epoch 898 loss = 0.002410\n",
      "epoch 899 loss = 0.008345\n",
      "epoch 900 loss = 0.037579\n",
      "epoch 901 loss = 0.002487\n",
      "epoch 902 loss = 0.003646\n",
      "epoch 903 loss = 0.000267\n",
      "epoch 904 loss = 0.000315\n",
      "epoch 905 loss = 0.000208\n",
      "epoch 906 loss = 0.001232\n",
      "epoch 907 loss = 0.000375\n",
      "epoch 908 loss = 0.003347\n",
      "epoch 909 loss = 0.000401\n",
      "epoch 910 loss = 0.000266\n",
      "epoch 911 loss = 0.001988\n",
      "epoch 912 loss = 0.000656\n",
      "epoch 913 loss = 0.001197\n",
      "epoch 914 loss = 0.000622\n",
      "epoch 915 loss = 0.000677\n",
      "epoch 916 loss = 0.000391\n",
      "epoch 917 loss = 0.000865\n",
      "epoch 918 loss = 0.000732\n",
      "epoch 919 loss = 0.001345\n",
      "epoch 920 loss = 0.000218\n",
      "epoch 921 loss = 0.001562\n",
      "epoch 922 loss = 0.000853\n",
      "epoch 923 loss = 0.000272\n",
      "epoch 924 loss = 0.000463\n",
      "epoch 925 loss = 0.000630\n",
      "epoch 926 loss = 0.000508\n",
      "epoch 927 loss = 0.000483\n",
      "epoch 928 loss = 0.003655\n",
      "epoch 929 loss = 0.000550\n",
      "epoch 930 loss = 0.004568\n",
      "epoch 931 loss = 0.004430\n",
      "epoch 932 loss = 0.002505\n",
      "epoch 933 loss = 0.022602\n",
      "epoch 934 loss = 0.004746\n",
      "epoch 935 loss = 0.001154\n",
      "epoch 936 loss = 0.000281\n",
      "epoch 937 loss = 0.005227\n",
      "epoch 938 loss = 0.001614\n",
      "epoch 939 loss = 0.002758\n",
      "epoch 940 loss = 0.001668\n",
      "epoch 941 loss = 0.001468\n",
      "epoch 942 loss = 0.000581\n",
      "epoch 943 loss = 0.002177\n",
      "epoch 944 loss = 0.000770\n",
      "epoch 945 loss = 0.000744\n",
      "epoch 946 loss = 0.000827\n",
      "epoch 947 loss = 0.000776\n",
      "epoch 948 loss = 0.002659\n",
      "epoch 949 loss = 0.002294\n",
      "epoch 950 loss = 0.001328\n",
      "epoch 951 loss = 0.002390\n",
      "epoch 952 loss = 0.025526\n",
      "epoch 953 loss = 0.000669\n",
      "epoch 954 loss = 0.007738\n",
      "epoch 955 loss = 0.000996\n",
      "epoch 956 loss = 0.000400\n",
      "epoch 957 loss = 0.000335\n",
      "epoch 958 loss = 0.000335\n",
      "epoch 959 loss = 0.001696\n",
      "epoch 960 loss = 0.000755\n",
      "epoch 961 loss = 0.000633\n",
      "epoch 962 loss = 0.076906\n",
      "epoch 963 loss = 0.035101\n",
      "epoch 964 loss = 0.002918\n",
      "epoch 965 loss = 0.009538\n",
      "epoch 966 loss = 0.007209\n",
      "epoch 967 loss = 0.002775\n",
      "epoch 968 loss = 0.004561\n",
      "epoch 969 loss = 0.001131\n",
      "epoch 970 loss = 0.003618\n",
      "epoch 971 loss = 0.000670\n",
      "epoch 972 loss = 0.000730\n",
      "epoch 973 loss = 0.012074\n",
      "epoch 974 loss = 0.001535\n",
      "epoch 975 loss = 0.001907\n",
      "epoch 976 loss = 0.005286\n",
      "epoch 977 loss = 0.007886\n",
      "epoch 978 loss = 0.000421\n",
      "epoch 979 loss = 0.001152\n",
      "epoch 980 loss = 0.000917\n",
      "epoch 981 loss = 0.000553\n",
      "epoch 982 loss = 0.000245\n",
      "epoch 983 loss = 0.000411\n",
      "epoch 984 loss = 0.000215\n",
      "epoch 985 loss = 0.000897\n",
      "epoch 986 loss = 0.001307\n",
      "epoch 987 loss = 0.000642\n",
      "epoch 988 loss = 0.000907\n",
      "epoch 989 loss = 0.000833\n",
      "epoch 990 loss = 0.001536\n",
      "epoch 991 loss = 0.000393\n",
      "epoch 992 loss = 0.001070\n",
      "epoch 993 loss = 0.001925\n",
      "epoch 994 loss = 0.001731\n",
      "epoch 995 loss = 0.003561\n",
      "epoch 996 loss = 0.002126\n",
      "epoch 997 loss = 0.006488\n",
      "epoch 998 loss = 0.008991\n",
      "epoch 999 loss = 0.004918\n",
      "final loss = 0.004918\n",
      "accuracy_mc = tensor(0.4867, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4810, device='cuda:0')\n",
      "training time = 1076.911387681961 seconds\n",
      "testing time = 0.3182508945465088 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.119410\n",
      "epoch 1 loss = 1.954116\n",
      "epoch 2 loss = 1.817619\n",
      "epoch 3 loss = 1.713935\n",
      "epoch 4 loss = 1.635378\n",
      "epoch 5 loss = 1.495508\n",
      "epoch 6 loss = 1.417240\n",
      "epoch 7 loss = 1.340364\n",
      "epoch 8 loss = 1.220088\n",
      "epoch 9 loss = 1.111616\n",
      "epoch 10 loss = 1.053622\n",
      "epoch 11 loss = 0.960961\n",
      "epoch 12 loss = 0.945452\n",
      "epoch 13 loss = 0.811836\n",
      "epoch 14 loss = 0.760372\n",
      "epoch 15 loss = 0.695463\n",
      "epoch 16 loss = 0.609991\n",
      "epoch 17 loss = 0.598656\n",
      "epoch 18 loss = 0.601627\n",
      "epoch 19 loss = 0.501698\n",
      "epoch 20 loss = 0.490038\n",
      "epoch 21 loss = 0.491727\n",
      "epoch 22 loss = 0.384332\n",
      "epoch 23 loss = 0.367444\n",
      "epoch 24 loss = 0.301434\n",
      "epoch 25 loss = 0.255544\n",
      "epoch 26 loss = 0.238356\n",
      "epoch 27 loss = 0.182516\n",
      "epoch 28 loss = 0.178900\n",
      "epoch 29 loss = 0.208132\n",
      "epoch 30 loss = 0.197889\n",
      "epoch 31 loss = 0.212637\n",
      "epoch 32 loss = 0.110604\n",
      "epoch 33 loss = 0.079517\n",
      "epoch 34 loss = 0.074278\n",
      "epoch 35 loss = 0.104869\n",
      "epoch 36 loss = 0.097322\n",
      "epoch 37 loss = 0.087088\n",
      "epoch 38 loss = 0.227959\n",
      "epoch 39 loss = 0.096558\n",
      "epoch 40 loss = 0.111106\n",
      "epoch 41 loss = 0.056474\n",
      "epoch 42 loss = 0.068269\n",
      "epoch 43 loss = 0.065623\n",
      "epoch 44 loss = 0.062469\n",
      "epoch 45 loss = 0.084720\n",
      "epoch 46 loss = 0.086769\n",
      "epoch 47 loss = 0.050864\n",
      "epoch 48 loss = 0.049443\n",
      "epoch 49 loss = 0.052663\n",
      "epoch 50 loss = 0.139216\n",
      "epoch 51 loss = 0.058376\n",
      "epoch 52 loss = 0.046702\n",
      "epoch 53 loss = 0.154733\n",
      "epoch 54 loss = 0.008349\n",
      "epoch 55 loss = 0.014955\n",
      "epoch 56 loss = 0.016777\n",
      "epoch 57 loss = 0.032784\n",
      "epoch 58 loss = 0.009940\n",
      "epoch 59 loss = 0.007180\n",
      "epoch 60 loss = 0.013323\n",
      "epoch 61 loss = 0.024325\n",
      "epoch 62 loss = 0.011438\n",
      "epoch 63 loss = 0.009216\n",
      "epoch 64 loss = 0.006731\n",
      "epoch 65 loss = 0.016121\n",
      "epoch 66 loss = 0.003409\n",
      "epoch 67 loss = 0.014654\n",
      "epoch 68 loss = 0.013059\n",
      "epoch 69 loss = 0.005131\n",
      "epoch 70 loss = 0.006160\n",
      "epoch 71 loss = 0.007311\n",
      "epoch 72 loss = 0.008907\n",
      "epoch 73 loss = 0.002383\n",
      "epoch 74 loss = 0.008985\n",
      "epoch 75 loss = 0.003118\n",
      "epoch 76 loss = 0.002295\n",
      "epoch 77 loss = 0.018970\n",
      "epoch 78 loss = 0.009045\n",
      "epoch 79 loss = 0.007263\n",
      "epoch 80 loss = 0.014408\n",
      "epoch 81 loss = 0.005998\n",
      "epoch 82 loss = 0.015346\n",
      "epoch 83 loss = 0.003509\n",
      "epoch 84 loss = 0.008463\n",
      "epoch 85 loss = 0.008899\n",
      "epoch 86 loss = 0.009644\n",
      "epoch 87 loss = 0.014980\n",
      "epoch 88 loss = 0.002848\n",
      "epoch 89 loss = 0.079596\n",
      "epoch 90 loss = 0.013048\n",
      "epoch 91 loss = 0.002861\n",
      "epoch 92 loss = 0.003999\n",
      "epoch 93 loss = 0.009796\n",
      "epoch 94 loss = 0.027547\n",
      "epoch 95 loss = 0.033246\n",
      "epoch 96 loss = 0.009632\n",
      "epoch 97 loss = 0.035909\n",
      "epoch 98 loss = 0.004734\n",
      "epoch 99 loss = 0.020123\n",
      "epoch 100 loss = 0.002125\n",
      "epoch 101 loss = 0.006508\n",
      "epoch 102 loss = 0.004865\n",
      "epoch 103 loss = 0.002423\n",
      "epoch 104 loss = 0.017491\n",
      "epoch 105 loss = 0.020157\n",
      "epoch 106 loss = 0.001181\n",
      "epoch 107 loss = 0.012790\n",
      "epoch 108 loss = 0.006682\n",
      "epoch 109 loss = 0.008387\n",
      "epoch 110 loss = 0.018114\n",
      "epoch 111 loss = 0.000575\n",
      "epoch 112 loss = 0.002644\n",
      "epoch 113 loss = 0.002675\n",
      "epoch 114 loss = 0.007294\n",
      "epoch 115 loss = 0.000590\n",
      "epoch 116 loss = 0.001746\n",
      "epoch 117 loss = 0.000502\n",
      "epoch 118 loss = 0.001798\n",
      "epoch 119 loss = 0.002423\n",
      "epoch 120 loss = 0.002672\n",
      "epoch 121 loss = 0.001483\n",
      "epoch 122 loss = 0.012880\n",
      "epoch 123 loss = 0.016063\n",
      "epoch 124 loss = 0.005768\n",
      "epoch 125 loss = 0.003688\n",
      "epoch 126 loss = 0.001866\n",
      "epoch 127 loss = 0.001318\n",
      "epoch 128 loss = 0.003560\n",
      "epoch 129 loss = 0.026481\n",
      "epoch 130 loss = 0.004757\n",
      "epoch 131 loss = 0.018668\n",
      "epoch 132 loss = 0.005547\n",
      "epoch 133 loss = 0.001778\n",
      "epoch 134 loss = 0.000617\n",
      "epoch 135 loss = 0.000192\n",
      "epoch 136 loss = 0.003829\n",
      "epoch 137 loss = 0.025994\n",
      "epoch 138 loss = 0.001975\n",
      "epoch 139 loss = 0.000818\n",
      "epoch 140 loss = 0.003239\n",
      "epoch 141 loss = 0.005669\n",
      "epoch 142 loss = 0.000744\n",
      "epoch 143 loss = 0.002859\n",
      "epoch 144 loss = 0.001782\n",
      "epoch 145 loss = 0.018064\n",
      "epoch 146 loss = 0.003208\n",
      "epoch 147 loss = 0.004809\n",
      "epoch 148 loss = 0.014679\n",
      "epoch 149 loss = 0.001337\n",
      "epoch 150 loss = 0.002274\n",
      "epoch 151 loss = 0.004320\n",
      "epoch 152 loss = 0.000799\n",
      "epoch 153 loss = 0.002593\n",
      "epoch 154 loss = 0.002343\n",
      "epoch 155 loss = 0.001955\n",
      "epoch 156 loss = 0.004935\n",
      "epoch 157 loss = 0.002374\n",
      "epoch 158 loss = 0.005360\n",
      "epoch 159 loss = 0.044275\n",
      "epoch 160 loss = 0.033025\n",
      "epoch 161 loss = 0.001284\n",
      "epoch 162 loss = 0.000359\n",
      "epoch 163 loss = 0.001881\n",
      "epoch 164 loss = 0.000358\n",
      "epoch 165 loss = 0.006076\n",
      "epoch 166 loss = 0.004480\n",
      "epoch 167 loss = 0.000306\n",
      "epoch 168 loss = 0.001632\n",
      "epoch 169 loss = 0.002786\n",
      "epoch 170 loss = 0.036494\n",
      "epoch 171 loss = 0.001919\n",
      "epoch 172 loss = 0.004580\n",
      "epoch 173 loss = 0.074676\n",
      "epoch 174 loss = 0.001213\n",
      "epoch 175 loss = 0.003905\n",
      "epoch 176 loss = 0.026217\n",
      "epoch 177 loss = 0.001525\n",
      "epoch 178 loss = 0.000481\n",
      "epoch 179 loss = 0.002243\n",
      "epoch 180 loss = 0.004602\n",
      "epoch 181 loss = 0.000684\n",
      "epoch 182 loss = 0.000686\n",
      "epoch 183 loss = 0.005518\n",
      "epoch 184 loss = 0.004689\n",
      "epoch 185 loss = 0.003348\n",
      "epoch 186 loss = 0.001511\n",
      "epoch 187 loss = 0.004189\n",
      "epoch 188 loss = 0.000590\n",
      "epoch 189 loss = 0.133322\n",
      "epoch 190 loss = 0.003373\n",
      "epoch 191 loss = 0.001520\n",
      "epoch 192 loss = 0.005082\n",
      "epoch 193 loss = 0.015875\n",
      "epoch 194 loss = 0.003060\n",
      "epoch 195 loss = 0.002477\n",
      "epoch 196 loss = 0.003815\n",
      "epoch 197 loss = 0.010685\n",
      "epoch 198 loss = 0.002560\n",
      "epoch 199 loss = 0.000443\n",
      "epoch 200 loss = 0.000583\n",
      "epoch 201 loss = 0.001753\n",
      "epoch 202 loss = 0.001251\n",
      "epoch 203 loss = 0.020587\n",
      "epoch 204 loss = 0.003210\n",
      "epoch 205 loss = 0.001240\n",
      "epoch 206 loss = 0.000185\n",
      "epoch 207 loss = 0.010830\n",
      "epoch 208 loss = 0.000643\n",
      "epoch 209 loss = 0.000686\n",
      "epoch 210 loss = 0.000530\n",
      "epoch 211 loss = 0.000206\n",
      "epoch 212 loss = 0.009535\n",
      "epoch 213 loss = 0.001577\n",
      "epoch 214 loss = 0.012316\n",
      "epoch 215 loss = 0.001999\n",
      "epoch 216 loss = 0.005674\n",
      "epoch 217 loss = 0.002692\n",
      "epoch 218 loss = 0.003936\n",
      "epoch 219 loss = 0.016045\n",
      "epoch 220 loss = 0.000552\n",
      "epoch 221 loss = 0.002253\n",
      "epoch 222 loss = 0.000481\n",
      "epoch 223 loss = 0.020693\n",
      "epoch 224 loss = 0.000270\n",
      "epoch 225 loss = 0.088361\n",
      "epoch 226 loss = 0.001861\n",
      "epoch 227 loss = 0.004201\n",
      "epoch 228 loss = 0.002455\n",
      "epoch 229 loss = 0.000880\n",
      "epoch 230 loss = 0.000293\n",
      "epoch 231 loss = 0.081090\n",
      "epoch 232 loss = 0.007256\n",
      "epoch 233 loss = 0.002064\n",
      "epoch 234 loss = 0.003160\n",
      "epoch 235 loss = 0.224406\n",
      "epoch 236 loss = 0.002416\n",
      "epoch 237 loss = 0.000917\n",
      "epoch 238 loss = 0.001836\n",
      "epoch 239 loss = 0.014625\n",
      "epoch 240 loss = 0.014333\n",
      "epoch 241 loss = 0.005918\n",
      "epoch 242 loss = 0.000296\n",
      "epoch 243 loss = 0.002867\n",
      "epoch 244 loss = 0.002242\n",
      "epoch 245 loss = 0.001299\n",
      "epoch 246 loss = 0.003287\n",
      "epoch 247 loss = 0.005073\n",
      "epoch 248 loss = 0.010786\n",
      "epoch 249 loss = 0.000293\n",
      "epoch 250 loss = 0.000103\n",
      "epoch 251 loss = 0.000157\n",
      "epoch 252 loss = 0.002680\n",
      "epoch 253 loss = 0.000408\n",
      "epoch 254 loss = 0.000477\n",
      "epoch 255 loss = 0.010274\n",
      "epoch 256 loss = 0.000676\n",
      "epoch 257 loss = 0.004240\n",
      "epoch 258 loss = 0.007592\n",
      "epoch 259 loss = 0.002717\n",
      "epoch 260 loss = 0.000277\n",
      "epoch 261 loss = 0.000185\n",
      "epoch 262 loss = 0.000303\n",
      "epoch 263 loss = 0.000583\n",
      "epoch 264 loss = 0.000586\n",
      "epoch 265 loss = 0.000137\n",
      "epoch 266 loss = 0.005980\n",
      "epoch 267 loss = 0.096723\n",
      "epoch 268 loss = 0.001058\n",
      "epoch 269 loss = 0.001324\n",
      "epoch 270 loss = 0.003158\n",
      "epoch 271 loss = 0.006700\n",
      "epoch 272 loss = 0.002189\n",
      "epoch 273 loss = 0.000685\n",
      "epoch 274 loss = 0.004976\n",
      "epoch 275 loss = 0.000563\n",
      "epoch 276 loss = 0.000971\n",
      "epoch 277 loss = 0.002161\n",
      "epoch 278 loss = 0.004667\n",
      "epoch 279 loss = 0.000726\n",
      "epoch 280 loss = 0.002575\n",
      "epoch 281 loss = 0.000927\n",
      "epoch 282 loss = 0.001515\n",
      "epoch 283 loss = 0.000945\n",
      "epoch 284 loss = 0.002050\n",
      "epoch 285 loss = 0.005716\n",
      "epoch 286 loss = 0.000779\n",
      "epoch 287 loss = 0.001272\n",
      "epoch 288 loss = 0.000396\n",
      "epoch 289 loss = 0.001078\n",
      "epoch 290 loss = 0.001892\n",
      "epoch 291 loss = 0.133819\n",
      "epoch 292 loss = 0.013997\n",
      "epoch 293 loss = 0.060207\n",
      "epoch 294 loss = 0.005241\n",
      "epoch 295 loss = 0.008252\n",
      "epoch 296 loss = 0.002337\n",
      "epoch 297 loss = 0.000329\n",
      "epoch 298 loss = 0.000218\n",
      "epoch 299 loss = 0.001054\n",
      "epoch 300 loss = 0.000134\n",
      "epoch 301 loss = 0.001532\n",
      "epoch 302 loss = 0.000564\n",
      "epoch 303 loss = 0.002995\n",
      "epoch 304 loss = 0.000249\n",
      "epoch 305 loss = 0.001306\n",
      "epoch 306 loss = 0.001547\n",
      "epoch 307 loss = 0.002046\n",
      "epoch 308 loss = 0.004438\n",
      "epoch 309 loss = 0.001896\n",
      "epoch 310 loss = 0.000528\n",
      "epoch 311 loss = 0.002495\n",
      "epoch 312 loss = 0.003458\n",
      "epoch 313 loss = 0.002073\n",
      "epoch 314 loss = 0.011007\n",
      "epoch 315 loss = 0.000464\n",
      "epoch 316 loss = 0.002099\n",
      "epoch 317 loss = 0.001126\n",
      "epoch 318 loss = 0.002957\n",
      "epoch 319 loss = 0.000576\n",
      "epoch 320 loss = 0.008159\n",
      "epoch 321 loss = 0.001709\n",
      "epoch 322 loss = 0.008210\n",
      "epoch 323 loss = 0.002524\n",
      "epoch 324 loss = 0.001243\n",
      "epoch 325 loss = 0.000698\n",
      "epoch 326 loss = 0.005278\n",
      "epoch 327 loss = 0.002449\n",
      "epoch 328 loss = 0.000439\n",
      "epoch 329 loss = 0.000595\n",
      "epoch 330 loss = 0.001100\n",
      "epoch 331 loss = 0.002030\n",
      "epoch 332 loss = 0.000764\n",
      "epoch 333 loss = 0.021046\n",
      "epoch 334 loss = 0.001145\n",
      "epoch 335 loss = 0.009291\n",
      "epoch 336 loss = 0.000429\n",
      "epoch 337 loss = 0.000190\n",
      "epoch 338 loss = 0.000238\n",
      "epoch 339 loss = 0.000760\n",
      "epoch 340 loss = 0.000182\n",
      "epoch 341 loss = 0.001686\n",
      "epoch 342 loss = 0.104588\n",
      "epoch 343 loss = 0.017459\n",
      "epoch 344 loss = 0.047336\n",
      "epoch 345 loss = 0.212559\n",
      "epoch 346 loss = 0.007884\n",
      "epoch 347 loss = 0.002002\n",
      "epoch 348 loss = 0.000832\n",
      "epoch 349 loss = 0.002246\n",
      "epoch 350 loss = 0.002797\n",
      "epoch 351 loss = 0.002289\n",
      "epoch 352 loss = 0.001959\n",
      "epoch 353 loss = 0.001519\n",
      "epoch 354 loss = 0.002713\n",
      "epoch 355 loss = 0.000551\n",
      "epoch 356 loss = 0.001674\n",
      "epoch 357 loss = 0.000113\n",
      "epoch 358 loss = 0.001011\n",
      "epoch 359 loss = 0.000913\n",
      "epoch 360 loss = 0.000696\n",
      "epoch 361 loss = 0.000888\n",
      "epoch 362 loss = 0.001017\n",
      "epoch 363 loss = 0.000966\n",
      "epoch 364 loss = 0.017607\n",
      "epoch 365 loss = 0.001120\n",
      "epoch 366 loss = 0.000243\n",
      "epoch 367 loss = 0.000266\n",
      "epoch 368 loss = 0.000591\n",
      "epoch 369 loss = 0.004921\n",
      "epoch 370 loss = 0.000298\n",
      "epoch 371 loss = 0.000459\n",
      "epoch 372 loss = 0.000142\n",
      "epoch 373 loss = 0.001391\n",
      "epoch 374 loss = 0.001375\n",
      "epoch 375 loss = 0.000198\n",
      "epoch 376 loss = 0.000240\n",
      "epoch 377 loss = 0.000313\n",
      "epoch 378 loss = 0.000500\n",
      "epoch 379 loss = 0.010647\n",
      "epoch 380 loss = 0.002867\n",
      "epoch 381 loss = 0.001099\n",
      "epoch 382 loss = 0.003439\n",
      "epoch 383 loss = 0.004387\n",
      "epoch 384 loss = 0.001870\n",
      "epoch 385 loss = 0.000705\n",
      "epoch 386 loss = 0.000369\n",
      "epoch 387 loss = 0.001311\n",
      "epoch 388 loss = 0.003555\n",
      "epoch 389 loss = 0.005181\n",
      "epoch 390 loss = 0.003483\n",
      "epoch 391 loss = 0.007091\n",
      "epoch 392 loss = 0.001457\n",
      "epoch 393 loss = 0.001474\n",
      "epoch 394 loss = 0.003339\n",
      "epoch 395 loss = 0.000830\n",
      "epoch 396 loss = 0.006655\n",
      "epoch 397 loss = 0.011348\n",
      "epoch 398 loss = 0.000956\n",
      "epoch 399 loss = 0.006384\n",
      "epoch 400 loss = 0.000531\n",
      "epoch 401 loss = 0.004347\n",
      "epoch 402 loss = 0.006939\n",
      "epoch 403 loss = 0.000779\n",
      "epoch 404 loss = 0.001083\n",
      "epoch 405 loss = 0.000143\n",
      "epoch 406 loss = 0.002036\n",
      "epoch 407 loss = 0.033452\n",
      "epoch 408 loss = 0.024906\n",
      "epoch 409 loss = 0.029651\n",
      "epoch 410 loss = 0.022185\n",
      "epoch 411 loss = 0.000712\n",
      "epoch 412 loss = 0.001520\n",
      "epoch 413 loss = 0.023445\n",
      "epoch 414 loss = 0.001065\n",
      "epoch 415 loss = 0.000970\n",
      "epoch 416 loss = 0.000961\n",
      "epoch 417 loss = 0.000144\n",
      "epoch 418 loss = 0.055005\n",
      "epoch 419 loss = 0.003855\n",
      "epoch 420 loss = 0.000527\n",
      "epoch 421 loss = 0.000428\n",
      "epoch 422 loss = 0.000250\n",
      "epoch 423 loss = 0.000879\n",
      "epoch 424 loss = 0.074223\n",
      "epoch 425 loss = 0.023236\n",
      "epoch 426 loss = 0.012533\n",
      "epoch 427 loss = 0.000533\n",
      "epoch 428 loss = 0.000496\n",
      "epoch 429 loss = 0.000398\n",
      "epoch 430 loss = 0.000178\n",
      "epoch 431 loss = 0.000313\n",
      "epoch 432 loss = 0.000203\n",
      "epoch 433 loss = 0.000519\n",
      "epoch 434 loss = 0.004822\n",
      "epoch 435 loss = 0.000576\n",
      "epoch 436 loss = 0.002166\n",
      "epoch 437 loss = 0.000827\n",
      "epoch 438 loss = 0.000651\n",
      "epoch 439 loss = 0.001624\n",
      "epoch 440 loss = 0.001954\n",
      "epoch 441 loss = 0.000437\n",
      "epoch 442 loss = 0.000071\n",
      "epoch 443 loss = 0.004190\n",
      "epoch 444 loss = 0.005555\n",
      "epoch 445 loss = 0.000832\n",
      "epoch 446 loss = 0.003568\n",
      "epoch 447 loss = 0.018286\n",
      "epoch 448 loss = 0.000221\n",
      "epoch 449 loss = 0.000245\n",
      "epoch 450 loss = 0.000263\n",
      "epoch 451 loss = 0.000731\n",
      "epoch 452 loss = 0.000925\n",
      "epoch 453 loss = 0.000688\n",
      "epoch 454 loss = 0.000661\n",
      "epoch 455 loss = 0.000196\n",
      "epoch 456 loss = 0.001048\n",
      "epoch 457 loss = 0.000400\n",
      "epoch 458 loss = 0.001041\n",
      "epoch 459 loss = 0.000213\n",
      "epoch 460 loss = 0.001093\n",
      "epoch 461 loss = 0.007046\n",
      "epoch 462 loss = 0.002258\n",
      "epoch 463 loss = 0.004135\n",
      "epoch 464 loss = 0.003558\n",
      "epoch 465 loss = 0.005861\n",
      "epoch 466 loss = 0.006987\n",
      "epoch 467 loss = 0.010825\n",
      "epoch 468 loss = 0.001303\n",
      "epoch 469 loss = 0.007971\n",
      "epoch 470 loss = 0.000622\n",
      "epoch 471 loss = 0.001455\n",
      "epoch 472 loss = 0.004687\n",
      "epoch 473 loss = 0.013621\n",
      "epoch 474 loss = 0.016031\n",
      "epoch 475 loss = 0.006437\n",
      "epoch 476 loss = 0.001429\n",
      "epoch 477 loss = 0.000213\n",
      "epoch 478 loss = 0.001744\n",
      "epoch 479 loss = 0.002295\n",
      "epoch 480 loss = 0.010119\n",
      "epoch 481 loss = 0.000691\n",
      "epoch 482 loss = 0.000678\n",
      "epoch 483 loss = 0.008706\n",
      "epoch 484 loss = 0.002116\n",
      "epoch 485 loss = 0.000831\n",
      "epoch 486 loss = 0.004641\n",
      "epoch 487 loss = 0.001028\n",
      "epoch 488 loss = 0.000449\n",
      "epoch 489 loss = 0.000305\n",
      "epoch 490 loss = 0.000977\n",
      "epoch 491 loss = 0.000366\n",
      "epoch 492 loss = 0.000582\n",
      "epoch 493 loss = 0.000443\n",
      "epoch 494 loss = 0.000337\n",
      "epoch 495 loss = 0.003214\n",
      "epoch 496 loss = 0.000374\n",
      "epoch 497 loss = 0.000239\n",
      "epoch 498 loss = 0.000335\n",
      "epoch 499 loss = 0.000391\n",
      "epoch 500 loss = 0.000246\n",
      "epoch 501 loss = 0.002752\n",
      "epoch 502 loss = 0.000558\n",
      "epoch 503 loss = 0.008498\n",
      "epoch 504 loss = 0.000423\n",
      "epoch 505 loss = 0.005348\n",
      "epoch 506 loss = 0.003493\n",
      "epoch 507 loss = 0.002006\n",
      "epoch 508 loss = 0.010767\n",
      "epoch 509 loss = 0.044129\n",
      "epoch 510 loss = 0.005590\n",
      "epoch 511 loss = 0.004576\n",
      "epoch 512 loss = 0.000857\n",
      "epoch 513 loss = 0.003412\n",
      "epoch 514 loss = 0.000707\n",
      "epoch 515 loss = 0.010194\n",
      "epoch 516 loss = 0.002487\n",
      "epoch 517 loss = 0.000598\n",
      "epoch 518 loss = 0.000721\n",
      "epoch 519 loss = 0.000470\n",
      "epoch 520 loss = 0.000894\n",
      "epoch 521 loss = 0.000453\n",
      "epoch 522 loss = 0.000549\n",
      "epoch 523 loss = 0.000638\n",
      "epoch 524 loss = 0.000846\n",
      "epoch 525 loss = 0.000129\n",
      "epoch 526 loss = 0.000288\n",
      "epoch 527 loss = 0.000233\n",
      "epoch 528 loss = 0.056149\n",
      "epoch 529 loss = 0.004469\n",
      "epoch 530 loss = 0.001614\n",
      "epoch 531 loss = 0.001330\n",
      "epoch 532 loss = 0.014651\n",
      "epoch 533 loss = 0.001577\n",
      "epoch 534 loss = 0.003074\n",
      "epoch 535 loss = 0.000703\n",
      "epoch 536 loss = 0.000730\n",
      "epoch 537 loss = 0.030160\n",
      "epoch 538 loss = 0.000997\n",
      "epoch 539 loss = 0.000600\n",
      "epoch 540 loss = 0.001330\n",
      "epoch 541 loss = 0.001253\n",
      "epoch 542 loss = 0.001896\n",
      "epoch 543 loss = 0.022674\n",
      "epoch 544 loss = 0.009735\n",
      "epoch 545 loss = 0.001795\n",
      "epoch 546 loss = 0.013945\n",
      "epoch 547 loss = 0.000963\n",
      "epoch 548 loss = 0.000665\n",
      "epoch 549 loss = 0.008502\n",
      "epoch 550 loss = 0.002311\n",
      "epoch 551 loss = 0.001391\n",
      "epoch 552 loss = 0.000314\n",
      "epoch 553 loss = 0.001969\n",
      "epoch 554 loss = 0.000401\n",
      "epoch 555 loss = 0.000109\n",
      "epoch 556 loss = 0.000354\n",
      "epoch 557 loss = 0.000585\n",
      "epoch 558 loss = 0.000430\n",
      "epoch 559 loss = 0.000749\n",
      "epoch 560 loss = 0.047065\n",
      "epoch 561 loss = 0.001901\n",
      "epoch 562 loss = 0.020554\n",
      "epoch 563 loss = 0.014411\n",
      "epoch 564 loss = 0.003676\n",
      "epoch 565 loss = 0.022550\n",
      "epoch 566 loss = 0.002701\n",
      "epoch 567 loss = 0.012342\n",
      "epoch 568 loss = 0.026193\n",
      "epoch 569 loss = 0.000556\n",
      "epoch 570 loss = 0.003009\n",
      "epoch 571 loss = 0.004504\n",
      "epoch 572 loss = 0.000524\n",
      "epoch 573 loss = 0.020863\n",
      "epoch 574 loss = 0.007535\n",
      "epoch 575 loss = 0.000599\n",
      "epoch 576 loss = 0.000332\n",
      "epoch 577 loss = 0.000391\n",
      "epoch 578 loss = 0.000192\n",
      "epoch 579 loss = 0.000932\n",
      "epoch 580 loss = 0.000783\n",
      "epoch 581 loss = 0.002276\n",
      "epoch 582 loss = 0.000567\n",
      "epoch 583 loss = 0.000670\n",
      "epoch 584 loss = 0.002594\n",
      "epoch 585 loss = 0.000856\n",
      "epoch 586 loss = 0.000692\n",
      "epoch 587 loss = 0.001437\n",
      "epoch 588 loss = 0.000689\n",
      "epoch 589 loss = 0.000426\n",
      "epoch 590 loss = 0.000610\n",
      "epoch 591 loss = 0.000134\n",
      "epoch 592 loss = 0.002243\n",
      "epoch 593 loss = 0.002275\n",
      "epoch 594 loss = 0.001353\n",
      "epoch 595 loss = 0.003992\n",
      "epoch 596 loss = 0.008692\n",
      "epoch 597 loss = 0.000521\n",
      "epoch 598 loss = 0.007410\n",
      "epoch 599 loss = 0.003360\n",
      "epoch 600 loss = 0.014083\n",
      "epoch 601 loss = 0.002614\n",
      "epoch 602 loss = 0.000217\n",
      "epoch 603 loss = 0.001240\n",
      "epoch 604 loss = 0.000125\n",
      "epoch 605 loss = 0.000538\n",
      "epoch 606 loss = 0.000582\n",
      "epoch 607 loss = 0.000599\n",
      "epoch 608 loss = 0.000676\n",
      "epoch 609 loss = 0.002156\n",
      "epoch 610 loss = 0.006958\n",
      "epoch 611 loss = 0.005170\n",
      "epoch 612 loss = 0.000835\n",
      "epoch 613 loss = 0.001093\n",
      "epoch 614 loss = 0.003487\n",
      "epoch 615 loss = 0.000266\n",
      "epoch 616 loss = 0.000693\n",
      "epoch 617 loss = 0.001473\n",
      "epoch 618 loss = 0.000859\n",
      "epoch 619 loss = 0.000123\n",
      "epoch 620 loss = 0.002754\n",
      "epoch 621 loss = 0.000808\n",
      "epoch 622 loss = 0.000187\n",
      "epoch 623 loss = 0.000124\n",
      "epoch 624 loss = 0.002390\n",
      "epoch 625 loss = 0.025515\n",
      "epoch 626 loss = 0.000706\n",
      "epoch 627 loss = 0.000644\n",
      "epoch 628 loss = 0.006849\n",
      "epoch 629 loss = 0.005641\n",
      "epoch 630 loss = 0.003366\n",
      "epoch 631 loss = 0.001391\n",
      "epoch 632 loss = 0.024815\n",
      "epoch 633 loss = 0.001217\n",
      "epoch 634 loss = 0.000371\n",
      "epoch 635 loss = 0.001777\n",
      "epoch 636 loss = 0.000718\n",
      "epoch 637 loss = 0.000825\n",
      "epoch 638 loss = 0.039597\n",
      "epoch 639 loss = 0.001119\n",
      "epoch 640 loss = 0.004478\n",
      "epoch 641 loss = 0.013999\n",
      "epoch 642 loss = 0.000541\n",
      "epoch 643 loss = 0.000653\n",
      "epoch 644 loss = 0.002165\n",
      "epoch 645 loss = 0.000471\n",
      "epoch 646 loss = 0.000185\n",
      "epoch 647 loss = 0.000476\n",
      "epoch 648 loss = 0.002085\n",
      "epoch 649 loss = 0.004127\n",
      "epoch 650 loss = 0.000047\n",
      "epoch 651 loss = 0.000225\n",
      "epoch 652 loss = 0.000044\n",
      "epoch 653 loss = 0.000153\n",
      "epoch 654 loss = 0.000073\n",
      "epoch 655 loss = 0.000251\n",
      "epoch 656 loss = 0.000188\n",
      "epoch 657 loss = 0.000903\n",
      "epoch 658 loss = 0.000315\n",
      "epoch 659 loss = 0.000223\n",
      "epoch 660 loss = 0.000097\n",
      "epoch 661 loss = 0.000245\n",
      "epoch 662 loss = 0.000136\n",
      "epoch 663 loss = 0.000536\n",
      "epoch 664 loss = 0.002946\n",
      "epoch 665 loss = 0.004903\n",
      "epoch 666 loss = 0.000621\n",
      "epoch 667 loss = 0.008066\n",
      "epoch 668 loss = 0.000439\n",
      "epoch 669 loss = 0.004944\n",
      "epoch 670 loss = 0.001471\n",
      "epoch 671 loss = 0.002706\n",
      "epoch 672 loss = 0.001056\n",
      "epoch 673 loss = 0.005025\n",
      "epoch 674 loss = 0.000636\n",
      "epoch 675 loss = 0.002964\n",
      "epoch 676 loss = 0.008289\n",
      "epoch 677 loss = 0.067373\n",
      "epoch 678 loss = 0.023103\n",
      "epoch 679 loss = 0.001698\n",
      "epoch 680 loss = 0.007243\n",
      "epoch 681 loss = 0.000532\n",
      "epoch 682 loss = 0.000681\n",
      "epoch 683 loss = 0.000770\n",
      "epoch 684 loss = 0.004736\n",
      "epoch 685 loss = 0.000530\n",
      "epoch 686 loss = 0.000279\n",
      "epoch 687 loss = 0.002023\n",
      "epoch 688 loss = 0.000474\n",
      "epoch 689 loss = 0.001159\n",
      "epoch 690 loss = 0.001342\n",
      "epoch 691 loss = 0.000752\n",
      "epoch 692 loss = 0.000728\n",
      "epoch 693 loss = 0.000324\n",
      "epoch 694 loss = 0.000246\n",
      "epoch 695 loss = 0.001141\n",
      "epoch 696 loss = 0.001109\n",
      "epoch 697 loss = 0.002926\n",
      "epoch 698 loss = 0.001354\n",
      "epoch 699 loss = 0.102198\n",
      "epoch 700 loss = 0.015756\n",
      "epoch 701 loss = 0.010258\n",
      "epoch 702 loss = 0.001876\n",
      "epoch 703 loss = 0.000596\n",
      "epoch 704 loss = 0.002755\n",
      "epoch 705 loss = 0.005835\n",
      "epoch 706 loss = 0.000724\n",
      "epoch 707 loss = 0.000597\n",
      "epoch 708 loss = 0.000799\n",
      "epoch 709 loss = 0.000153\n",
      "epoch 710 loss = 0.000291\n",
      "epoch 711 loss = 0.000229\n",
      "epoch 712 loss = 0.000094\n",
      "epoch 713 loss = 0.000249\n",
      "epoch 714 loss = 0.000413\n",
      "epoch 715 loss = 0.000460\n",
      "epoch 716 loss = 0.000302\n",
      "epoch 717 loss = 0.002385\n",
      "epoch 718 loss = 0.002746\n",
      "epoch 719 loss = 0.000680\n",
      "epoch 720 loss = 0.000693\n",
      "epoch 721 loss = 0.000670\n",
      "epoch 722 loss = 0.010816\n",
      "epoch 723 loss = 0.013444\n",
      "epoch 724 loss = 0.000662\n",
      "epoch 725 loss = 0.025433\n",
      "epoch 726 loss = 0.000751\n",
      "epoch 727 loss = 0.000899\n",
      "epoch 728 loss = 0.002237\n",
      "epoch 729 loss = 0.001011\n",
      "epoch 730 loss = 0.001348\n",
      "epoch 731 loss = 0.002262\n",
      "epoch 732 loss = 0.000342\n",
      "epoch 733 loss = 0.000496\n",
      "epoch 734 loss = 0.002383\n",
      "epoch 735 loss = 0.000745\n",
      "epoch 736 loss = 0.002405\n",
      "epoch 737 loss = 0.003929\n",
      "epoch 738 loss = 0.001822\n",
      "epoch 739 loss = 0.001756\n",
      "epoch 740 loss = 0.024619\n",
      "epoch 741 loss = 0.021630\n",
      "epoch 742 loss = 0.015381\n",
      "epoch 743 loss = 0.004283\n",
      "epoch 744 loss = 0.000426\n",
      "epoch 745 loss = 0.000301\n",
      "epoch 746 loss = 0.000713\n",
      "epoch 747 loss = 0.002998\n",
      "epoch 748 loss = 0.000921\n",
      "epoch 749 loss = 0.000358\n",
      "epoch 750 loss = 0.000939\n",
      "epoch 751 loss = 0.000859\n",
      "epoch 752 loss = 0.000687\n",
      "epoch 753 loss = 0.016217\n",
      "epoch 754 loss = 0.000567\n",
      "epoch 755 loss = 0.001794\n",
      "epoch 756 loss = 0.000138\n",
      "epoch 757 loss = 0.001936\n",
      "epoch 758 loss = 0.001111\n",
      "epoch 759 loss = 0.000097\n",
      "epoch 760 loss = 0.000504\n",
      "epoch 761 loss = 0.000392\n",
      "epoch 762 loss = 0.000662\n",
      "epoch 763 loss = 0.000454\n",
      "epoch 764 loss = 0.002491\n",
      "epoch 765 loss = 0.000919\n",
      "epoch 766 loss = 0.003038\n",
      "epoch 767 loss = 0.000191\n",
      "epoch 768 loss = 0.001816\n",
      "epoch 769 loss = 0.013330\n",
      "epoch 770 loss = 0.000267\n",
      "epoch 771 loss = 0.000723\n",
      "epoch 772 loss = 0.000734\n",
      "epoch 773 loss = 0.008985\n",
      "epoch 774 loss = 0.003741\n",
      "epoch 775 loss = 0.002155\n",
      "epoch 776 loss = 0.008851\n",
      "epoch 777 loss = 0.000397\n",
      "epoch 778 loss = 0.000110\n",
      "epoch 779 loss = 0.000196\n",
      "epoch 780 loss = 0.001949\n",
      "epoch 781 loss = 0.000109\n",
      "epoch 782 loss = 0.103598\n",
      "epoch 783 loss = 0.042997\n",
      "epoch 784 loss = 0.000390\n",
      "epoch 785 loss = 0.010739\n",
      "epoch 786 loss = 0.001695\n",
      "epoch 787 loss = 0.000328\n",
      "epoch 788 loss = 0.000855\n",
      "epoch 789 loss = 0.004675\n",
      "epoch 790 loss = 0.000786\n",
      "epoch 791 loss = 0.000732\n",
      "epoch 792 loss = 0.001684\n",
      "epoch 793 loss = 0.000782\n",
      "epoch 794 loss = 0.000773\n",
      "epoch 795 loss = 0.000659\n",
      "epoch 796 loss = 0.002994\n",
      "epoch 797 loss = 0.001178\n",
      "epoch 798 loss = 0.000730\n",
      "epoch 799 loss = 0.001260\n",
      "epoch 800 loss = 0.000977\n",
      "epoch 801 loss = 0.002582\n",
      "epoch 802 loss = 0.004400\n",
      "epoch 803 loss = 0.000773\n",
      "epoch 804 loss = 0.000355\n",
      "epoch 805 loss = 0.001347\n",
      "epoch 806 loss = 0.000267\n",
      "epoch 807 loss = 0.000150\n",
      "epoch 808 loss = 0.000161\n",
      "epoch 809 loss = 0.000175\n",
      "epoch 810 loss = 0.000088\n",
      "epoch 811 loss = 0.000592\n",
      "epoch 812 loss = 0.000182\n",
      "epoch 813 loss = 0.000301\n",
      "epoch 814 loss = 0.001555\n",
      "epoch 815 loss = 0.002382\n",
      "epoch 816 loss = 0.000592\n",
      "epoch 817 loss = 0.000392\n",
      "epoch 818 loss = 0.004933\n",
      "epoch 819 loss = 0.007808\n",
      "epoch 820 loss = 0.002251\n",
      "epoch 821 loss = 0.013336\n",
      "epoch 822 loss = 0.001565\n",
      "epoch 823 loss = 0.002116\n",
      "epoch 824 loss = 0.000666\n",
      "epoch 825 loss = 0.000321\n",
      "epoch 826 loss = 0.000489\n",
      "epoch 827 loss = 0.003133\n",
      "epoch 828 loss = 0.001324\n",
      "epoch 829 loss = 0.028423\n",
      "epoch 830 loss = 0.002139\n",
      "epoch 831 loss = 0.037576\n",
      "epoch 832 loss = 0.000750\n",
      "epoch 833 loss = 0.000681\n",
      "epoch 834 loss = 0.000990\n",
      "epoch 835 loss = 0.008358\n",
      "epoch 836 loss = 0.032645\n",
      "epoch 837 loss = 0.001012\n",
      "epoch 838 loss = 0.000305\n",
      "epoch 839 loss = 0.000492\n",
      "epoch 840 loss = 0.006276\n",
      "epoch 841 loss = 0.000769\n",
      "epoch 842 loss = 0.000917\n",
      "epoch 843 loss = 0.000529\n",
      "epoch 844 loss = 0.000089\n",
      "epoch 845 loss = 0.000122\n",
      "epoch 846 loss = 0.000480\n",
      "epoch 847 loss = 0.002405\n",
      "epoch 848 loss = 0.000116\n",
      "epoch 849 loss = 0.001036\n",
      "epoch 850 loss = 0.000291\n",
      "epoch 851 loss = 0.000135\n",
      "epoch 852 loss = 0.002365\n",
      "epoch 853 loss = 0.000990\n",
      "epoch 854 loss = 0.000447\n",
      "epoch 855 loss = 0.000356\n",
      "epoch 856 loss = 0.000352\n",
      "epoch 857 loss = 0.001985\n",
      "epoch 858 loss = 0.004196\n",
      "epoch 859 loss = 0.002589\n",
      "epoch 860 loss = 0.000173\n",
      "epoch 861 loss = 0.000214\n",
      "epoch 862 loss = 0.000064\n",
      "epoch 863 loss = 0.000183\n",
      "epoch 864 loss = 0.000292\n",
      "epoch 865 loss = 0.001859\n",
      "epoch 866 loss = 0.001869\n",
      "epoch 867 loss = 0.002253\n",
      "epoch 868 loss = 0.075365\n",
      "epoch 869 loss = 0.013583\n",
      "epoch 870 loss = 0.010519\n",
      "epoch 871 loss = 0.011005\n",
      "epoch 872 loss = 0.001556\n",
      "epoch 873 loss = 0.002181\n",
      "epoch 874 loss = 0.000736\n",
      "epoch 875 loss = 0.000152\n",
      "epoch 876 loss = 0.000154\n",
      "epoch 877 loss = 0.000380\n",
      "epoch 878 loss = 0.001336\n",
      "epoch 879 loss = 0.000357\n",
      "epoch 880 loss = 0.014811\n",
      "epoch 881 loss = 0.000161\n",
      "epoch 882 loss = 0.040522\n",
      "epoch 883 loss = 0.011534\n",
      "epoch 884 loss = 0.000210\n",
      "epoch 885 loss = 0.000169\n",
      "epoch 886 loss = 0.000508\n",
      "epoch 887 loss = 0.009399\n",
      "epoch 888 loss = 0.000141\n",
      "epoch 889 loss = 0.000118\n",
      "epoch 890 loss = 0.000157\n",
      "epoch 891 loss = 0.000132\n",
      "epoch 892 loss = 0.000140\n",
      "epoch 893 loss = 0.000492\n",
      "epoch 894 loss = 0.000341\n",
      "epoch 895 loss = 0.000395\n",
      "epoch 896 loss = 0.000443\n",
      "epoch 897 loss = 0.001308\n",
      "epoch 898 loss = 0.000277\n",
      "epoch 899 loss = 0.000621\n",
      "epoch 900 loss = 0.001244\n",
      "epoch 901 loss = 0.000645\n",
      "epoch 902 loss = 0.001113\n",
      "epoch 903 loss = 0.000208\n",
      "epoch 904 loss = 0.001424\n",
      "epoch 905 loss = 0.007374\n",
      "epoch 906 loss = 0.006761\n",
      "epoch 907 loss = 0.000536\n",
      "epoch 908 loss = 0.005061\n",
      "epoch 909 loss = 0.001912\n",
      "epoch 910 loss = 0.004309\n",
      "epoch 911 loss = 0.000538\n",
      "epoch 912 loss = 0.001213\n",
      "epoch 913 loss = 0.002459\n",
      "epoch 914 loss = 0.000802\n",
      "epoch 915 loss = 0.000463\n",
      "epoch 916 loss = 0.000259\n",
      "epoch 917 loss = 0.000449\n",
      "epoch 918 loss = 0.002830\n",
      "epoch 919 loss = 0.001477\n",
      "epoch 920 loss = 0.002671\n",
      "epoch 921 loss = 0.002206\n",
      "epoch 922 loss = 0.002842\n",
      "epoch 923 loss = 0.019128\n",
      "epoch 924 loss = 0.000666\n",
      "epoch 925 loss = 0.000830\n",
      "epoch 926 loss = 0.000489\n",
      "epoch 927 loss = 0.001885\n",
      "epoch 928 loss = 0.000138\n",
      "epoch 929 loss = 0.000653\n",
      "epoch 930 loss = 0.002539\n",
      "epoch 931 loss = 0.004776\n",
      "epoch 932 loss = 0.000470\n",
      "epoch 933 loss = 0.000732\n",
      "epoch 934 loss = 0.002491\n",
      "epoch 935 loss = 0.000966\n",
      "epoch 936 loss = 0.005280\n",
      "epoch 937 loss = 0.000349\n",
      "epoch 938 loss = 0.000723\n",
      "epoch 939 loss = 0.153654\n",
      "epoch 940 loss = 0.010253\n",
      "epoch 941 loss = 0.001097\n",
      "epoch 942 loss = 0.001800\n",
      "epoch 943 loss = 0.006972\n",
      "epoch 944 loss = 0.002278\n",
      "epoch 945 loss = 0.000526\n",
      "epoch 946 loss = 0.000557\n",
      "epoch 947 loss = 0.000427\n",
      "epoch 948 loss = 0.001768\n",
      "epoch 949 loss = 0.000553\n",
      "epoch 950 loss = 0.000237\n",
      "epoch 951 loss = 0.000172\n",
      "epoch 952 loss = 0.000191\n",
      "epoch 953 loss = 0.001150\n",
      "epoch 954 loss = 0.000513\n",
      "epoch 955 loss = 0.000456\n",
      "epoch 956 loss = 0.001098\n",
      "epoch 957 loss = 0.000440\n",
      "epoch 958 loss = 0.001120\n",
      "epoch 959 loss = 0.000236\n",
      "epoch 960 loss = 0.000743\n",
      "epoch 961 loss = 0.000945\n",
      "epoch 962 loss = 0.004341\n",
      "epoch 963 loss = 0.003182\n",
      "epoch 964 loss = 0.002256\n",
      "epoch 965 loss = 0.010670\n",
      "epoch 966 loss = 0.001310\n",
      "epoch 967 loss = 0.000957\n",
      "epoch 968 loss = 0.054952\n",
      "epoch 969 loss = 0.000835\n",
      "epoch 970 loss = 0.001153\n",
      "epoch 971 loss = 0.000517\n",
      "epoch 972 loss = 0.001595\n",
      "epoch 973 loss = 0.009442\n",
      "epoch 974 loss = 0.000980\n",
      "epoch 975 loss = 0.001809\n",
      "epoch 976 loss = 0.000623\n",
      "epoch 977 loss = 0.029773\n",
      "epoch 978 loss = 0.000963\n",
      "epoch 979 loss = 0.003876\n",
      "epoch 980 loss = 0.004397\n",
      "epoch 981 loss = 0.009859\n",
      "epoch 982 loss = 0.029443\n",
      "epoch 983 loss = 0.001866\n",
      "epoch 984 loss = 0.001748\n",
      "epoch 985 loss = 0.000474\n",
      "epoch 986 loss = 0.000642\n",
      "epoch 987 loss = 0.000344\n",
      "epoch 988 loss = 0.000284\n",
      "epoch 989 loss = 0.000196\n",
      "epoch 990 loss = 0.000497\n",
      "epoch 991 loss = 0.000387\n",
      "epoch 992 loss = 0.000477\n",
      "epoch 993 loss = 0.000731\n",
      "epoch 994 loss = 0.001004\n",
      "epoch 995 loss = 0.001359\n",
      "epoch 996 loss = 0.002087\n",
      "epoch 997 loss = 0.001259\n",
      "epoch 998 loss = 0.000561\n",
      "epoch 999 loss = 0.000847\n",
      "final loss = 0.000847\n",
      "accuracy_mc = tensor(0.4703, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4735, device='cuda:0')\n",
      "training time = 1076.6472918987274 seconds\n",
      "testing time = 0.31064534187316895 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 1.951436\n",
      "epoch 1 loss = 1.795273\n",
      "epoch 2 loss = 1.779206\n",
      "epoch 3 loss = 1.643412\n",
      "epoch 4 loss = 1.554673\n",
      "epoch 5 loss = 1.439673\n",
      "epoch 6 loss = 1.310756\n",
      "epoch 7 loss = 1.189536\n",
      "epoch 8 loss = 1.098544\n",
      "epoch 9 loss = 1.011589\n",
      "epoch 10 loss = 0.909562\n",
      "epoch 11 loss = 0.821421\n",
      "epoch 12 loss = 0.780851\n",
      "epoch 13 loss = 0.715147\n",
      "epoch 14 loss = 0.646673\n",
      "epoch 15 loss = 0.593466\n",
      "epoch 16 loss = 0.530024\n",
      "epoch 17 loss = 0.494598\n",
      "epoch 18 loss = 0.441931\n",
      "epoch 19 loss = 0.403135\n",
      "epoch 20 loss = 0.381044\n",
      "epoch 21 loss = 0.367324\n",
      "epoch 22 loss = 0.399472\n",
      "epoch 23 loss = 0.386430\n",
      "epoch 24 loss = 0.318191\n",
      "epoch 25 loss = 0.311144\n",
      "epoch 26 loss = 0.296624\n",
      "epoch 27 loss = 0.277202\n",
      "epoch 28 loss = 0.227242\n",
      "epoch 29 loss = 0.247665\n",
      "epoch 30 loss = 0.197659\n",
      "epoch 31 loss = 0.188071\n",
      "epoch 32 loss = 0.183330\n",
      "epoch 33 loss = 0.133790\n",
      "epoch 34 loss = 0.134517\n",
      "epoch 35 loss = 0.093420\n",
      "epoch 36 loss = 0.096159\n",
      "epoch 37 loss = 0.071419\n",
      "epoch 38 loss = 0.081632\n",
      "epoch 39 loss = 0.078044\n",
      "epoch 40 loss = 0.072024\n",
      "epoch 41 loss = 0.161260\n",
      "epoch 42 loss = 0.132080\n",
      "epoch 43 loss = 0.054631\n",
      "epoch 44 loss = 0.053927\n",
      "epoch 45 loss = 0.055129\n",
      "epoch 46 loss = 0.041325\n",
      "epoch 47 loss = 0.031478\n",
      "epoch 48 loss = 0.080249\n",
      "epoch 49 loss = 0.051654\n",
      "epoch 50 loss = 0.053990\n",
      "epoch 51 loss = 0.029148\n",
      "epoch 52 loss = 0.038855\n",
      "epoch 53 loss = 0.043463\n",
      "epoch 54 loss = 0.055133\n",
      "epoch 55 loss = 0.027727\n",
      "epoch 56 loss = 0.016350\n",
      "epoch 57 loss = 0.026424\n",
      "epoch 58 loss = 0.112180\n",
      "epoch 59 loss = 0.028972\n",
      "epoch 60 loss = 0.073914\n",
      "epoch 61 loss = 0.048055\n",
      "epoch 62 loss = 0.132541\n",
      "epoch 63 loss = 0.026187\n",
      "epoch 64 loss = 0.036399\n",
      "epoch 65 loss = 0.035063\n",
      "epoch 66 loss = 0.074987\n",
      "epoch 67 loss = 0.009622\n",
      "epoch 68 loss = 0.065256\n",
      "epoch 69 loss = 0.026094\n",
      "epoch 70 loss = 0.009738\n",
      "epoch 71 loss = 0.010716\n",
      "epoch 72 loss = 0.009787\n",
      "epoch 73 loss = 0.066943\n",
      "epoch 74 loss = 0.008860\n",
      "epoch 75 loss = 0.019018\n",
      "epoch 76 loss = 0.095760\n",
      "epoch 77 loss = 0.003941\n",
      "epoch 78 loss = 0.007700\n",
      "epoch 79 loss = 0.001743\n",
      "epoch 80 loss = 0.008659\n",
      "epoch 81 loss = 0.000974\n",
      "epoch 82 loss = 0.006521\n",
      "epoch 83 loss = 0.004525\n",
      "epoch 84 loss = 0.007211\n",
      "epoch 85 loss = 0.152813\n",
      "epoch 86 loss = 0.002381\n",
      "epoch 87 loss = 0.001369\n",
      "epoch 88 loss = 0.002336\n",
      "epoch 89 loss = 0.004787\n",
      "epoch 90 loss = 0.003213\n",
      "epoch 91 loss = 0.003235\n",
      "epoch 92 loss = 0.003744\n",
      "epoch 93 loss = 0.007561\n",
      "epoch 94 loss = 0.004379\n",
      "epoch 95 loss = 0.000924\n",
      "epoch 96 loss = 0.002394\n",
      "epoch 97 loss = 0.000650\n",
      "epoch 98 loss = 0.006881\n",
      "epoch 99 loss = 0.001787\n",
      "epoch 100 loss = 0.004148\n",
      "epoch 101 loss = 0.001170\n",
      "epoch 102 loss = 0.006086\n",
      "epoch 103 loss = 0.002461\n",
      "epoch 104 loss = 0.001119\n",
      "epoch 105 loss = 0.002306\n",
      "epoch 106 loss = 0.006849\n",
      "epoch 107 loss = 0.012495\n",
      "epoch 108 loss = 0.001307\n",
      "epoch 109 loss = 0.001675\n",
      "epoch 110 loss = 0.003462\n",
      "epoch 111 loss = 0.003775\n",
      "epoch 112 loss = 0.001659\n",
      "epoch 113 loss = 0.005454\n",
      "epoch 114 loss = 0.052308\n",
      "epoch 115 loss = 0.000876\n",
      "epoch 116 loss = 0.009518\n",
      "epoch 117 loss = 0.004294\n",
      "epoch 118 loss = 0.006421\n",
      "epoch 119 loss = 0.000651\n",
      "epoch 120 loss = 0.007158\n",
      "epoch 121 loss = 0.001530\n",
      "epoch 122 loss = 0.001088\n",
      "epoch 123 loss = 0.003142\n",
      "epoch 124 loss = 0.008903\n",
      "epoch 125 loss = 0.005839\n",
      "epoch 126 loss = 0.003779\n",
      "epoch 127 loss = 0.030636\n",
      "epoch 128 loss = 0.103223\n",
      "epoch 129 loss = 0.009599\n",
      "epoch 130 loss = 0.054486\n",
      "epoch 131 loss = 0.003248\n",
      "epoch 132 loss = 0.004748\n",
      "epoch 133 loss = 0.036784\n",
      "epoch 134 loss = 0.029031\n",
      "epoch 135 loss = 0.001814\n",
      "epoch 136 loss = 0.001766\n",
      "epoch 137 loss = 0.002435\n",
      "epoch 138 loss = 0.004505\n",
      "epoch 139 loss = 0.004807\n",
      "epoch 140 loss = 0.046251\n",
      "epoch 141 loss = 0.008579\n",
      "epoch 142 loss = 0.014919\n",
      "epoch 143 loss = 0.002921\n",
      "epoch 144 loss = 0.024393\n",
      "epoch 145 loss = 0.023638\n",
      "epoch 146 loss = 0.036516\n",
      "epoch 147 loss = 0.004133\n",
      "epoch 148 loss = 0.007350\n",
      "epoch 149 loss = 0.000562\n",
      "epoch 150 loss = 0.000566\n",
      "epoch 151 loss = 0.004342\n",
      "epoch 152 loss = 0.002670\n",
      "epoch 153 loss = 0.103236\n",
      "epoch 154 loss = 0.002897\n",
      "epoch 155 loss = 0.000447\n",
      "epoch 156 loss = 0.049154\n",
      "epoch 157 loss = 0.001907\n",
      "epoch 158 loss = 0.032842\n",
      "epoch 159 loss = 0.001866\n",
      "epoch 160 loss = 0.000473\n",
      "epoch 161 loss = 0.005178\n",
      "epoch 162 loss = 0.000493\n",
      "epoch 163 loss = 0.001071\n",
      "epoch 164 loss = 0.000435\n",
      "epoch 165 loss = 0.000261\n",
      "epoch 166 loss = 0.000350\n",
      "epoch 167 loss = 0.000227\n",
      "epoch 168 loss = 0.022065\n",
      "epoch 169 loss = 0.001322\n",
      "epoch 170 loss = 0.000956\n",
      "epoch 171 loss = 0.002295\n",
      "epoch 172 loss = 0.004352\n",
      "epoch 173 loss = 0.000602\n",
      "epoch 174 loss = 0.004194\n",
      "epoch 175 loss = 0.003592\n",
      "epoch 176 loss = 0.025536\n",
      "epoch 177 loss = 0.001107\n",
      "epoch 178 loss = 0.001011\n",
      "epoch 179 loss = 0.000486\n",
      "epoch 180 loss = 0.002041\n",
      "epoch 181 loss = 0.000763\n",
      "epoch 182 loss = 0.000969\n",
      "epoch 183 loss = 0.014449\n",
      "epoch 184 loss = 0.001809\n",
      "epoch 185 loss = 0.001212\n",
      "epoch 186 loss = 0.002300\n",
      "epoch 187 loss = 0.001272\n",
      "epoch 188 loss = 0.001631\n",
      "epoch 189 loss = 0.000469\n",
      "epoch 190 loss = 0.001106\n",
      "epoch 191 loss = 0.000284\n",
      "epoch 192 loss = 0.000708\n",
      "epoch 193 loss = 0.000568\n",
      "epoch 194 loss = 0.009111\n",
      "epoch 195 loss = 0.003476\n",
      "epoch 196 loss = 0.000470\n",
      "epoch 197 loss = 0.002806\n",
      "epoch 198 loss = 0.008266\n",
      "epoch 199 loss = 0.000422\n",
      "epoch 200 loss = 0.002418\n",
      "epoch 201 loss = 0.000525\n",
      "epoch 202 loss = 0.003201\n",
      "epoch 203 loss = 0.001024\n",
      "epoch 204 loss = 0.001349\n",
      "epoch 205 loss = 0.004742\n",
      "epoch 206 loss = 0.001458\n",
      "epoch 207 loss = 0.010557\n",
      "epoch 208 loss = 0.003749\n",
      "epoch 209 loss = 0.001251\n",
      "epoch 210 loss = 0.001204\n",
      "epoch 211 loss = 0.000299\n",
      "epoch 212 loss = 0.001300\n",
      "epoch 213 loss = 0.011297\n",
      "epoch 214 loss = 0.000397\n",
      "epoch 215 loss = 0.107776\n",
      "epoch 216 loss = 0.002575\n",
      "epoch 217 loss = 0.007869\n",
      "epoch 218 loss = 0.002249\n",
      "epoch 219 loss = 0.008861\n",
      "epoch 220 loss = 0.002258\n",
      "epoch 221 loss = 0.007985\n",
      "epoch 222 loss = 0.000309\n",
      "epoch 223 loss = 0.001763\n",
      "epoch 224 loss = 0.003502\n",
      "epoch 225 loss = 0.004406\n",
      "epoch 226 loss = 0.004064\n",
      "epoch 227 loss = 0.067547\n",
      "epoch 228 loss = 0.001883\n",
      "epoch 229 loss = 0.001481\n",
      "epoch 230 loss = 0.001753\n",
      "epoch 231 loss = 0.000607\n",
      "epoch 232 loss = 0.003254\n",
      "epoch 233 loss = 0.000664\n",
      "epoch 234 loss = 0.010818\n",
      "epoch 235 loss = 0.006889\n",
      "epoch 236 loss = 0.000336\n",
      "epoch 237 loss = 0.001345\n",
      "epoch 238 loss = 0.012343\n",
      "epoch 239 loss = 0.001435\n",
      "epoch 240 loss = 0.012657\n",
      "epoch 241 loss = 0.000702\n",
      "epoch 242 loss = 0.001642\n",
      "epoch 243 loss = 0.000725\n",
      "epoch 244 loss = 0.000738\n",
      "epoch 245 loss = 0.001746\n",
      "epoch 246 loss = 0.001136\n",
      "epoch 247 loss = 0.013111\n",
      "epoch 248 loss = 0.003906\n",
      "epoch 249 loss = 0.007481\n",
      "epoch 250 loss = 0.001021\n",
      "epoch 251 loss = 0.000845\n",
      "epoch 252 loss = 0.007670\n"
     ]
    }
   ],
   "source": [
    "for subset_prop, dropout_rate, reg_strength, n_epoch in itertools.product(\n",
    "    subset_proportions,\n",
    "    dropout_rates, reg_strengths, \n",
    "    n_epochs,\n",
    "):  \n",
    "    # Reset the random number generator for each method (to produce identical results)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    pyro.set_rng_seed(random_seed)\n",
    "\n",
    "    # Print parameter combinations being tested\n",
    "    print(\n",
    "        \"subset %f, dropout_rate %f, reg_strength %f\"\n",
    "        % (subset_prop, dropout_rate, reg_strength))\n",
    "\n",
    "    print(\"n_epoch %d\" % n_epoch)\n",
    "    print()\n",
    "\n",
    "    \"\"\"\n",
    "    Results file storage\n",
    "    \"\"\"\n",
    "\n",
    "    # Create directory to store results for the current test configuration\n",
    "    test_results_path = os.path.join(\n",
    "        './test_results',\n",
    "        'error_convergence_2',\n",
    "        'CIFAR-10',\n",
    "        test_start_time,\n",
    "        (\n",
    "            str(subset_prop)\n",
    "            + '_' + str(dropout_rate) \n",
    "            + '_' + str(reg_strength)\n",
    "            + '_' + str(n_epoch)),\n",
    "    )\n",
    "\n",
    "    os.makedirs(test_results_path, exist_ok=True)\n",
    "\n",
    "    test_results_accuracy_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_mc.txt\"\n",
    "    )\n",
    "\n",
    "    test_results_accuracy_non_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_non_mc.txt\"\n",
    "    )    \n",
    "\n",
    "    \"\"\"\n",
    "    Dataset multiple splits prep\n",
    "    \"\"\"\n",
    "    # Prepare new subset of the original dataset\n",
    "    subset = datasets.CIFAR10(\n",
    "        root='./datasets_files', limit_size=subset_prop, transform=transform, download=True)\n",
    "\n",
    "    # Determine sizes of training and testing set\n",
    "    train_size = int(dataset_train_size * len(subset))\n",
    "    test_size = len(subset) - train_size\n",
    "\n",
    "    # Print the size of the subset\n",
    "    print(\"subset size = \" + str(subset.data.shape))\n",
    "    print(\"training set size = %d\" % train_size)\n",
    "    print(\"test set size = %d\" % test_size)\n",
    "    print()\n",
    "\n",
    "    # Prepare multiple sets of random train-test splits \n",
    "    # to test the parameter combination\n",
    "    subset_splits = []\n",
    "\n",
    "    for _ in range(n_splits):\n",
    "        train, test = random_split(subset, lengths=[train_size, test_size])\n",
    "        subset_splits.append((train, test))\n",
    "\n",
    "    \"\"\"\n",
    "    Training & testing\n",
    "    \"\"\"\n",
    "\n",
    "    # Try learning with different splits\n",
    "    for s, (train, test) in enumerate(subset_splits):\n",
    "\n",
    "        \"\"\"\n",
    "        Training\n",
    "        \"\"\"\n",
    "\n",
    "        print('Training with split %d' % s)\n",
    "\n",
    "        train_loader = DataLoader(train, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "        # Prepare network\n",
    "        network = models.SimpleCIFAR10MCDropout(\n",
    "            dropout_rate=dropout_rate,\n",
    "            dropout_type='bernoulli',\n",
    "        )\n",
    "\n",
    "        # Send the whole model to the selected torch.device\n",
    "        network.to(torch_device)\n",
    "\n",
    "        # Model to train mode\n",
    "        network.train()\n",
    "\n",
    "        # Adam optimizer\n",
    "        # https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam\n",
    "        # NOTE: Need to set L2 regularization from here\n",
    "        optimizer = optim.Adam(\n",
    "            network.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=reg_strength, # L2 regularization\n",
    "        )\n",
    "\n",
    "        # Record training start time (for this split)\n",
    "        tic = time.time()\n",
    "\n",
    "        for epoch in range(n_epoch): # loop over the dataset multiple times\n",
    "            # Mini-batches\n",
    "            for data in train_loader:\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, targets = data\n",
    "\n",
    "                # Store the batch to torch_device's memory\n",
    "                inputs = inputs.to(torch_device)\n",
    "                targets = targets.to(torch_device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = network(inputs)\n",
    "\n",
    "                loss = objective(outputs, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            print(\"epoch %d loss = %f\" % (epoch, loss.item()))\n",
    "\n",
    "        # Record training end time\n",
    "        toc = time.time()\n",
    "\n",
    "        # Report the final loss\n",
    "        print(\"final loss = %f\" % (loss.item()))\n",
    "\n",
    "        \"\"\"\n",
    "        Testing\n",
    "        \"\"\"\n",
    "\n",
    "        # Model to eval mode\n",
    "        network.eval()\n",
    "\n",
    "        # Store the batch to torch_device's memory\n",
    "        test_loader = DataLoader(test, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "        # Record testing start time\n",
    "        tic_testing = time.time()\n",
    "\n",
    "        _, mean, metrics = network.predict_dist(test_loader, n_prediction, torch_device)\n",
    "\n",
    "        # Record testing end time\n",
    "        toc_testing = time.time()\n",
    "\n",
    "        # Record all the scores to the score files\n",
    "        if len(metrics) > 0:\n",
    "            for key, value in metrics.items():\n",
    "                print(str(key) + \" = \" + str(value))\n",
    "\n",
    "                if key == 'accuracy_mc':\n",
    "                    with open(test_results_accuracy_mc_path, 'a+') as accuracy_mc_file:\n",
    "                        accuracy_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "                elif key == 'accuracy_non_mc':\n",
    "                    with open(test_results_accuracy_non_mc_path, 'a+') as accuracy_non_mc_file:\n",
    "                        accuracy_non_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "        # Report the total training time\n",
    "        print(\"training time = \" + str(toc - tic) + \" seconds\")\n",
    "\n",
    "        # Report the total testing time\n",
    "        print(\"testing time = \" + str(toc_testing - tic_testing) + \" seconds\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDxkRM5aVrdf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment_convergence_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
