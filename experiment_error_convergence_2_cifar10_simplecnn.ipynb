{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bka_bK83VFHh"
   },
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D5NSPs_cJZe"
   },
   "source": [
    "### Random seed / PyTorch / CUDA related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13750,
     "status": "ok",
     "timestamp": 1575180944142,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "pHbfpytEVFHu",
    "outputId": "fd0feb97-838c-4192-a398-8b8fa37eaa4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.40.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.3.1)\n",
      "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.10.1)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.4)\n",
      "/content/drive/My Drive/Colab Notebooks/bayesian-dl-experiments\n",
      "datasets_files\n",
      "experiment_comparison_toy.ipynb\n",
      "experiment_error_convergence_1_uci_fcnet.ipynb\n",
      "experiment_error_convergence_2_cifar10_simplecnn.ipynb\n",
      "experiment_number_of_test_predictions_1_uci_fcnet.ipynb\n",
      "experiment_number_of_test_predictions_2_cifar10_simplecnn.ipynb\n",
      "LICENSE\n",
      "README.md\n",
      "ronald_bdl\n",
      "test_results\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # If there's a package I need to install separately, do it here\n",
    "    !pip install pyro-ppl\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd 'drive/My Drive/Colab Notebooks/bayesian-dl-experiments'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls\n",
    "\n",
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seeds\n",
    "# Based on https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random_seed = 682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqlpuws9Y-8U"
   },
   "source": [
    "### Third party libraries (NumPy, PyTorch, Pyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13733,
     "status": "ok",
     "timestamp": 1575180944147,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "2zNVvKmZY-8X",
    "outputId": "3ca382ed-1dbf-4a53-aa08-c7a67860462d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.8 (default, Oct  7 2019, 12:59:55) \n",
      "[GCC 8.3.0]\n",
      "NumPy Version: 1.17.4\n",
      "PyTorch Version: 1.3.1\n",
      "Pyro Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Third party libraries import\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print version information\n",
    "print(\"Python Version: \" + sys.version)\n",
    "print(\"NumPy Version: \" + np.__version__)\n",
    "print(\"PyTorch Version: \" + torch.__version__)\n",
    "print(\"Pyro Version: \" + pyro.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14626,
     "status": "ok",
     "timestamp": 1575180945068,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "uyRIfCC5Y-8g",
    "outputId": "d3100c67-7d75-4faf-82a3-fc7889963e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 10.1.243\n",
      "cuDNN Version: 7603\n",
      "CUDA Device Name: Tesla P100-PCIE-16GB\n",
      "CUDA Capabilities: (6, 0)\n"
     ]
    }
   ],
   "source": [
    "# More imports...\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader, RandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pyro.infer import SVI, Trace_ELBO, HMC, MCMC\n",
    "\n",
    "# Import model and dataset classes from ronald_bdl\n",
    "from ronald_bdl import models, datasets\n",
    "\n",
    "# pyplot setting\n",
    "%matplotlib inline\n",
    "\n",
    "# torch.device / CUDA Setup\n",
    "use_cuda = True\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Disable 'benchmark' mode\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    use_pin_memory = True # Faster Host to GPU copies with page-locked memory\n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIFRoH3AcJZn"
   },
   "source": [
    "### Variable settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGRg2u0Q_I3n"
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2gc_i7T_HVw"
   },
   "outputs": [],
   "source": [
    "# CIFAR10 data transformation setting\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Set the proportion of the original dataset to be available as a whole\n",
    "subset_proportions = [0.05]\n",
    "\n",
    "# Proportion of the dataset to be used for training\n",
    "dataset_train_size = 0.8\n",
    "\n",
    "# Number of dataset splits\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzxIZiUcA8D8"
   },
   "source": [
    "#### NN settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_pzGq1_cJZp"
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "dropout_rates = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Regularization strengths\n",
    "reg_strengths = [0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuTnXABzVFKI"
   },
   "source": [
    "\n",
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p19qFgSAVFKS"
   },
   "outputs": [],
   "source": [
    "# Epochs\n",
    "n_epochs = [10, 100, 500]\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 0.005 # PyTorch default value is 0.001\n",
    "\n",
    "# Training data batch sizes\n",
    "n_training_batch = 64\n",
    "\n",
    "# Number of test predictions (for each data point)\n",
    "n_prediction = 500\n",
    "\n",
    "# Cross Entropy to minimize\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test start time\n",
    "test_start_time = datetime.datetime.today().strftime('%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PpzPMI8VFKE"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6466304,
     "status": "ok",
     "timestamp": 1575187841811,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "m4kavCiTVFKf",
    "outputId": "8864b9b2-49f4-4278-e0a4-308c24648a88",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset 0.050000, dropout_rate 0.100000, reg_strength 0.050000\n",
      "n_epoch 10\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.317856\n",
      "epoch 1 loss = 2.252765\n",
      "epoch 2 loss = 2.174119\n",
      "epoch 3 loss = 2.100922\n",
      "epoch 4 loss = 2.185141\n",
      "epoch 5 loss = 2.091575\n",
      "epoch 6 loss = 2.150214\n",
      "epoch 7 loss = 1.900217\n",
      "epoch 8 loss = 1.905949\n",
      "epoch 9 loss = 1.917629\n",
      "final loss = 1.917629\n",
      "accuracy_mc = tensor(0.2513, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2554, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9965, device='cuda:0')\n",
      "training time = 3.6018831729888916 seconds\n",
      "testing time = 1.9399752616882324 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.309723\n",
      "epoch 1 loss = 2.286542\n",
      "epoch 2 loss = 2.233207\n",
      "epoch 3 loss = 2.222860\n",
      "epoch 4 loss = 2.227787\n",
      "epoch 5 loss = 2.173721\n",
      "epoch 6 loss = 2.126311\n",
      "epoch 7 loss = 2.125345\n",
      "epoch 8 loss = 2.085964\n",
      "epoch 9 loss = 1.979229\n",
      "final loss = 1.979229\n",
      "accuracy_mc = tensor(0.2638, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2599, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9757, device='cuda:0')\n",
      "training time = 3.689837694168091 seconds\n",
      "testing time = 1.8798558712005615 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.285139\n",
      "epoch 1 loss = 2.281767\n",
      "epoch 2 loss = 2.280556\n",
      "epoch 3 loss = 2.258152\n",
      "epoch 4 loss = 2.235668\n",
      "epoch 5 loss = 2.142098\n",
      "epoch 6 loss = 2.117823\n",
      "epoch 7 loss = 2.052528\n",
      "epoch 8 loss = 2.062834\n",
      "epoch 9 loss = 2.065474\n",
      "final loss = 2.065474\n",
      "accuracy_mc = tensor(0.2270, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2316, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0991, device='cuda:0')\n",
      "training time = 3.5580251216888428 seconds\n",
      "testing time = 1.8960576057434082 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.293510\n",
      "epoch 1 loss = 2.253445\n",
      "epoch 2 loss = 2.215032\n",
      "epoch 3 loss = 2.181492\n",
      "epoch 4 loss = 2.062757\n",
      "epoch 5 loss = 1.913109\n",
      "epoch 6 loss = 1.913736\n",
      "epoch 7 loss = 1.845500\n",
      "epoch 8 loss = 1.961760\n",
      "epoch 9 loss = 1.922032\n",
      "final loss = 1.922032\n",
      "accuracy_mc = tensor(0.2547, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2647, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0336, device='cuda:0')\n",
      "training time = 3.630283832550049 seconds\n",
      "testing time = 1.920241117477417 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 2.296307\n",
      "epoch 1 loss = 2.247617\n",
      "epoch 2 loss = 2.232392\n",
      "epoch 3 loss = 2.212340\n",
      "epoch 4 loss = 2.238401\n",
      "epoch 5 loss = 2.122577\n",
      "epoch 6 loss = 2.069609\n",
      "epoch 7 loss = 1.942233\n",
      "epoch 8 loss = 1.893514\n",
      "epoch 9 loss = 1.973176\n",
      "final loss = 1.973176\n",
      "accuracy_mc = tensor(0.2388, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2424, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0621, device='cuda:0')\n",
      "training time = 3.5626726150512695 seconds\n",
      "testing time = 1.8820059299468994 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.303029\n",
      "epoch 1 loss = 2.264925\n",
      "epoch 2 loss = 2.275677\n",
      "epoch 3 loss = 2.271236\n",
      "epoch 4 loss = 2.264003\n",
      "epoch 5 loss = 2.315243\n",
      "epoch 6 loss = 2.315717\n",
      "epoch 7 loss = 2.261100\n",
      "epoch 8 loss = 2.242210\n",
      "epoch 9 loss = 2.189731\n",
      "final loss = 2.189731\n",
      "accuracy_mc = tensor(0.2610, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2696, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0451, device='cuda:0')\n",
      "training time = 3.592668294906616 seconds\n",
      "testing time = 1.9003500938415527 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.265980\n",
      "epoch 1 loss = 2.197982\n",
      "epoch 2 loss = 2.151912\n",
      "epoch 3 loss = 2.214257\n",
      "epoch 4 loss = 2.109260\n",
      "epoch 5 loss = 2.110786\n",
      "epoch 6 loss = 2.369677\n",
      "epoch 7 loss = 2.276383\n",
      "epoch 8 loss = 2.219980\n",
      "epoch 9 loss = 2.287485\n",
      "final loss = 2.287485\n",
      "accuracy_mc = tensor(0.2613, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2718, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9206, device='cuda:0')\n",
      "training time = 3.5655953884124756 seconds\n",
      "testing time = 1.9181244373321533 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.268834\n",
      "epoch 1 loss = 2.254382\n",
      "epoch 2 loss = 2.078202\n",
      "epoch 3 loss = 1.985939\n",
      "epoch 4 loss = 1.953857\n",
      "epoch 5 loss = 1.946997\n",
      "epoch 6 loss = 1.876917\n",
      "epoch 7 loss = 1.984446\n",
      "epoch 8 loss = 1.845273\n",
      "epoch 9 loss = 1.811852\n",
      "final loss = 1.811852\n",
      "accuracy_mc = tensor(0.2869, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2837, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9731, device='cuda:0')\n",
      "training time = 3.5949316024780273 seconds\n",
      "testing time = 1.8709232807159424 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.296458\n",
      "epoch 1 loss = 2.267522\n",
      "epoch 2 loss = 2.228511\n",
      "epoch 3 loss = 2.181428\n",
      "epoch 4 loss = 2.113856\n",
      "epoch 5 loss = 2.072294\n",
      "epoch 6 loss = 2.144771\n",
      "epoch 7 loss = 2.069030\n",
      "epoch 8 loss = 2.116164\n",
      "epoch 9 loss = 2.034928\n",
      "final loss = 2.034928\n",
      "accuracy_mc = tensor(0.2848, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2804, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0501, device='cuda:0')\n",
      "training time = 3.539104700088501 seconds\n",
      "testing time = 1.8508853912353516 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 2.253758\n",
      "epoch 1 loss = 2.211853\n",
      "epoch 2 loss = 2.141693\n",
      "epoch 3 loss = 2.070978\n",
      "epoch 4 loss = 2.090707\n",
      "epoch 5 loss = 2.093765\n",
      "epoch 6 loss = 2.042633\n",
      "epoch 7 loss = 1.896639\n",
      "epoch 8 loss = 1.946186\n",
      "epoch 9 loss = 1.917725\n",
      "final loss = 1.917725\n",
      "accuracy_mc = tensor(0.2712, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3046, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9725, device='cuda:0')\n",
      "training time = 3.480754852294922 seconds\n",
      "testing time = 1.901308536529541 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.100000, reg_strength 0.050000\n",
      "n_epoch 100\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.317856\n",
      "epoch 1 loss = 2.252765\n",
      "epoch 2 loss = 2.174119\n",
      "epoch 3 loss = 2.100922\n",
      "epoch 4 loss = 2.185141\n",
      "epoch 5 loss = 2.091575\n",
      "epoch 6 loss = 2.150214\n",
      "epoch 7 loss = 1.900217\n",
      "epoch 8 loss = 1.905949\n",
      "epoch 9 loss = 1.917629\n",
      "epoch 10 loss = 1.886251\n",
      "epoch 11 loss = 1.892022\n",
      "epoch 12 loss = 1.898929\n",
      "epoch 13 loss = 1.917226\n",
      "epoch 14 loss = 1.753546\n",
      "epoch 15 loss = 2.001490\n",
      "epoch 16 loss = 1.839867\n",
      "epoch 17 loss = 1.753230\n",
      "epoch 18 loss = 1.955188\n",
      "epoch 19 loss = 1.768133\n",
      "epoch 20 loss = 1.800068\n",
      "epoch 21 loss = 1.802749\n",
      "epoch 22 loss = 1.847447\n",
      "epoch 23 loss = 1.738507\n",
      "epoch 24 loss = 1.718504\n",
      "epoch 25 loss = 1.698337\n",
      "epoch 26 loss = 1.642361\n",
      "epoch 27 loss = 1.637040\n",
      "epoch 28 loss = 1.542509\n",
      "epoch 29 loss = 1.514600\n",
      "epoch 30 loss = 1.770178\n",
      "epoch 31 loss = 1.575148\n",
      "epoch 32 loss = 1.649191\n",
      "epoch 33 loss = 1.552578\n",
      "epoch 34 loss = 1.341750\n",
      "epoch 35 loss = 1.540254\n",
      "epoch 36 loss = 1.474427\n",
      "epoch 37 loss = 1.709211\n",
      "epoch 38 loss = 1.652411\n",
      "epoch 39 loss = 1.414580\n",
      "epoch 40 loss = 1.486777\n",
      "epoch 41 loss = 1.440903\n",
      "epoch 42 loss = 1.510480\n",
      "epoch 43 loss = 1.753461\n",
      "epoch 44 loss = 1.365193\n",
      "epoch 45 loss = 1.516225\n",
      "epoch 46 loss = 1.565745\n",
      "epoch 47 loss = 1.640988\n",
      "epoch 48 loss = 1.382540\n",
      "epoch 49 loss = 1.251514\n",
      "epoch 50 loss = 1.454218\n",
      "epoch 51 loss = 1.304464\n",
      "epoch 52 loss = 1.406613\n",
      "epoch 53 loss = 1.219122\n",
      "epoch 54 loss = 1.332033\n",
      "epoch 55 loss = 1.265636\n",
      "epoch 56 loss = 1.259180\n",
      "epoch 57 loss = 1.292711\n",
      "epoch 58 loss = 1.134335\n",
      "epoch 59 loss = 1.404328\n",
      "epoch 60 loss = 1.289568\n",
      "epoch 61 loss = 1.607781\n",
      "epoch 62 loss = 1.300573\n",
      "epoch 63 loss = 1.443587\n",
      "epoch 64 loss = 1.220234\n",
      "epoch 65 loss = 1.382396\n",
      "epoch 66 loss = 1.221771\n",
      "epoch 67 loss = 1.100482\n",
      "epoch 68 loss = 1.491554\n",
      "epoch 69 loss = 1.204298\n",
      "epoch 70 loss = 1.204234\n",
      "epoch 71 loss = 1.138319\n",
      "epoch 72 loss = 1.128270\n",
      "epoch 73 loss = 1.094257\n",
      "epoch 74 loss = 1.123292\n",
      "epoch 75 loss = 1.212409\n",
      "epoch 76 loss = 1.190230\n",
      "epoch 77 loss = 1.014180\n",
      "epoch 78 loss = 1.676319\n",
      "epoch 79 loss = 1.143987\n",
      "epoch 80 loss = 1.399117\n",
      "epoch 81 loss = 1.347387\n",
      "epoch 82 loss = 1.166762\n",
      "epoch 83 loss = 1.307499\n",
      "epoch 84 loss = 1.239277\n",
      "epoch 85 loss = 1.164706\n",
      "epoch 86 loss = 1.026600\n",
      "epoch 87 loss = 1.007407\n",
      "epoch 88 loss = 1.041210\n",
      "epoch 89 loss = 1.321449\n",
      "epoch 90 loss = 1.271776\n",
      "epoch 91 loss = 1.116017\n",
      "epoch 92 loss = 1.098562\n",
      "epoch 93 loss = 0.950442\n",
      "epoch 94 loss = 1.012095\n",
      "epoch 95 loss = 0.967540\n",
      "epoch 96 loss = 1.184044\n",
      "epoch 97 loss = 1.095813\n",
      "epoch 98 loss = 1.084234\n",
      "epoch 99 loss = 1.004203\n",
      "final loss = 1.004203\n",
      "accuracy_mc = tensor(0.3182, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3288, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.7908, device='cuda:0')\n",
      "training time = 35.37945222854614 seconds\n",
      "testing time = 1.8857109546661377 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.275008\n",
      "epoch 1 loss = 2.234120\n",
      "epoch 2 loss = 2.177522\n",
      "epoch 3 loss = 2.026494\n",
      "epoch 4 loss = 2.034169\n",
      "epoch 5 loss = 2.046046\n",
      "epoch 6 loss = 1.973289\n",
      "epoch 7 loss = 1.863603\n",
      "epoch 8 loss = 2.026568\n",
      "epoch 9 loss = 1.974901\n",
      "epoch 10 loss = 1.979121\n",
      "epoch 11 loss = 1.979827\n",
      "epoch 12 loss = 1.903984\n",
      "epoch 13 loss = 1.947881\n",
      "epoch 14 loss = 1.979768\n",
      "epoch 15 loss = 1.941973\n",
      "epoch 16 loss = 1.956561\n",
      "epoch 17 loss = 1.832596\n",
      "epoch 18 loss = 1.845020\n",
      "epoch 19 loss = 1.982807\n",
      "epoch 20 loss = 1.771557\n",
      "epoch 21 loss = 1.885026\n",
      "epoch 22 loss = 1.752138\n",
      "epoch 23 loss = 1.800820\n",
      "epoch 24 loss = 1.853804\n",
      "epoch 25 loss = 1.673104\n",
      "epoch 26 loss = 1.723497\n",
      "epoch 27 loss = 1.568895\n",
      "epoch 28 loss = 1.761660\n",
      "epoch 29 loss = 1.656354\n",
      "epoch 30 loss = 1.562611\n",
      "epoch 31 loss = 1.610471\n",
      "epoch 32 loss = 1.631231\n",
      "epoch 33 loss = 1.687668\n",
      "epoch 34 loss = 1.585142\n",
      "epoch 35 loss = 1.755900\n",
      "epoch 36 loss = 1.602762\n",
      "epoch 37 loss = 1.673310\n",
      "epoch 38 loss = 1.493421\n",
      "epoch 39 loss = 1.525251\n",
      "epoch 40 loss = 1.486328\n",
      "epoch 41 loss = 1.705758\n",
      "epoch 42 loss = 1.719219\n",
      "epoch 43 loss = 1.559797\n",
      "epoch 44 loss = 1.605220\n",
      "epoch 45 loss = 1.622548\n",
      "epoch 46 loss = 1.461872\n",
      "epoch 47 loss = 1.555507\n",
      "epoch 48 loss = 1.591209\n",
      "epoch 49 loss = 1.453795\n",
      "epoch 50 loss = 1.669053\n",
      "epoch 51 loss = 1.609394\n",
      "epoch 52 loss = 1.514704\n",
      "epoch 53 loss = 1.616357\n",
      "epoch 54 loss = 1.488143\n",
      "epoch 55 loss = 1.728838\n",
      "epoch 56 loss = 1.511505\n",
      "epoch 57 loss = 1.420618\n",
      "epoch 58 loss = 1.520320\n",
      "epoch 59 loss = 1.456200\n",
      "epoch 60 loss = 1.590123\n",
      "epoch 61 loss = 1.604155\n",
      "epoch 62 loss = 1.350396\n",
      "epoch 63 loss = 1.442990\n",
      "epoch 64 loss = 1.386182\n",
      "epoch 65 loss = 1.560856\n",
      "epoch 66 loss = 1.439102\n",
      "epoch 67 loss = 1.428428\n",
      "epoch 68 loss = 1.592575\n",
      "epoch 69 loss = 1.536880\n",
      "epoch 70 loss = 1.555750\n",
      "epoch 71 loss = 1.399818\n",
      "epoch 72 loss = 1.452975\n",
      "epoch 73 loss = 1.370342\n",
      "epoch 74 loss = 1.430613\n",
      "epoch 75 loss = 1.442440\n",
      "epoch 76 loss = 1.590731\n",
      "epoch 77 loss = 1.589822\n",
      "epoch 78 loss = 1.518037\n",
      "epoch 79 loss = 1.387283\n",
      "epoch 80 loss = 1.340015\n",
      "epoch 81 loss = 1.638836\n",
      "epoch 82 loss = 1.442709\n",
      "epoch 83 loss = 1.535716\n",
      "epoch 84 loss = 1.437097\n",
      "epoch 85 loss = 1.367737\n",
      "epoch 86 loss = 1.443259\n",
      "epoch 87 loss = 1.354631\n",
      "epoch 88 loss = 1.316364\n",
      "epoch 89 loss = 1.355350\n",
      "epoch 90 loss = 1.407670\n",
      "epoch 91 loss = 1.484030\n",
      "epoch 92 loss = 1.477950\n",
      "epoch 93 loss = 1.439577\n",
      "epoch 94 loss = 1.420429\n",
      "epoch 95 loss = 1.271268\n",
      "epoch 96 loss = 1.379767\n",
      "epoch 97 loss = 1.420976\n",
      "epoch 98 loss = 1.552585\n",
      "epoch 99 loss = 1.536544\n",
      "final loss = 1.536544\n",
      "accuracy_mc = tensor(0.3699, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3858, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6953, device='cuda:0')\n",
      "training time = 35.26247477531433 seconds\n",
      "testing time = 1.8926641941070557 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.273888\n",
      "epoch 1 loss = 2.282130\n",
      "epoch 2 loss = 2.287034\n",
      "epoch 3 loss = 2.197657\n",
      "epoch 4 loss = 2.157986\n",
      "epoch 5 loss = 2.101748\n",
      "epoch 6 loss = 2.101408\n",
      "epoch 7 loss = 1.990867\n",
      "epoch 8 loss = 1.968155\n",
      "epoch 9 loss = 1.942994\n",
      "epoch 10 loss = 1.935726\n",
      "epoch 11 loss = 1.923139\n",
      "epoch 12 loss = 1.959701\n",
      "epoch 13 loss = 1.876990\n",
      "epoch 14 loss = 1.869968\n",
      "epoch 15 loss = 1.921285\n",
      "epoch 16 loss = 1.761465\n",
      "epoch 17 loss = 1.792197\n",
      "epoch 18 loss = 1.789232\n",
      "epoch 19 loss = 1.907983\n",
      "epoch 20 loss = 1.775824\n",
      "epoch 21 loss = 1.716616\n",
      "epoch 22 loss = 1.760591\n",
      "epoch 23 loss = 1.726864\n",
      "epoch 24 loss = 1.756646\n",
      "epoch 25 loss = 1.745643\n",
      "epoch 26 loss = 1.726655\n",
      "epoch 27 loss = 1.721317\n",
      "epoch 28 loss = 1.697995\n",
      "epoch 29 loss = 1.788384\n",
      "epoch 30 loss = 1.773229\n",
      "epoch 31 loss = 1.700035\n",
      "epoch 32 loss = 1.685586\n",
      "epoch 33 loss = 1.792282\n",
      "epoch 34 loss = 1.753060\n",
      "epoch 35 loss = 1.716344\n",
      "epoch 36 loss = 1.725067\n",
      "epoch 37 loss = 1.622618\n",
      "epoch 38 loss = 1.639934\n",
      "epoch 39 loss = 1.632772\n",
      "epoch 40 loss = 1.736185\n",
      "epoch 41 loss = 1.666524\n",
      "epoch 42 loss = 1.701472\n",
      "epoch 43 loss = 1.599767\n",
      "epoch 44 loss = 1.614982\n",
      "epoch 45 loss = 1.651111\n",
      "epoch 46 loss = 1.600804\n",
      "epoch 47 loss = 1.626014\n",
      "epoch 48 loss = 1.705487\n",
      "epoch 49 loss = 1.602111\n",
      "epoch 50 loss = 1.548986\n",
      "epoch 51 loss = 1.617488\n",
      "epoch 52 loss = 1.535335\n",
      "epoch 53 loss = 1.569843\n",
      "epoch 54 loss = 1.597352\n",
      "epoch 55 loss = 1.592430\n",
      "epoch 56 loss = 1.506995\n",
      "epoch 57 loss = 1.571985\n",
      "epoch 58 loss = 1.593437\n",
      "epoch 59 loss = 1.609251\n",
      "epoch 60 loss = 1.458690\n",
      "epoch 61 loss = 1.688152\n",
      "epoch 62 loss = 1.713391\n",
      "epoch 63 loss = 1.631857\n",
      "epoch 64 loss = 1.662258\n",
      "epoch 65 loss = 1.648826\n",
      "epoch 66 loss = 1.651149\n",
      "epoch 67 loss = 1.408525\n",
      "epoch 68 loss = 1.530656\n",
      "epoch 69 loss = 1.504994\n",
      "epoch 70 loss = 1.753489\n",
      "epoch 71 loss = 1.648442\n",
      "epoch 72 loss = 1.512783\n",
      "epoch 73 loss = 1.570223\n",
      "epoch 74 loss = 1.411018\n",
      "epoch 75 loss = 1.467006\n",
      "epoch 76 loss = 1.449642\n",
      "epoch 77 loss = 1.431215\n",
      "epoch 78 loss = 1.318036\n",
      "epoch 79 loss = 1.397624\n",
      "epoch 80 loss = 1.367370\n",
      "epoch 81 loss = 1.390384\n",
      "epoch 82 loss = 1.444016\n",
      "epoch 83 loss = 1.407399\n",
      "epoch 84 loss = 1.526521\n",
      "epoch 85 loss = 1.538865\n",
      "epoch 86 loss = 1.415161\n",
      "epoch 87 loss = 1.656192\n",
      "epoch 88 loss = 1.484362\n",
      "epoch 89 loss = 1.398360\n",
      "epoch 90 loss = 1.445677\n",
      "epoch 91 loss = 1.403521\n",
      "epoch 92 loss = 1.253453\n",
      "epoch 93 loss = 1.484316\n",
      "epoch 94 loss = 1.512057\n",
      "epoch 95 loss = 1.590769\n",
      "epoch 96 loss = 1.418041\n",
      "epoch 97 loss = 1.318431\n",
      "epoch 98 loss = 1.467966\n",
      "epoch 99 loss = 1.468578\n",
      "final loss = 1.468578\n",
      "accuracy_mc = tensor(0.3607, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3612, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6902, device='cuda:0')\n",
      "training time = 35.427624464035034 seconds\n",
      "testing time = 1.8489961624145508 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.250199\n",
      "epoch 1 loss = 2.170565\n",
      "epoch 2 loss = 2.005353\n",
      "epoch 3 loss = 2.018360\n",
      "epoch 4 loss = 1.877194\n",
      "epoch 5 loss = 1.843845\n",
      "epoch 6 loss = 1.793700\n",
      "epoch 7 loss = 1.902975\n",
      "epoch 8 loss = 1.806589\n",
      "epoch 9 loss = 1.699694\n",
      "epoch 10 loss = 1.771272\n",
      "epoch 11 loss = 1.651045\n",
      "epoch 12 loss = 1.785641\n",
      "epoch 13 loss = 1.827320\n",
      "epoch 14 loss = 1.736159\n",
      "epoch 15 loss = 1.707291\n",
      "epoch 16 loss = 1.738745\n",
      "epoch 17 loss = 1.804497\n",
      "epoch 18 loss = 1.580943\n",
      "epoch 19 loss = 1.660996\n",
      "epoch 20 loss = 1.700079\n",
      "epoch 21 loss = 1.623676\n",
      "epoch 22 loss = 1.660566\n",
      "epoch 23 loss = 1.778592\n",
      "epoch 24 loss = 1.571402\n",
      "epoch 25 loss = 1.637527\n",
      "epoch 26 loss = 1.636824\n",
      "epoch 27 loss = 1.668831\n",
      "epoch 28 loss = 1.621516\n",
      "epoch 29 loss = 1.604504\n",
      "epoch 30 loss = 1.706415\n",
      "epoch 31 loss = 1.608871\n",
      "epoch 32 loss = 1.577183\n",
      "epoch 33 loss = 1.447632\n",
      "epoch 34 loss = 1.489614\n",
      "epoch 35 loss = 1.728180\n",
      "epoch 36 loss = 1.521663\n",
      "epoch 37 loss = 1.546414\n",
      "epoch 38 loss = 1.547135\n",
      "epoch 39 loss = 1.577670\n",
      "epoch 40 loss = 1.458727\n",
      "epoch 41 loss = 1.685914\n",
      "epoch 42 loss = 1.484970\n",
      "epoch 43 loss = 1.662399\n",
      "epoch 44 loss = 1.461181\n",
      "epoch 45 loss = 1.456135\n",
      "epoch 46 loss = 1.401418\n",
      "epoch 47 loss = 1.574467\n",
      "epoch 48 loss = 1.422898\n",
      "epoch 49 loss = 1.389672\n",
      "epoch 50 loss = 1.482744\n",
      "epoch 51 loss = 1.710719\n",
      "epoch 52 loss = 1.468749\n",
      "epoch 53 loss = 1.331615\n",
      "epoch 54 loss = 1.642000\n",
      "epoch 55 loss = 1.173525\n",
      "epoch 56 loss = 1.503225\n",
      "epoch 57 loss = 1.397270\n",
      "epoch 58 loss = 1.373476\n",
      "epoch 59 loss = 1.305222\n",
      "epoch 60 loss = 1.324094\n",
      "epoch 61 loss = 1.562918\n",
      "epoch 62 loss = 1.338992\n",
      "epoch 63 loss = 1.448450\n",
      "epoch 64 loss = 1.474306\n",
      "epoch 65 loss = 1.250268\n",
      "epoch 66 loss = 1.560788\n",
      "epoch 67 loss = 1.448422\n",
      "epoch 68 loss = 1.247946\n",
      "epoch 69 loss = 1.395247\n",
      "epoch 70 loss = 1.362887\n",
      "epoch 71 loss = 1.414245\n",
      "epoch 72 loss = 1.377080\n",
      "epoch 73 loss = 1.480096\n",
      "epoch 74 loss = 1.279438\n",
      "epoch 75 loss = 1.336671\n",
      "epoch 76 loss = 1.151052\n",
      "epoch 77 loss = 1.356854\n",
      "epoch 78 loss = 1.554008\n",
      "epoch 79 loss = 1.396311\n",
      "epoch 80 loss = 1.561118\n",
      "epoch 81 loss = 1.268417\n",
      "epoch 82 loss = 1.316284\n",
      "epoch 83 loss = 1.220100\n",
      "epoch 84 loss = 1.249502\n",
      "epoch 85 loss = 1.480160\n",
      "epoch 86 loss = 1.101918\n",
      "epoch 87 loss = 1.211159\n",
      "epoch 88 loss = 1.192062\n",
      "epoch 89 loss = 1.193736\n",
      "epoch 90 loss = 1.436699\n",
      "epoch 91 loss = 1.224771\n",
      "epoch 92 loss = 1.260947\n",
      "epoch 93 loss = 1.096761\n",
      "epoch 94 loss = 1.120645\n",
      "epoch 95 loss = 1.193238\n",
      "epoch 96 loss = 1.044423\n",
      "epoch 97 loss = 1.108913\n",
      "epoch 98 loss = 0.913230\n",
      "epoch 99 loss = 1.189638\n",
      "final loss = 1.189638\n",
      "accuracy_mc = tensor(0.3628, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3639, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.7420, device='cuda:0')\n",
      "training time = 35.05681347846985 seconds\n",
      "testing time = 1.8528153896331787 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 2.270549\n",
      "epoch 1 loss = 2.173491\n",
      "epoch 2 loss = 2.103803\n",
      "epoch 3 loss = 1.986669\n",
      "epoch 4 loss = 1.938525\n",
      "epoch 5 loss = 1.818918\n",
      "epoch 6 loss = 1.772572\n",
      "epoch 7 loss = 1.698220\n",
      "epoch 8 loss = 1.737228\n",
      "epoch 9 loss = 1.804903\n",
      "epoch 10 loss = 1.892358\n",
      "epoch 11 loss = 1.716900\n",
      "epoch 12 loss = 1.764323\n",
      "epoch 13 loss = 1.872074\n",
      "epoch 14 loss = 1.809193\n",
      "epoch 15 loss = 1.725892\n",
      "epoch 16 loss = 1.889509\n",
      "epoch 17 loss = 1.850384\n",
      "epoch 18 loss = 1.709218\n",
      "epoch 19 loss = 1.825442\n",
      "epoch 20 loss = 1.902776\n",
      "epoch 21 loss = 1.842617\n",
      "epoch 22 loss = 1.720574\n",
      "epoch 23 loss = 1.754194\n",
      "epoch 24 loss = 1.828588\n",
      "epoch 25 loss = 1.844737\n",
      "epoch 26 loss = 1.801297\n",
      "epoch 27 loss = 1.755718\n",
      "epoch 28 loss = 1.764442\n",
      "epoch 29 loss = 1.705356\n",
      "epoch 30 loss = 1.603765\n",
      "epoch 31 loss = 1.634834\n",
      "epoch 32 loss = 1.646671\n",
      "epoch 33 loss = 1.630669\n",
      "epoch 34 loss = 1.528354\n",
      "epoch 35 loss = 1.609661\n",
      "epoch 36 loss = 1.746492\n",
      "epoch 37 loss = 1.572956\n",
      "epoch 38 loss = 1.557570\n",
      "epoch 39 loss = 1.773345\n",
      "epoch 40 loss = 1.745014\n",
      "epoch 41 loss = 1.492395\n",
      "epoch 42 loss = 1.520693\n",
      "epoch 43 loss = 1.555914\n",
      "epoch 44 loss = 1.504639\n",
      "epoch 45 loss = 1.670356\n",
      "epoch 46 loss = 1.573867\n",
      "epoch 47 loss = 1.505985\n",
      "epoch 48 loss = 1.503668\n",
      "epoch 49 loss = 1.721895\n",
      "epoch 50 loss = 1.338345\n",
      "epoch 51 loss = 1.513917\n",
      "epoch 52 loss = 1.511298\n",
      "epoch 53 loss = 1.431917\n",
      "epoch 54 loss = 1.438380\n",
      "epoch 55 loss = 1.495289\n",
      "epoch 56 loss = 1.432566\n",
      "epoch 57 loss = 1.432620\n",
      "epoch 58 loss = 1.330531\n",
      "epoch 59 loss = 1.596819\n",
      "epoch 60 loss = 1.553511\n",
      "epoch 61 loss = 1.486147\n",
      "epoch 62 loss = 1.491956\n",
      "epoch 63 loss = 1.547137\n",
      "epoch 64 loss = 1.442054\n",
      "epoch 65 loss = 1.436392\n",
      "epoch 66 loss = 1.440362\n",
      "epoch 67 loss = 1.477961\n",
      "epoch 68 loss = 1.381698\n",
      "epoch 69 loss = 1.521120\n",
      "epoch 70 loss = 1.359318\n",
      "epoch 71 loss = 1.451342\n",
      "epoch 72 loss = 1.376854\n",
      "epoch 73 loss = 1.278154\n",
      "epoch 74 loss = 1.357033\n",
      "epoch 75 loss = 1.340526\n",
      "epoch 76 loss = 1.748336\n",
      "epoch 77 loss = 1.401307\n",
      "epoch 78 loss = 1.377185\n",
      "epoch 79 loss = 1.488455\n",
      "epoch 80 loss = 1.435206\n",
      "epoch 81 loss = 1.420430\n",
      "epoch 82 loss = 1.358914\n",
      "epoch 83 loss = 1.503600\n",
      "epoch 84 loss = 1.413220\n",
      "epoch 85 loss = 1.209094\n",
      "epoch 86 loss = 1.604112\n",
      "epoch 87 loss = 1.288688\n",
      "epoch 88 loss = 1.263935\n",
      "epoch 89 loss = 1.425530\n",
      "epoch 90 loss = 1.301697\n",
      "epoch 91 loss = 1.340812\n",
      "epoch 92 loss = 1.297297\n",
      "epoch 93 loss = 1.421855\n",
      "epoch 94 loss = 1.309573\n",
      "epoch 95 loss = 1.241600\n",
      "epoch 96 loss = 1.297613\n",
      "epoch 97 loss = 1.364689\n",
      "epoch 98 loss = 1.250955\n",
      "epoch 99 loss = 1.414774\n",
      "final loss = 1.414774\n",
      "accuracy_mc = tensor(0.4086, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4032, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6278, device='cuda:0')\n",
      "training time = 35.032270193099976 seconds\n",
      "testing time = 1.8875823020935059 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.284677\n",
      "epoch 1 loss = 2.250950\n",
      "epoch 2 loss = 2.251920\n",
      "epoch 3 loss = 2.233222\n",
      "epoch 4 loss = 2.206735\n",
      "epoch 5 loss = 2.272102\n",
      "epoch 6 loss = 2.220901\n",
      "epoch 7 loss = 2.234575\n",
      "epoch 8 loss = 2.127787\n",
      "epoch 9 loss = 2.089198\n",
      "epoch 10 loss = 2.103641\n",
      "epoch 11 loss = 2.087562\n",
      "epoch 12 loss = 2.065744\n",
      "epoch 13 loss = 2.026856\n",
      "epoch 14 loss = 2.091164\n",
      "epoch 15 loss = 2.018694\n",
      "epoch 16 loss = 1.998439\n",
      "epoch 17 loss = 2.043446\n",
      "epoch 18 loss = 1.990619\n",
      "epoch 19 loss = 2.085783\n",
      "epoch 20 loss = 2.013237\n",
      "epoch 21 loss = 1.885033\n",
      "epoch 22 loss = 1.924875\n",
      "epoch 23 loss = 2.009012\n",
      "epoch 24 loss = 1.862664\n",
      "epoch 25 loss = 1.950703\n",
      "epoch 26 loss = 2.072242\n",
      "epoch 27 loss = 1.909030\n",
      "epoch 28 loss = 1.952058\n",
      "epoch 29 loss = 1.992799\n",
      "epoch 30 loss = 1.751139\n",
      "epoch 31 loss = 1.921297\n",
      "epoch 32 loss = 1.976675\n",
      "epoch 33 loss = 1.810665\n",
      "epoch 34 loss = 1.948096\n",
      "epoch 35 loss = 1.929245\n",
      "epoch 36 loss = 1.858985\n",
      "epoch 37 loss = 1.781366\n",
      "epoch 38 loss = 1.818247\n",
      "epoch 39 loss = 1.743015\n",
      "epoch 40 loss = 1.781056\n",
      "epoch 41 loss = 1.897669\n",
      "epoch 42 loss = 1.820296\n",
      "epoch 43 loss = 1.764608\n",
      "epoch 44 loss = 1.885181\n",
      "epoch 45 loss = 1.825065\n",
      "epoch 46 loss = 1.712274\n",
      "epoch 47 loss = 1.769638\n",
      "epoch 48 loss = 1.725703\n",
      "epoch 49 loss = 1.756717\n",
      "epoch 50 loss = 1.994922\n",
      "epoch 51 loss = 1.742499\n",
      "epoch 52 loss = 1.790974\n",
      "epoch 53 loss = 1.798311\n",
      "epoch 54 loss = 1.741930\n",
      "epoch 55 loss = 1.846746\n",
      "epoch 56 loss = 1.660275\n",
      "epoch 57 loss = 1.676577\n",
      "epoch 58 loss = 1.618128\n",
      "epoch 59 loss = 1.728547\n",
      "epoch 60 loss = 1.728867\n",
      "epoch 61 loss = 1.748105\n",
      "epoch 62 loss = 1.747676\n",
      "epoch 63 loss = 1.627156\n",
      "epoch 64 loss = 1.671638\n",
      "epoch 65 loss = 1.774446\n",
      "epoch 66 loss = 1.687729\n",
      "epoch 67 loss = 1.547221\n",
      "epoch 68 loss = 1.631357\n",
      "epoch 69 loss = 1.738414\n",
      "epoch 70 loss = 1.679824\n",
      "epoch 71 loss = 1.573501\n",
      "epoch 72 loss = 1.544547\n",
      "epoch 73 loss = 1.571079\n",
      "epoch 74 loss = 1.650849\n",
      "epoch 75 loss = 1.538020\n",
      "epoch 76 loss = 1.860420\n",
      "epoch 77 loss = 1.684795\n",
      "epoch 78 loss = 1.743209\n",
      "epoch 79 loss = 1.631027\n",
      "epoch 80 loss = 1.672510\n",
      "epoch 81 loss = 1.617486\n",
      "epoch 82 loss = 1.653539\n",
      "epoch 83 loss = 1.437150\n",
      "epoch 84 loss = 1.535998\n",
      "epoch 85 loss = 1.773226\n",
      "epoch 86 loss = 1.562961\n",
      "epoch 87 loss = 1.816478\n",
      "epoch 88 loss = 1.739908\n",
      "epoch 89 loss = 1.508822\n",
      "epoch 90 loss = 1.586745\n",
      "epoch 91 loss = 1.550867\n",
      "epoch 92 loss = 1.574878\n",
      "epoch 93 loss = 1.575407\n",
      "epoch 94 loss = 1.502037\n",
      "epoch 95 loss = 1.687111\n",
      "epoch 96 loss = 1.501941\n",
      "epoch 97 loss = 1.601694\n",
      "epoch 98 loss = 1.714561\n",
      "epoch 99 loss = 1.594992\n",
      "final loss = 1.594992\n",
      "accuracy_mc = tensor(0.3089, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3236, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.7781, device='cuda:0')\n",
      "training time = 35.059603691101074 seconds\n",
      "testing time = 1.9497363567352295 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.257075\n",
      "epoch 1 loss = 2.232252\n",
      "epoch 2 loss = 2.208550\n",
      "epoch 3 loss = 2.176589\n",
      "epoch 4 loss = 2.132108\n",
      "epoch 5 loss = 2.314094\n",
      "epoch 6 loss = 2.139832\n",
      "epoch 7 loss = 2.413534\n",
      "epoch 8 loss = 2.244241\n",
      "epoch 9 loss = 2.308976\n",
      "epoch 10 loss = 2.097609\n",
      "epoch 11 loss = 2.040581\n",
      "epoch 12 loss = 2.309516\n",
      "epoch 13 loss = 2.167855\n",
      "epoch 14 loss = 2.237883\n",
      "epoch 15 loss = 2.263326\n",
      "epoch 16 loss = 2.237904\n",
      "epoch 17 loss = 2.170818\n",
      "epoch 18 loss = 2.114189\n",
      "epoch 19 loss = 2.170949\n",
      "epoch 20 loss = 2.031849\n",
      "epoch 21 loss = 2.074343\n",
      "epoch 22 loss = 2.026274\n",
      "epoch 23 loss = 2.063639\n",
      "epoch 24 loss = 1.976510\n",
      "epoch 25 loss = 2.117547\n",
      "epoch 26 loss = 2.007969\n",
      "epoch 27 loss = 1.816486\n",
      "epoch 28 loss = 1.945317\n",
      "epoch 29 loss = 2.006497\n",
      "epoch 30 loss = 1.979781\n",
      "epoch 31 loss = 1.897784\n",
      "epoch 32 loss = 2.142114\n",
      "epoch 33 loss = 1.869826\n",
      "epoch 34 loss = 2.089410\n",
      "epoch 35 loss = 1.914804\n",
      "epoch 36 loss = 2.046168\n",
      "epoch 37 loss = 1.797209\n",
      "epoch 38 loss = 1.697095\n",
      "epoch 39 loss = 1.906443\n",
      "epoch 40 loss = 1.972067\n",
      "epoch 41 loss = 1.862291\n",
      "epoch 42 loss = 1.894392\n",
      "epoch 43 loss = 1.843064\n",
      "epoch 44 loss = 1.916831\n",
      "epoch 45 loss = 1.862810\n",
      "epoch 46 loss = 1.938462\n",
      "epoch 47 loss = 1.888489\n",
      "epoch 48 loss = 1.892130\n",
      "epoch 49 loss = 1.950160\n",
      "epoch 50 loss = 1.847618\n",
      "epoch 51 loss = 1.843543\n",
      "epoch 52 loss = 1.887357\n",
      "epoch 53 loss = 1.662982\n",
      "epoch 54 loss = 1.985015\n",
      "epoch 55 loss = 1.872503\n",
      "epoch 56 loss = 1.795058\n",
      "epoch 57 loss = 1.664779\n",
      "epoch 58 loss = 1.920852\n",
      "epoch 59 loss = 1.781451\n",
      "epoch 60 loss = 2.149243\n",
      "epoch 61 loss = 1.785913\n",
      "epoch 62 loss = 1.835858\n",
      "epoch 63 loss = 1.675116\n",
      "epoch 64 loss = 1.931262\n",
      "epoch 65 loss = 1.800753\n",
      "epoch 66 loss = 1.839339\n",
      "epoch 67 loss = 1.685265\n",
      "epoch 68 loss = 1.798655\n",
      "epoch 69 loss = 1.846232\n",
      "epoch 70 loss = 1.912453\n",
      "epoch 71 loss = 1.772447\n",
      "epoch 72 loss = 1.693881\n",
      "epoch 73 loss = 1.840914\n",
      "epoch 74 loss = 1.795586\n",
      "epoch 75 loss = 1.856943\n",
      "epoch 76 loss = 1.732183\n",
      "epoch 77 loss = 1.736000\n",
      "epoch 78 loss = 1.717012\n",
      "epoch 79 loss = 1.735451\n",
      "epoch 80 loss = 1.725743\n",
      "epoch 81 loss = 1.752761\n",
      "epoch 82 loss = 1.693109\n",
      "epoch 83 loss = 1.745231\n",
      "epoch 84 loss = 1.721282\n",
      "epoch 85 loss = 1.780678\n",
      "epoch 86 loss = 1.927003\n",
      "epoch 87 loss = 1.636986\n",
      "epoch 88 loss = 1.536352\n",
      "epoch 89 loss = 1.754361\n",
      "epoch 90 loss = 1.743706\n",
      "epoch 91 loss = 1.717851\n",
      "epoch 92 loss = 1.659739\n",
      "epoch 93 loss = 1.634530\n",
      "epoch 94 loss = 1.907122\n",
      "epoch 95 loss = 1.891902\n",
      "epoch 96 loss = 1.628952\n",
      "epoch 97 loss = 1.671631\n",
      "epoch 98 loss = 1.743947\n",
      "epoch 99 loss = 1.566342\n",
      "final loss = 1.566342\n",
      "accuracy_mc = tensor(0.4046, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4091, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6587, device='cuda:0')\n",
      "training time = 35.165260553359985 seconds\n",
      "testing time = 1.910200834274292 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.325619\n",
      "epoch 1 loss = 2.243941\n",
      "epoch 2 loss = 2.160032\n",
      "epoch 3 loss = 1.982113\n",
      "epoch 4 loss = 1.969526\n",
      "epoch 5 loss = 1.889444\n",
      "epoch 6 loss = 1.738300\n",
      "epoch 7 loss = 1.810683\n",
      "epoch 8 loss = 1.726493\n",
      "epoch 9 loss = 1.818353\n",
      "epoch 10 loss = 1.842393\n",
      "epoch 11 loss = 1.791126\n",
      "epoch 12 loss = 1.742667\n",
      "epoch 13 loss = 1.729625\n",
      "epoch 14 loss = 1.773612\n",
      "epoch 15 loss = 1.746037\n",
      "epoch 16 loss = 1.565180\n",
      "epoch 17 loss = 1.681691\n",
      "epoch 18 loss = 1.722359\n",
      "epoch 19 loss = 1.711974\n",
      "epoch 20 loss = 1.757847\n",
      "epoch 21 loss = 1.794517\n",
      "epoch 22 loss = 1.728431\n",
      "epoch 23 loss = 1.825690\n",
      "epoch 24 loss = 1.744379\n",
      "epoch 25 loss = 1.614665\n",
      "epoch 26 loss = 1.549838\n",
      "epoch 27 loss = 1.750669\n",
      "epoch 28 loss = 1.669140\n",
      "epoch 29 loss = 1.602582\n",
      "epoch 30 loss = 1.637931\n",
      "epoch 31 loss = 1.642970\n",
      "epoch 32 loss = 1.589158\n",
      "epoch 33 loss = 1.584504\n",
      "epoch 34 loss = 1.642351\n",
      "epoch 35 loss = 1.658850\n",
      "epoch 36 loss = 1.612727\n",
      "epoch 37 loss = 1.616240\n",
      "epoch 38 loss = 1.701444\n",
      "epoch 39 loss = 1.647820\n",
      "epoch 40 loss = 1.780788\n",
      "epoch 41 loss = 1.733680\n",
      "epoch 42 loss = 1.753695\n",
      "epoch 43 loss = 1.694123\n",
      "epoch 44 loss = 1.636070\n",
      "epoch 45 loss = 1.644505\n",
      "epoch 46 loss = 1.651336\n",
      "epoch 47 loss = 1.610815\n",
      "epoch 48 loss = 1.553099\n",
      "epoch 49 loss = 1.666690\n",
      "epoch 50 loss = 1.565567\n",
      "epoch 51 loss = 1.615332\n",
      "epoch 52 loss = 1.672725\n",
      "epoch 53 loss = 1.666958\n",
      "epoch 54 loss = 1.580210\n",
      "epoch 55 loss = 1.698080\n",
      "epoch 56 loss = 1.525129\n",
      "epoch 57 loss = 1.580424\n",
      "epoch 58 loss = 1.564570\n",
      "epoch 59 loss = 1.489980\n",
      "epoch 60 loss = 1.532984\n",
      "epoch 61 loss = 1.507957\n",
      "epoch 62 loss = 1.650786\n",
      "epoch 63 loss = 1.625132\n",
      "epoch 64 loss = 1.595443\n",
      "epoch 65 loss = 1.572373\n",
      "epoch 66 loss = 1.686929\n",
      "epoch 67 loss = 1.437406\n",
      "epoch 68 loss = 1.535780\n",
      "epoch 69 loss = 1.507475\n",
      "epoch 70 loss = 1.395516\n",
      "epoch 71 loss = 1.459286\n",
      "epoch 72 loss = 1.406775\n",
      "epoch 73 loss = 1.442695\n",
      "epoch 74 loss = 1.475893\n",
      "epoch 75 loss = 1.450416\n",
      "epoch 76 loss = 1.331487\n",
      "epoch 77 loss = 1.432674\n",
      "epoch 78 loss = 1.359852\n",
      "epoch 79 loss = 1.523811\n",
      "epoch 80 loss = 1.406114\n",
      "epoch 81 loss = 1.393785\n",
      "epoch 82 loss = 1.546784\n",
      "epoch 83 loss = 1.322801\n",
      "epoch 84 loss = 1.423070\n",
      "epoch 85 loss = 1.422027\n",
      "epoch 86 loss = 1.646901\n",
      "epoch 87 loss = 1.394087\n",
      "epoch 88 loss = 1.457111\n",
      "epoch 89 loss = 1.510064\n",
      "epoch 90 loss = 1.479947\n",
      "epoch 91 loss = 1.471111\n",
      "epoch 92 loss = 1.340557\n",
      "epoch 93 loss = 1.609636\n",
      "epoch 94 loss = 1.581003\n",
      "epoch 95 loss = 1.456980\n",
      "epoch 96 loss = 1.470733\n",
      "epoch 97 loss = 1.425768\n",
      "epoch 98 loss = 1.374141\n",
      "epoch 99 loss = 1.448973\n",
      "final loss = 1.448973\n",
      "accuracy_mc = tensor(0.4030, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4126, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.7649, device='cuda:0')\n",
      "training time = 34.82981634140015 seconds\n",
      "testing time = 1.8490092754364014 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.280089\n",
      "epoch 1 loss = 2.270557\n",
      "epoch 2 loss = 2.278561\n",
      "epoch 3 loss = 2.253259\n",
      "epoch 4 loss = 2.208019\n",
      "epoch 5 loss = 2.221080\n",
      "epoch 6 loss = 2.129729\n",
      "epoch 7 loss = 2.090274\n",
      "epoch 8 loss = 1.991523\n",
      "epoch 9 loss = 2.088072\n",
      "epoch 10 loss = 2.032627\n",
      "epoch 11 loss = 2.029454\n",
      "epoch 12 loss = 2.041162\n",
      "epoch 13 loss = 1.988904\n",
      "epoch 14 loss = 1.973953\n",
      "epoch 15 loss = 2.024205\n",
      "epoch 16 loss = 1.976337\n",
      "epoch 17 loss = 1.932971\n",
      "epoch 18 loss = 2.002361\n",
      "epoch 19 loss = 1.906929\n",
      "epoch 20 loss = 1.809574\n",
      "epoch 21 loss = 1.814751\n",
      "epoch 22 loss = 1.815848\n",
      "epoch 23 loss = 1.854868\n",
      "epoch 24 loss = 1.794341\n",
      "epoch 25 loss = 1.864034\n",
      "epoch 26 loss = 2.023085\n",
      "epoch 27 loss = 1.855641\n",
      "epoch 28 loss = 1.856140\n",
      "epoch 29 loss = 1.920215\n",
      "epoch 30 loss = 1.844160\n",
      "epoch 31 loss = 1.712697\n",
      "epoch 32 loss = 1.887652\n",
      "epoch 33 loss = 1.860672\n",
      "epoch 34 loss = 1.834593\n",
      "epoch 35 loss = 1.750596\n",
      "epoch 36 loss = 1.872021\n",
      "epoch 37 loss = 1.682892\n",
      "epoch 38 loss = 1.828077\n",
      "epoch 39 loss = 1.669720\n",
      "epoch 40 loss = 1.923423\n",
      "epoch 41 loss = 1.702578\n",
      "epoch 42 loss = 1.646409\n",
      "epoch 43 loss = 1.755422\n",
      "epoch 44 loss = 1.795114\n",
      "epoch 45 loss = 1.860497\n",
      "epoch 46 loss = 1.651325\n",
      "epoch 47 loss = 1.755443\n",
      "epoch 48 loss = 1.650547\n",
      "epoch 49 loss = 1.816294\n",
      "epoch 50 loss = 1.703728\n",
      "epoch 51 loss = 1.769131\n",
      "epoch 52 loss = 1.723244\n",
      "epoch 53 loss = 1.672185\n",
      "epoch 54 loss = 1.685327\n",
      "epoch 55 loss = 1.679063\n",
      "epoch 56 loss = 1.623854\n",
      "epoch 57 loss = 1.756338\n",
      "epoch 58 loss = 1.793727\n",
      "epoch 59 loss = 1.783995\n",
      "epoch 60 loss = 1.644451\n",
      "epoch 61 loss = 1.676357\n",
      "epoch 62 loss = 1.714113\n",
      "epoch 63 loss = 1.725336\n",
      "epoch 64 loss = 1.752077\n",
      "epoch 65 loss = 1.653510\n",
      "epoch 66 loss = 1.574375\n",
      "epoch 67 loss = 1.627796\n",
      "epoch 68 loss = 1.570023\n",
      "epoch 69 loss = 1.666061\n",
      "epoch 70 loss = 1.810632\n",
      "epoch 71 loss = 1.826287\n",
      "epoch 72 loss = 1.628133\n",
      "epoch 73 loss = 1.594207\n",
      "epoch 74 loss = 1.587664\n",
      "epoch 75 loss = 1.643438\n",
      "epoch 76 loss = 1.625413\n",
      "epoch 77 loss = 1.600900\n",
      "epoch 78 loss = 1.571061\n",
      "epoch 79 loss = 1.568832\n",
      "epoch 80 loss = 1.709695\n",
      "epoch 81 loss = 1.824542\n",
      "epoch 82 loss = 1.782605\n",
      "epoch 83 loss = 1.704749\n",
      "epoch 84 loss = 1.644233\n",
      "epoch 85 loss = 1.786663\n",
      "epoch 86 loss = 1.578372\n",
      "epoch 87 loss = 1.607120\n",
      "epoch 88 loss = 1.540928\n",
      "epoch 89 loss = 1.718181\n",
      "epoch 90 loss = 1.605965\n",
      "epoch 91 loss = 1.549129\n",
      "epoch 92 loss = 1.599207\n",
      "epoch 93 loss = 1.801892\n",
      "epoch 94 loss = 1.462765\n",
      "epoch 95 loss = 1.648999\n",
      "epoch 96 loss = 1.631399\n",
      "epoch 97 loss = 1.484130\n",
      "epoch 98 loss = 1.590804\n",
      "epoch 99 loss = 1.545384\n",
      "final loss = 1.545384\n",
      "accuracy_mc = tensor(0.3578, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3647, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.8150, device='cuda:0')\n",
      "training time = 34.162272930145264 seconds\n",
      "testing time = 1.9170010089874268 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 2.295499\n",
      "epoch 1 loss = 2.273804\n",
      "epoch 2 loss = 2.287026\n",
      "epoch 3 loss = 2.261544\n",
      "epoch 4 loss = 2.233755\n",
      "epoch 5 loss = 2.169441\n",
      "epoch 6 loss = 2.039544\n",
      "epoch 7 loss = 2.044425\n",
      "epoch 8 loss = 2.111461\n",
      "epoch 9 loss = 2.045525\n",
      "epoch 10 loss = 2.126680\n",
      "epoch 11 loss = 1.875538\n",
      "epoch 12 loss = 1.959572\n",
      "epoch 13 loss = 1.872620\n",
      "epoch 14 loss = 1.831853\n",
      "epoch 15 loss = 1.902425\n",
      "epoch 16 loss = 1.950754\n",
      "epoch 17 loss = 1.888258\n",
      "epoch 18 loss = 1.918355\n",
      "epoch 19 loss = 1.886407\n",
      "epoch 20 loss = 2.056365\n",
      "epoch 21 loss = 1.897922\n",
      "epoch 22 loss = 1.876505\n",
      "epoch 23 loss = 1.836987\n",
      "epoch 24 loss = 1.779443\n",
      "epoch 25 loss = 1.825091\n",
      "epoch 26 loss = 1.803945\n",
      "epoch 27 loss = 1.884909\n",
      "epoch 28 loss = 1.785884\n",
      "epoch 29 loss = 1.853692\n",
      "epoch 30 loss = 1.764996\n",
      "epoch 31 loss = 1.726550\n",
      "epoch 32 loss = 1.838750\n",
      "epoch 33 loss = 1.750516\n",
      "epoch 34 loss = 1.795052\n",
      "epoch 35 loss = 1.880443\n",
      "epoch 36 loss = 1.711830\n",
      "epoch 37 loss = 1.789917\n",
      "epoch 38 loss = 1.819702\n",
      "epoch 39 loss = 1.754773\n",
      "epoch 40 loss = 1.684174\n",
      "epoch 41 loss = 1.860209\n",
      "epoch 42 loss = 1.713818\n",
      "epoch 43 loss = 1.641702\n",
      "epoch 44 loss = 1.821853\n",
      "epoch 45 loss = 1.695084\n",
      "epoch 46 loss = 1.702594\n",
      "epoch 47 loss = 1.647634\n",
      "epoch 48 loss = 1.629449\n",
      "epoch 49 loss = 1.624744\n",
      "epoch 50 loss = 1.634843\n",
      "epoch 51 loss = 1.639918\n",
      "epoch 52 loss = 1.675934\n",
      "epoch 53 loss = 1.666560\n",
      "epoch 54 loss = 1.618471\n",
      "epoch 55 loss = 1.714457\n",
      "epoch 56 loss = 1.651800\n",
      "epoch 57 loss = 1.577974\n",
      "epoch 58 loss = 1.635144\n",
      "epoch 59 loss = 1.663257\n",
      "epoch 60 loss = 1.676697\n",
      "epoch 61 loss = 1.571222\n",
      "epoch 62 loss = 1.568223\n",
      "epoch 63 loss = 1.681353\n",
      "epoch 64 loss = 1.766656\n",
      "epoch 65 loss = 1.574380\n",
      "epoch 66 loss = 1.624778\n",
      "epoch 67 loss = 1.453256\n",
      "epoch 68 loss = 1.446357\n",
      "epoch 69 loss = 1.398126\n",
      "epoch 70 loss = 1.551243\n",
      "epoch 71 loss = 1.564485\n",
      "epoch 72 loss = 1.448486\n",
      "epoch 73 loss = 1.682779\n",
      "epoch 74 loss = 1.530678\n",
      "epoch 75 loss = 1.586959\n",
      "epoch 76 loss = 1.698190\n",
      "epoch 77 loss = 1.578634\n",
      "epoch 78 loss = 1.459827\n",
      "epoch 79 loss = 1.313929\n",
      "epoch 80 loss = 1.469165\n",
      "epoch 81 loss = 1.519379\n",
      "epoch 82 loss = 1.498840\n",
      "epoch 83 loss = 1.402969\n",
      "epoch 84 loss = 1.502016\n",
      "epoch 85 loss = 1.438759\n",
      "epoch 86 loss = 1.555708\n",
      "epoch 87 loss = 1.379271\n",
      "epoch 88 loss = 1.561381\n",
      "epoch 89 loss = 1.622009\n",
      "epoch 90 loss = 1.398077\n",
      "epoch 91 loss = 1.383496\n",
      "epoch 92 loss = 1.594996\n",
      "epoch 93 loss = 1.502969\n",
      "epoch 94 loss = 1.491069\n",
      "epoch 95 loss = 1.487463\n",
      "epoch 96 loss = 1.442070\n",
      "epoch 97 loss = 1.448132\n",
      "epoch 98 loss = 1.447669\n",
      "epoch 99 loss = 1.384888\n",
      "final loss = 1.384888\n",
      "accuracy_mc = tensor(0.4003, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4011, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6882, device='cuda:0')\n",
      "training time = 34.42453479766846 seconds\n",
      "testing time = 1.9020442962646484 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.100000, reg_strength 0.050000\n",
      "n_epoch 500\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.317856\n",
      "epoch 1 loss = 2.252765\n",
      "epoch 2 loss = 2.174119\n",
      "epoch 3 loss = 2.100922\n",
      "epoch 4 loss = 2.185141\n",
      "epoch 5 loss = 2.091575\n",
      "epoch 6 loss = 2.150214\n",
      "epoch 7 loss = 1.900217\n",
      "epoch 8 loss = 1.905949\n",
      "epoch 9 loss = 1.917629\n",
      "epoch 10 loss = 1.886251\n",
      "epoch 11 loss = 1.892022\n",
      "epoch 12 loss = 1.898929\n",
      "epoch 13 loss = 1.917226\n",
      "epoch 14 loss = 1.753546\n",
      "epoch 15 loss = 2.001490\n",
      "epoch 16 loss = 1.839867\n",
      "epoch 17 loss = 1.753230\n",
      "epoch 18 loss = 1.955188\n",
      "epoch 19 loss = 1.768133\n",
      "epoch 20 loss = 1.800068\n",
      "epoch 21 loss = 1.802749\n",
      "epoch 22 loss = 1.847447\n",
      "epoch 23 loss = 1.738507\n",
      "epoch 24 loss = 1.718504\n",
      "epoch 25 loss = 1.698337\n",
      "epoch 26 loss = 1.642361\n",
      "epoch 27 loss = 1.637040\n",
      "epoch 28 loss = 1.542509\n",
      "epoch 29 loss = 1.514600\n",
      "epoch 30 loss = 1.770178\n",
      "epoch 31 loss = 1.575148\n",
      "epoch 32 loss = 1.649191\n",
      "epoch 33 loss = 1.552578\n",
      "epoch 34 loss = 1.341750\n",
      "epoch 35 loss = 1.540254\n",
      "epoch 36 loss = 1.474427\n",
      "epoch 37 loss = 1.709211\n",
      "epoch 38 loss = 1.652411\n",
      "epoch 39 loss = 1.414580\n",
      "epoch 40 loss = 1.486777\n",
      "epoch 41 loss = 1.440903\n",
      "epoch 42 loss = 1.510480\n",
      "epoch 43 loss = 1.753461\n",
      "epoch 44 loss = 1.365193\n",
      "epoch 45 loss = 1.516225\n",
      "epoch 46 loss = 1.565745\n",
      "epoch 47 loss = 1.640988\n",
      "epoch 48 loss = 1.382540\n",
      "epoch 49 loss = 1.251514\n",
      "epoch 50 loss = 1.454218\n",
      "epoch 51 loss = 1.304464\n",
      "epoch 52 loss = 1.406613\n",
      "epoch 53 loss = 1.219122\n",
      "epoch 54 loss = 1.332033\n",
      "epoch 55 loss = 1.265636\n",
      "epoch 56 loss = 1.259180\n",
      "epoch 57 loss = 1.292711\n",
      "epoch 58 loss = 1.134335\n",
      "epoch 59 loss = 1.404328\n",
      "epoch 60 loss = 1.289568\n",
      "epoch 61 loss = 1.607781\n",
      "epoch 62 loss = 1.300573\n",
      "epoch 63 loss = 1.443587\n",
      "epoch 64 loss = 1.220234\n",
      "epoch 65 loss = 1.382396\n",
      "epoch 66 loss = 1.221771\n",
      "epoch 67 loss = 1.100482\n",
      "epoch 68 loss = 1.491554\n",
      "epoch 69 loss = 1.204298\n",
      "epoch 70 loss = 1.204234\n",
      "epoch 71 loss = 1.138319\n",
      "epoch 72 loss = 1.128270\n",
      "epoch 73 loss = 1.094257\n",
      "epoch 74 loss = 1.123292\n",
      "epoch 75 loss = 1.212409\n",
      "epoch 76 loss = 1.190230\n",
      "epoch 77 loss = 1.014180\n",
      "epoch 78 loss = 1.676319\n",
      "epoch 79 loss = 1.143987\n",
      "epoch 80 loss = 1.399117\n",
      "epoch 81 loss = 1.347387\n",
      "epoch 82 loss = 1.166762\n",
      "epoch 83 loss = 1.307499\n",
      "epoch 84 loss = 1.239277\n",
      "epoch 85 loss = 1.164706\n",
      "epoch 86 loss = 1.026600\n",
      "epoch 87 loss = 1.007407\n",
      "epoch 88 loss = 1.041210\n",
      "epoch 89 loss = 1.321449\n",
      "epoch 90 loss = 1.271776\n",
      "epoch 91 loss = 1.116017\n",
      "epoch 92 loss = 1.098562\n",
      "epoch 93 loss = 0.950442\n",
      "epoch 94 loss = 1.012095\n",
      "epoch 95 loss = 0.967540\n",
      "epoch 96 loss = 1.184044\n",
      "epoch 97 loss = 1.095813\n",
      "epoch 98 loss = 1.084234\n",
      "epoch 99 loss = 1.004203\n",
      "epoch 100 loss = 1.227052\n",
      "epoch 101 loss = 1.321285\n",
      "epoch 102 loss = 1.195843\n",
      "epoch 103 loss = 0.928509\n",
      "epoch 104 loss = 1.221444\n",
      "epoch 105 loss = 1.106448\n",
      "epoch 106 loss = 1.229889\n",
      "epoch 107 loss = 1.231522\n",
      "epoch 108 loss = 1.094939\n",
      "epoch 109 loss = 1.091403\n",
      "epoch 110 loss = 1.070105\n",
      "epoch 111 loss = 0.931940\n",
      "epoch 112 loss = 0.868468\n",
      "epoch 113 loss = 1.082727\n",
      "epoch 114 loss = 1.100901\n",
      "epoch 115 loss = 0.959933\n",
      "epoch 116 loss = 0.919845\n",
      "epoch 117 loss = 0.941512\n",
      "epoch 118 loss = 1.099844\n",
      "epoch 119 loss = 0.947937\n",
      "epoch 120 loss = 0.871951\n",
      "epoch 121 loss = 1.133031\n",
      "epoch 122 loss = 1.025883\n",
      "epoch 123 loss = 1.271379\n",
      "epoch 124 loss = 1.254732\n",
      "epoch 125 loss = 1.141241\n",
      "epoch 126 loss = 1.017897\n",
      "epoch 127 loss = 0.977531\n",
      "epoch 128 loss = 0.922414\n",
      "epoch 129 loss = 0.938448\n",
      "epoch 130 loss = 0.859865\n",
      "epoch 131 loss = 0.946891\n",
      "epoch 132 loss = 1.040391\n",
      "epoch 133 loss = 0.976069\n",
      "epoch 134 loss = 1.012762\n",
      "epoch 135 loss = 1.066591\n",
      "epoch 136 loss = 0.947501\n",
      "epoch 137 loss = 1.078664\n",
      "epoch 138 loss = 1.059663\n",
      "epoch 139 loss = 1.053042\n",
      "epoch 140 loss = 1.457749\n",
      "epoch 141 loss = 0.841664\n",
      "epoch 142 loss = 0.956957\n",
      "epoch 143 loss = 1.122732\n",
      "epoch 144 loss = 1.006216\n",
      "epoch 145 loss = 0.923616\n",
      "epoch 146 loss = 1.164868\n",
      "epoch 147 loss = 0.900602\n",
      "epoch 148 loss = 1.002343\n",
      "epoch 149 loss = 0.794520\n",
      "epoch 150 loss = 1.003336\n",
      "epoch 151 loss = 0.946732\n",
      "epoch 152 loss = 1.140490\n",
      "epoch 153 loss = 0.968069\n",
      "epoch 154 loss = 1.487630\n",
      "epoch 155 loss = 1.055107\n",
      "epoch 156 loss = 1.292014\n",
      "epoch 157 loss = 0.834807\n",
      "epoch 158 loss = 1.053914\n",
      "epoch 159 loss = 1.016205\n",
      "epoch 160 loss = 0.983792\n",
      "epoch 161 loss = 1.156201\n",
      "epoch 162 loss = 0.916240\n",
      "epoch 163 loss = 0.872191\n",
      "epoch 164 loss = 0.742036\n",
      "epoch 165 loss = 0.951088\n",
      "epoch 166 loss = 0.798295\n",
      "epoch 167 loss = 0.751358\n",
      "epoch 168 loss = 1.121632\n",
      "epoch 169 loss = 0.933949\n",
      "epoch 170 loss = 0.903002\n",
      "epoch 171 loss = 0.705922\n",
      "epoch 172 loss = 1.062397\n",
      "epoch 173 loss = 0.897890\n",
      "epoch 174 loss = 1.077295\n",
      "epoch 175 loss = 0.967148\n",
      "epoch 176 loss = 0.654953\n",
      "epoch 177 loss = 0.779790\n",
      "epoch 178 loss = 0.922913\n",
      "epoch 179 loss = 0.873886\n",
      "epoch 180 loss = 0.861075\n",
      "epoch 181 loss = 0.904599\n",
      "epoch 182 loss = 0.815006\n",
      "epoch 183 loss = 0.731555\n",
      "epoch 184 loss = 0.734454\n",
      "epoch 185 loss = 0.669330\n",
      "epoch 186 loss = 0.590849\n",
      "epoch 187 loss = 1.012092\n",
      "epoch 188 loss = 0.815169\n",
      "epoch 189 loss = 0.826108\n",
      "epoch 190 loss = 0.897363\n",
      "epoch 191 loss = 0.744078\n",
      "epoch 192 loss = 1.128354\n",
      "epoch 193 loss = 0.842333\n",
      "epoch 194 loss = 0.804821\n",
      "epoch 195 loss = 0.856514\n",
      "epoch 196 loss = 1.146520\n",
      "epoch 197 loss = 0.674586\n",
      "epoch 198 loss = 0.932244\n",
      "epoch 199 loss = 0.827365\n",
      "epoch 200 loss = 0.884217\n",
      "epoch 201 loss = 0.806146\n",
      "epoch 202 loss = 0.793210\n",
      "epoch 203 loss = 1.005782\n",
      "epoch 204 loss = 0.760556\n",
      "epoch 205 loss = 0.782662\n",
      "epoch 206 loss = 0.779107\n",
      "epoch 207 loss = 0.731581\n",
      "epoch 208 loss = 0.906405\n",
      "epoch 209 loss = 0.719750\n",
      "epoch 210 loss = 0.955671\n",
      "epoch 211 loss = 0.600153\n",
      "epoch 212 loss = 0.859786\n",
      "epoch 213 loss = 0.816334\n",
      "epoch 214 loss = 0.688917\n",
      "epoch 215 loss = 0.765050\n",
      "epoch 216 loss = 0.871824\n",
      "epoch 217 loss = 0.715369\n",
      "epoch 218 loss = 0.813480\n",
      "epoch 219 loss = 1.053215\n",
      "epoch 220 loss = 0.743254\n",
      "epoch 221 loss = 0.892119\n",
      "epoch 222 loss = 0.763108\n",
      "epoch 223 loss = 0.656736\n",
      "epoch 224 loss = 0.758511\n",
      "epoch 225 loss = 0.767171\n",
      "epoch 226 loss = 0.766993\n",
      "epoch 227 loss = 0.800466\n",
      "epoch 228 loss = 0.554642\n",
      "epoch 229 loss = 0.650820\n",
      "epoch 230 loss = 0.599446\n",
      "epoch 231 loss = 0.720704\n",
      "epoch 232 loss = 0.591768\n",
      "epoch 233 loss = 0.698752\n",
      "epoch 234 loss = 0.742549\n",
      "epoch 235 loss = 0.683450\n",
      "epoch 236 loss = 0.579666\n",
      "epoch 237 loss = 0.587957\n",
      "epoch 238 loss = 0.647060\n",
      "epoch 239 loss = 0.625621\n",
      "epoch 240 loss = 0.807564\n",
      "epoch 241 loss = 0.643942\n",
      "epoch 242 loss = 0.750981\n",
      "epoch 243 loss = 0.698308\n",
      "epoch 244 loss = 0.808119\n",
      "epoch 245 loss = 0.762414\n",
      "epoch 246 loss = 0.836788\n",
      "epoch 247 loss = 0.830340\n",
      "epoch 248 loss = 0.719515\n",
      "epoch 249 loss = 0.787668\n",
      "epoch 250 loss = 0.708346\n",
      "epoch 251 loss = 0.810519\n",
      "epoch 252 loss = 0.668735\n",
      "epoch 253 loss = 0.886291\n",
      "epoch 254 loss = 0.707491\n",
      "epoch 255 loss = 0.683324\n",
      "epoch 256 loss = 0.810963\n",
      "epoch 257 loss = 0.581272\n",
      "epoch 258 loss = 1.064943\n",
      "epoch 259 loss = 0.874495\n",
      "epoch 260 loss = 0.715147\n",
      "epoch 261 loss = 0.665597\n",
      "epoch 262 loss = 0.598059\n",
      "epoch 263 loss = 0.833003\n",
      "epoch 264 loss = 0.653947\n",
      "epoch 265 loss = 0.645662\n",
      "epoch 266 loss = 0.687097\n",
      "epoch 267 loss = 0.829513\n",
      "epoch 268 loss = 0.701073\n",
      "epoch 269 loss = 0.877300\n",
      "epoch 270 loss = 0.813283\n",
      "epoch 271 loss = 0.730015\n",
      "epoch 272 loss = 0.598573\n",
      "epoch 273 loss = 0.851245\n",
      "epoch 274 loss = 0.599123\n",
      "epoch 275 loss = 0.677302\n",
      "epoch 276 loss = 0.904958\n",
      "epoch 277 loss = 0.785452\n",
      "epoch 278 loss = 0.626325\n",
      "epoch 279 loss = 0.703169\n",
      "epoch 280 loss = 0.637672\n",
      "epoch 281 loss = 0.581638\n",
      "epoch 282 loss = 0.639203\n",
      "epoch 283 loss = 0.778740\n",
      "epoch 284 loss = 0.851337\n",
      "epoch 285 loss = 0.714842\n",
      "epoch 286 loss = 0.762242\n",
      "epoch 287 loss = 0.622069\n",
      "epoch 288 loss = 0.694384\n",
      "epoch 289 loss = 0.828112\n",
      "epoch 290 loss = 0.637491\n",
      "epoch 291 loss = 0.681815\n",
      "epoch 292 loss = 0.727724\n",
      "epoch 293 loss = 0.681647\n",
      "epoch 294 loss = 0.779320\n",
      "epoch 295 loss = 0.841222\n",
      "epoch 296 loss = 0.644397\n",
      "epoch 297 loss = 0.896237\n",
      "epoch 298 loss = 0.797636\n",
      "epoch 299 loss = 0.555114\n",
      "epoch 300 loss = 0.756731\n",
      "epoch 301 loss = 0.615540\n",
      "epoch 302 loss = 1.070609\n",
      "epoch 303 loss = 0.807380\n",
      "epoch 304 loss = 0.593901\n",
      "epoch 305 loss = 0.595769\n",
      "epoch 306 loss = 0.650373\n",
      "epoch 307 loss = 0.699938\n",
      "epoch 308 loss = 0.832556\n",
      "epoch 309 loss = 0.621085\n",
      "epoch 310 loss = 0.677933\n",
      "epoch 311 loss = 0.793759\n",
      "epoch 312 loss = 0.718315\n",
      "epoch 313 loss = 0.864692\n",
      "epoch 314 loss = 0.839355\n",
      "epoch 315 loss = 0.724712\n",
      "epoch 316 loss = 0.684815\n",
      "epoch 317 loss = 0.873025\n",
      "epoch 318 loss = 0.711622\n",
      "epoch 319 loss = 0.588322\n",
      "epoch 320 loss = 0.607008\n",
      "epoch 321 loss = 0.681467\n",
      "epoch 322 loss = 0.557229\n",
      "epoch 323 loss = 0.583382\n",
      "epoch 324 loss = 0.556374\n",
      "epoch 325 loss = 0.537060\n",
      "epoch 326 loss = 0.565201\n",
      "epoch 327 loss = 0.633698\n",
      "epoch 328 loss = 0.506520\n",
      "epoch 329 loss = 0.693677\n",
      "epoch 330 loss = 0.615310\n",
      "epoch 331 loss = 0.504879\n",
      "epoch 332 loss = 0.758349\n",
      "epoch 333 loss = 0.595459\n",
      "epoch 334 loss = 0.559131\n",
      "epoch 335 loss = 0.643481\n",
      "epoch 336 loss = 0.607529\n",
      "epoch 337 loss = 0.637103\n",
      "epoch 338 loss = 0.749375\n",
      "epoch 339 loss = 0.471783\n",
      "epoch 340 loss = 0.636327\n",
      "epoch 341 loss = 0.692187\n",
      "epoch 342 loss = 0.614056\n",
      "epoch 343 loss = 0.834105\n",
      "epoch 344 loss = 0.694266\n",
      "epoch 345 loss = 0.746156\n",
      "epoch 346 loss = 0.874036\n",
      "epoch 347 loss = 0.638172\n",
      "epoch 348 loss = 0.582670\n",
      "epoch 349 loss = 0.739228\n",
      "epoch 350 loss = 0.618737\n",
      "epoch 351 loss = 0.776951\n",
      "epoch 352 loss = 0.574921\n",
      "epoch 353 loss = 0.895199\n",
      "epoch 354 loss = 0.533179\n",
      "epoch 355 loss = 0.574911\n",
      "epoch 356 loss = 0.536040\n",
      "epoch 357 loss = 0.655839\n",
      "epoch 358 loss = 0.784045\n",
      "epoch 359 loss = 0.792467\n",
      "epoch 360 loss = 0.978900\n",
      "epoch 361 loss = 0.682470\n",
      "epoch 362 loss = 0.424417\n",
      "epoch 363 loss = 0.814600\n",
      "epoch 364 loss = 0.490381\n",
      "epoch 365 loss = 0.690195\n",
      "epoch 366 loss = 0.875895\n",
      "epoch 367 loss = 0.752849\n",
      "epoch 368 loss = 0.469370\n",
      "epoch 369 loss = 0.500303\n",
      "epoch 370 loss = 0.677927\n",
      "epoch 371 loss = 0.785193\n",
      "epoch 372 loss = 0.871865\n",
      "epoch 373 loss = 0.575763\n",
      "epoch 374 loss = 0.555499\n",
      "epoch 375 loss = 0.447915\n",
      "epoch 376 loss = 1.066278\n",
      "epoch 377 loss = 0.566354\n",
      "epoch 378 loss = 0.685403\n",
      "epoch 379 loss = 0.645750\n",
      "epoch 380 loss = 0.486422\n",
      "epoch 381 loss = 0.694964\n",
      "epoch 382 loss = 0.643290\n",
      "epoch 383 loss = 0.778341\n",
      "epoch 384 loss = 0.672206\n",
      "epoch 385 loss = 0.668584\n",
      "epoch 386 loss = 0.752937\n",
      "epoch 387 loss = 0.616764\n",
      "epoch 388 loss = 0.541408\n",
      "epoch 389 loss = 0.634245\n",
      "epoch 390 loss = 0.588384\n",
      "epoch 391 loss = 0.635285\n",
      "epoch 392 loss = 0.712133\n",
      "epoch 393 loss = 0.591088\n",
      "epoch 394 loss = 0.671250\n",
      "epoch 395 loss = 0.653390\n",
      "epoch 396 loss = 0.407525\n",
      "epoch 397 loss = 0.622457\n",
      "epoch 398 loss = 0.831601\n",
      "epoch 399 loss = 0.629673\n",
      "epoch 400 loss = 0.423632\n",
      "epoch 401 loss = 0.387859\n",
      "epoch 402 loss = 1.018048\n",
      "epoch 403 loss = 0.602846\n",
      "epoch 404 loss = 0.553734\n",
      "epoch 405 loss = 0.534416\n",
      "epoch 406 loss = 0.461464\n",
      "epoch 407 loss = 0.571558\n",
      "epoch 408 loss = 0.820951\n",
      "epoch 409 loss = 0.763976\n",
      "epoch 410 loss = 0.644218\n",
      "epoch 411 loss = 0.585668\n",
      "epoch 412 loss = 0.507614\n",
      "epoch 413 loss = 0.556264\n",
      "epoch 414 loss = 0.582842\n",
      "epoch 415 loss = 0.637458\n",
      "epoch 416 loss = 0.538565\n",
      "epoch 417 loss = 0.708825\n",
      "epoch 418 loss = 0.463408\n",
      "epoch 419 loss = 0.697068\n",
      "epoch 420 loss = 0.674576\n",
      "epoch 421 loss = 0.677336\n",
      "epoch 422 loss = 0.513703\n",
      "epoch 423 loss = 0.719687\n",
      "epoch 424 loss = 0.566648\n",
      "epoch 425 loss = 0.498332\n",
      "epoch 426 loss = 0.586534\n",
      "epoch 427 loss = 0.581319\n",
      "epoch 428 loss = 0.841592\n",
      "epoch 429 loss = 0.615424\n",
      "epoch 430 loss = 0.652657\n",
      "epoch 431 loss = 0.523397\n",
      "epoch 432 loss = 0.462754\n",
      "epoch 433 loss = 0.872638\n",
      "epoch 434 loss = 0.621537\n",
      "epoch 435 loss = 0.684769\n",
      "epoch 436 loss = 0.842517\n",
      "epoch 437 loss = 0.803565\n",
      "epoch 438 loss = 0.673740\n",
      "epoch 439 loss = 0.657795\n",
      "epoch 440 loss = 0.697458\n",
      "epoch 441 loss = 0.555437\n",
      "epoch 442 loss = 0.576460\n",
      "epoch 443 loss = 0.606467\n",
      "epoch 444 loss = 0.894203\n",
      "epoch 445 loss = 0.578365\n",
      "epoch 446 loss = 0.465499\n",
      "epoch 447 loss = 0.476111\n",
      "epoch 448 loss = 0.830154\n",
      "epoch 449 loss = 0.566332\n",
      "epoch 450 loss = 0.657493\n",
      "epoch 451 loss = 0.593734\n",
      "epoch 452 loss = 0.826699\n",
      "epoch 453 loss = 0.662596\n",
      "epoch 454 loss = 0.558421\n",
      "epoch 455 loss = 0.411991\n",
      "epoch 456 loss = 0.692279\n",
      "epoch 457 loss = 0.528611\n",
      "epoch 458 loss = 0.440627\n",
      "epoch 459 loss = 0.496680\n",
      "epoch 460 loss = 0.681885\n",
      "epoch 461 loss = 0.611830\n",
      "epoch 462 loss = 0.515256\n",
      "epoch 463 loss = 0.535055\n",
      "epoch 464 loss = 0.582381\n",
      "epoch 465 loss = 0.616315\n",
      "epoch 466 loss = 0.550230\n",
      "epoch 467 loss = 0.540387\n",
      "epoch 468 loss = 0.569548\n",
      "epoch 469 loss = 0.524699\n",
      "epoch 470 loss = 0.522817\n",
      "epoch 471 loss = 0.568174\n",
      "epoch 472 loss = 0.422356\n",
      "epoch 473 loss = 0.616745\n",
      "epoch 474 loss = 0.461812\n",
      "epoch 475 loss = 0.424125\n",
      "epoch 476 loss = 0.419434\n",
      "epoch 477 loss = 0.672485\n",
      "epoch 478 loss = 0.428213\n",
      "epoch 479 loss = 0.783819\n",
      "epoch 480 loss = 0.585174\n",
      "epoch 481 loss = 0.398216\n",
      "epoch 482 loss = 0.678169\n",
      "epoch 483 loss = 0.451989\n",
      "epoch 484 loss = 0.435426\n",
      "epoch 485 loss = 0.406711\n",
      "epoch 486 loss = 0.696057\n",
      "epoch 487 loss = 0.642807\n",
      "epoch 488 loss = 0.621290\n",
      "epoch 489 loss = 0.578855\n",
      "epoch 490 loss = 0.554205\n",
      "epoch 491 loss = 0.443661\n",
      "epoch 492 loss = 0.538413\n",
      "epoch 493 loss = 0.434286\n",
      "epoch 494 loss = 0.811517\n",
      "epoch 495 loss = 0.394413\n",
      "epoch 496 loss = 0.415742\n",
      "epoch 497 loss = 0.643353\n",
      "epoch 498 loss = 0.532962\n",
      "epoch 499 loss = 0.806103\n",
      "final loss = 0.806103\n",
      "accuracy_mc = tensor(0.3556, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3567, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9649, device='cuda:0')\n",
      "training time = 171.10792088508606 seconds\n",
      "testing time = 1.8921234607696533 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.251512\n",
      "epoch 1 loss = 2.173354\n",
      "epoch 2 loss = 2.071375\n",
      "epoch 3 loss = 2.034198\n",
      "epoch 4 loss = 1.956213\n",
      "epoch 5 loss = 2.093828\n",
      "epoch 6 loss = 2.098440\n",
      "epoch 7 loss = 2.005486\n",
      "epoch 8 loss = 1.958282\n",
      "epoch 9 loss = 1.954241\n",
      "epoch 10 loss = 2.089650\n",
      "epoch 11 loss = 1.952115\n",
      "epoch 12 loss = 2.094494\n",
      "epoch 13 loss = 1.977876\n",
      "epoch 14 loss = 2.137717\n",
      "epoch 15 loss = 2.131923\n",
      "epoch 16 loss = 1.876710\n",
      "epoch 17 loss = 1.988887\n",
      "epoch 18 loss = 2.019516\n",
      "epoch 19 loss = 2.032748\n",
      "epoch 20 loss = 2.120270\n",
      "epoch 21 loss = 2.126452\n",
      "epoch 22 loss = 1.838623\n",
      "epoch 23 loss = 1.923470\n",
      "epoch 24 loss = 1.979135\n",
      "epoch 25 loss = 1.932572\n",
      "epoch 26 loss = 1.946969\n",
      "epoch 27 loss = 2.190314\n",
      "epoch 28 loss = 1.989404\n",
      "epoch 29 loss = 2.090874\n",
      "epoch 30 loss = 2.008447\n",
      "epoch 31 loss = 1.867383\n",
      "epoch 32 loss = 2.031334\n",
      "epoch 33 loss = 1.993566\n",
      "epoch 34 loss = 1.708463\n",
      "epoch 35 loss = 1.808429\n",
      "epoch 36 loss = 1.822846\n",
      "epoch 37 loss = 1.837925\n",
      "epoch 38 loss = 1.818377\n",
      "epoch 39 loss = 1.806340\n",
      "epoch 40 loss = 1.786800\n",
      "epoch 41 loss = 1.816473\n",
      "epoch 42 loss = 1.676417\n",
      "epoch 43 loss = 1.700849\n",
      "epoch 44 loss = 1.682247\n",
      "epoch 45 loss = 1.791505\n",
      "epoch 46 loss = 1.580201\n",
      "epoch 47 loss = 1.640196\n",
      "epoch 48 loss = 1.683012\n",
      "epoch 49 loss = 1.621357\n",
      "epoch 50 loss = 1.584742\n",
      "epoch 51 loss = 1.527327\n",
      "epoch 52 loss = 1.598156\n",
      "epoch 53 loss = 1.545393\n",
      "epoch 54 loss = 1.558911\n",
      "epoch 55 loss = 1.560320\n",
      "epoch 56 loss = 1.701443\n",
      "epoch 57 loss = 1.667089\n",
      "epoch 58 loss = 1.562198\n",
      "epoch 59 loss = 1.634209\n",
      "epoch 60 loss = 1.555629\n",
      "epoch 61 loss = 1.458470\n",
      "epoch 62 loss = 1.567770\n",
      "epoch 63 loss = 1.768398\n",
      "epoch 64 loss = 1.515469\n",
      "epoch 65 loss = 1.494735\n",
      "epoch 66 loss = 1.665634\n",
      "epoch 67 loss = 1.511060\n",
      "epoch 68 loss = 1.493881\n",
      "epoch 69 loss = 1.515033\n",
      "epoch 70 loss = 1.515172\n",
      "epoch 71 loss = 1.540051\n",
      "epoch 72 loss = 1.445933\n",
      "epoch 73 loss = 1.448972\n",
      "epoch 74 loss = 1.461775\n",
      "epoch 75 loss = 1.503781\n",
      "epoch 76 loss = 1.590062\n",
      "epoch 77 loss = 1.478518\n",
      "epoch 78 loss = 1.617331\n",
      "epoch 79 loss = 1.482044\n",
      "epoch 80 loss = 1.481686\n",
      "epoch 81 loss = 1.537552\n",
      "epoch 82 loss = 1.552459\n",
      "epoch 83 loss = 1.536289\n",
      "epoch 84 loss = 1.527649\n",
      "epoch 85 loss = 1.380556\n",
      "epoch 86 loss = 1.419509\n",
      "epoch 87 loss = 1.576617\n",
      "epoch 88 loss = 1.279461\n",
      "epoch 89 loss = 1.482870\n",
      "epoch 90 loss = 1.388577\n",
      "epoch 91 loss = 1.443535\n",
      "epoch 92 loss = 1.749692\n",
      "epoch 93 loss = 1.471726\n",
      "epoch 94 loss = 1.515641\n",
      "epoch 95 loss = 1.400847\n",
      "epoch 96 loss = 1.272123\n",
      "epoch 97 loss = 1.466068\n",
      "epoch 98 loss = 1.421449\n",
      "epoch 99 loss = 1.418935\n",
      "epoch 100 loss = 1.432924\n",
      "epoch 101 loss = 1.479628\n",
      "epoch 102 loss = 1.349051\n",
      "epoch 103 loss = 1.590628\n",
      "epoch 104 loss = 1.329732\n",
      "epoch 105 loss = 1.593611\n",
      "epoch 106 loss = 1.353809\n",
      "epoch 107 loss = 1.438795\n",
      "epoch 108 loss = 1.438509\n",
      "epoch 109 loss = 1.521503\n",
      "epoch 110 loss = 1.399601\n",
      "epoch 111 loss = 1.377853\n",
      "epoch 112 loss = 1.622270\n",
      "epoch 113 loss = 1.403028\n",
      "epoch 114 loss = 1.319465\n",
      "epoch 115 loss = 1.489993\n",
      "epoch 116 loss = 1.276400\n",
      "epoch 117 loss = 1.471963\n",
      "epoch 118 loss = 1.462476\n",
      "epoch 119 loss = 1.429355\n",
      "epoch 120 loss = 1.337617\n",
      "epoch 121 loss = 1.308815\n",
      "epoch 122 loss = 1.476091\n",
      "epoch 123 loss = 1.329284\n",
      "epoch 124 loss = 1.391763\n",
      "epoch 125 loss = 1.354226\n",
      "epoch 126 loss = 1.476881\n",
      "epoch 127 loss = 1.374546\n",
      "epoch 128 loss = 1.293966\n",
      "epoch 129 loss = 1.336327\n",
      "epoch 130 loss = 1.490784\n",
      "epoch 131 loss = 1.306263\n",
      "epoch 132 loss = 1.467541\n",
      "epoch 133 loss = 1.382441\n",
      "epoch 134 loss = 1.305446\n",
      "epoch 135 loss = 1.423758\n",
      "epoch 136 loss = 1.257805\n",
      "epoch 137 loss = 1.478072\n",
      "epoch 138 loss = 1.321905\n",
      "epoch 139 loss = 1.253693\n",
      "epoch 140 loss = 1.409889\n",
      "epoch 141 loss = 1.490881\n",
      "epoch 142 loss = 1.221075\n",
      "epoch 143 loss = 1.512263\n",
      "epoch 144 loss = 1.390443\n",
      "epoch 145 loss = 1.352871\n",
      "epoch 146 loss = 1.249003\n",
      "epoch 147 loss = 1.557110\n",
      "epoch 148 loss = 1.527026\n",
      "epoch 149 loss = 1.273576\n",
      "epoch 150 loss = 1.370971\n",
      "epoch 151 loss = 1.378945\n",
      "epoch 152 loss = 1.507132\n",
      "epoch 153 loss = 1.514370\n",
      "epoch 154 loss = 1.555874\n",
      "epoch 155 loss = 1.631137\n",
      "epoch 156 loss = 1.338778\n",
      "epoch 157 loss = 1.432211\n",
      "epoch 158 loss = 1.348098\n",
      "epoch 159 loss = 1.559775\n",
      "epoch 160 loss = 1.421595\n",
      "epoch 161 loss = 1.261409\n",
      "epoch 162 loss = 1.626043\n",
      "epoch 163 loss = 1.452325\n",
      "epoch 164 loss = 1.203296\n",
      "epoch 165 loss = 1.446878\n",
      "epoch 166 loss = 1.441087\n",
      "epoch 167 loss = 1.494924\n",
      "epoch 168 loss = 1.355291\n",
      "epoch 169 loss = 1.348206\n",
      "epoch 170 loss = 1.304373\n",
      "epoch 171 loss = 1.242303\n",
      "epoch 172 loss = 1.309451\n",
      "epoch 173 loss = 1.207148\n",
      "epoch 174 loss = 1.326635\n",
      "epoch 175 loss = 1.295676\n",
      "epoch 176 loss = 1.271737\n",
      "epoch 177 loss = 1.444123\n",
      "epoch 178 loss = 1.275034\n",
      "epoch 179 loss = 1.538896\n",
      "epoch 180 loss = 1.359903\n",
      "epoch 181 loss = 1.584247\n",
      "epoch 182 loss = 1.346548\n",
      "epoch 183 loss = 1.482082\n",
      "epoch 184 loss = 1.302907\n",
      "epoch 185 loss = 1.494493\n",
      "epoch 186 loss = 1.231419\n",
      "epoch 187 loss = 1.263165\n",
      "epoch 188 loss = 1.292729\n",
      "epoch 189 loss = 1.286022\n",
      "epoch 190 loss = 1.503686\n",
      "epoch 191 loss = 1.278545\n",
      "epoch 192 loss = 1.340632\n",
      "epoch 193 loss = 1.391862\n",
      "epoch 194 loss = 1.209408\n",
      "epoch 195 loss = 1.345969\n",
      "epoch 196 loss = 1.478285\n",
      "epoch 197 loss = 1.252049\n",
      "epoch 198 loss = 1.297720\n",
      "epoch 199 loss = 1.205742\n",
      "epoch 200 loss = 1.694751\n",
      "epoch 201 loss = 1.446515\n",
      "epoch 202 loss = 1.417789\n",
      "epoch 203 loss = 1.331540\n",
      "epoch 204 loss = 1.254666\n",
      "epoch 205 loss = 1.314414\n",
      "epoch 206 loss = 1.597061\n",
      "epoch 207 loss = 1.197738\n",
      "epoch 208 loss = 1.258766\n",
      "epoch 209 loss = 1.181297\n",
      "epoch 210 loss = 1.280236\n",
      "epoch 211 loss = 1.220744\n",
      "epoch 212 loss = 1.282214\n",
      "epoch 213 loss = 1.548294\n",
      "epoch 214 loss = 1.410099\n",
      "epoch 215 loss = 1.375946\n",
      "epoch 216 loss = 1.533992\n",
      "epoch 217 loss = 1.420265\n",
      "epoch 218 loss = 1.178700\n",
      "epoch 219 loss = 1.293785\n",
      "epoch 220 loss = 1.469934\n",
      "epoch 221 loss = 1.159316\n",
      "epoch 222 loss = 1.347041\n",
      "epoch 223 loss = 1.338714\n",
      "epoch 224 loss = 1.334265\n",
      "epoch 225 loss = 1.255499\n",
      "epoch 226 loss = 1.170345\n",
      "epoch 227 loss = 1.344998\n",
      "epoch 228 loss = 1.202269\n",
      "epoch 229 loss = 1.131974\n",
      "epoch 230 loss = 1.210777\n",
      "epoch 231 loss = 1.254147\n",
      "epoch 232 loss = 1.275608\n",
      "epoch 233 loss = 1.378751\n",
      "epoch 234 loss = 1.146464\n",
      "epoch 235 loss = 1.172434\n",
      "epoch 236 loss = 1.220506\n",
      "epoch 237 loss = 1.447110\n",
      "epoch 238 loss = 1.213512\n",
      "epoch 239 loss = 1.326530\n",
      "epoch 240 loss = 1.208675\n",
      "epoch 241 loss = 1.350284\n",
      "epoch 242 loss = 1.317959\n",
      "epoch 243 loss = 1.301692\n",
      "epoch 244 loss = 1.282142\n",
      "epoch 245 loss = 1.143605\n",
      "epoch 246 loss = 1.204377\n",
      "epoch 247 loss = 1.541715\n",
      "epoch 248 loss = 1.285961\n",
      "epoch 249 loss = 1.219092\n",
      "epoch 250 loss = 1.204952\n",
      "epoch 251 loss = 1.312565\n",
      "epoch 252 loss = 1.164040\n",
      "epoch 253 loss = 1.112687\n",
      "epoch 254 loss = 1.126579\n",
      "epoch 255 loss = 1.289707\n",
      "epoch 256 loss = 1.311074\n",
      "epoch 257 loss = 1.196585\n",
      "epoch 258 loss = 1.156139\n",
      "epoch 259 loss = 1.497931\n",
      "epoch 260 loss = 1.205321\n",
      "epoch 261 loss = 1.436137\n",
      "epoch 262 loss = 1.196901\n",
      "epoch 263 loss = 1.345786\n",
      "epoch 264 loss = 1.311257\n",
      "epoch 265 loss = 1.175832\n",
      "epoch 266 loss = 1.101917\n",
      "epoch 267 loss = 1.166574\n",
      "epoch 268 loss = 1.110245\n",
      "epoch 269 loss = 1.158857\n",
      "epoch 270 loss = 1.161867\n",
      "epoch 271 loss = 1.447421\n",
      "epoch 272 loss = 1.299500\n",
      "epoch 273 loss = 1.411468\n",
      "epoch 274 loss = 1.414527\n",
      "epoch 275 loss = 1.311319\n",
      "epoch 276 loss = 1.153696\n",
      "epoch 277 loss = 1.286423\n",
      "epoch 278 loss = 1.456345\n",
      "epoch 279 loss = 1.231971\n",
      "epoch 280 loss = 1.259658\n",
      "epoch 281 loss = 1.524593\n",
      "epoch 282 loss = 1.373302\n",
      "epoch 283 loss = 1.084935\n",
      "epoch 284 loss = 1.182308\n",
      "epoch 285 loss = 1.209815\n",
      "epoch 286 loss = 1.117810\n",
      "epoch 287 loss = 1.119415\n",
      "epoch 288 loss = 1.364523\n",
      "epoch 289 loss = 1.144403\n",
      "epoch 290 loss = 1.463531\n",
      "epoch 291 loss = 1.165570\n",
      "epoch 292 loss = 1.082936\n",
      "epoch 293 loss = 1.215589\n",
      "epoch 294 loss = 1.305751\n",
      "epoch 295 loss = 1.235602\n",
      "epoch 296 loss = 1.379418\n",
      "epoch 297 loss = 1.277813\n",
      "epoch 298 loss = 1.340888\n",
      "epoch 299 loss = 1.163822\n",
      "epoch 300 loss = 1.070474\n",
      "epoch 301 loss = 1.176126\n",
      "epoch 302 loss = 1.177639\n",
      "epoch 303 loss = 1.206006\n",
      "epoch 304 loss = 1.466521\n",
      "epoch 305 loss = 1.500410\n",
      "epoch 306 loss = 1.229483\n",
      "epoch 307 loss = 1.201589\n",
      "epoch 308 loss = 1.432121\n",
      "epoch 309 loss = 1.050305\n",
      "epoch 310 loss = 1.160848\n",
      "epoch 311 loss = 1.275216\n",
      "epoch 312 loss = 1.350618\n",
      "epoch 313 loss = 1.232194\n",
      "epoch 314 loss = 1.255386\n",
      "epoch 315 loss = 1.378705\n",
      "epoch 316 loss = 1.122853\n",
      "epoch 317 loss = 1.136981\n",
      "epoch 318 loss = 1.212193\n",
      "epoch 319 loss = 1.283805\n",
      "epoch 320 loss = 1.206141\n",
      "epoch 321 loss = 1.269880\n",
      "epoch 322 loss = 1.510859\n",
      "epoch 323 loss = 1.270711\n",
      "epoch 324 loss = 1.260985\n",
      "epoch 325 loss = 1.266528\n",
      "epoch 326 loss = 1.116774\n",
      "epoch 327 loss = 1.484738\n",
      "epoch 328 loss = 1.547547\n",
      "epoch 329 loss = 1.188806\n",
      "epoch 330 loss = 1.200428\n",
      "epoch 331 loss = 1.350132\n",
      "epoch 332 loss = 1.094515\n",
      "epoch 333 loss = 1.092638\n",
      "epoch 334 loss = 1.394341\n",
      "epoch 335 loss = 1.191044\n",
      "epoch 336 loss = 1.329323\n",
      "epoch 337 loss = 1.165015\n",
      "epoch 338 loss = 1.219292\n",
      "epoch 339 loss = 1.319116\n",
      "epoch 340 loss = 1.454030\n",
      "epoch 341 loss = 1.068592\n",
      "epoch 342 loss = 1.132472\n",
      "epoch 343 loss = 1.071877\n",
      "epoch 344 loss = 1.111208\n",
      "epoch 345 loss = 1.227644\n",
      "epoch 346 loss = 1.205591\n",
      "epoch 347 loss = 1.134422\n",
      "epoch 348 loss = 1.059187\n",
      "epoch 349 loss = 1.056060\n",
      "epoch 350 loss = 1.165181\n",
      "epoch 351 loss = 1.080085\n",
      "epoch 352 loss = 1.247784\n",
      "epoch 353 loss = 1.243561\n",
      "epoch 354 loss = 1.536873\n",
      "epoch 355 loss = 1.093235\n",
      "epoch 356 loss = 1.186196\n",
      "epoch 357 loss = 1.413701\n",
      "epoch 358 loss = 1.417201\n",
      "epoch 359 loss = 1.302396\n",
      "epoch 360 loss = 1.301811\n",
      "epoch 361 loss = 1.337919\n",
      "epoch 362 loss = 1.094219\n",
      "epoch 363 loss = 1.226896\n",
      "epoch 364 loss = 1.184377\n",
      "epoch 365 loss = 1.221474\n",
      "epoch 366 loss = 1.316233\n",
      "epoch 367 loss = 1.165470\n",
      "epoch 368 loss = 1.129294\n",
      "epoch 369 loss = 1.195913\n",
      "epoch 370 loss = 1.113397\n",
      "epoch 371 loss = 1.151927\n",
      "epoch 372 loss = 1.151723\n",
      "epoch 373 loss = 1.339318\n",
      "epoch 374 loss = 1.384376\n",
      "epoch 375 loss = 1.132860\n",
      "epoch 376 loss = 1.103524\n",
      "epoch 377 loss = 1.238063\n",
      "epoch 378 loss = 1.068060\n",
      "epoch 379 loss = 1.264509\n",
      "epoch 380 loss = 1.122821\n",
      "epoch 381 loss = 1.204629\n",
      "epoch 382 loss = 1.507509\n",
      "epoch 383 loss = 1.018629\n",
      "epoch 384 loss = 1.068141\n",
      "epoch 385 loss = 1.144184\n",
      "epoch 386 loss = 1.082616\n",
      "epoch 387 loss = 1.472933\n",
      "epoch 388 loss = 1.257120\n",
      "epoch 389 loss = 1.222511\n",
      "epoch 390 loss = 1.110963\n",
      "epoch 391 loss = 1.142390\n",
      "epoch 392 loss = 1.266493\n",
      "epoch 393 loss = 1.371393\n",
      "epoch 394 loss = 1.307174\n",
      "epoch 395 loss = 1.135836\n",
      "epoch 396 loss = 1.248981\n",
      "epoch 397 loss = 1.346146\n",
      "epoch 398 loss = 1.192991\n",
      "epoch 399 loss = 1.300817\n",
      "epoch 400 loss = 1.284665\n",
      "epoch 401 loss = 1.312423\n",
      "epoch 402 loss = 1.302534\n",
      "epoch 403 loss = 1.342771\n",
      "epoch 404 loss = 1.138985\n",
      "epoch 405 loss = 1.316271\n",
      "epoch 406 loss = 1.296219\n",
      "epoch 407 loss = 1.265242\n",
      "epoch 408 loss = 1.260320\n",
      "epoch 409 loss = 1.282331\n",
      "epoch 410 loss = 1.118230\n",
      "epoch 411 loss = 1.220012\n",
      "epoch 412 loss = 1.114041\n",
      "epoch 413 loss = 1.056155\n",
      "epoch 414 loss = 1.172772\n",
      "epoch 415 loss = 1.229795\n",
      "epoch 416 loss = 1.028135\n",
      "epoch 417 loss = 1.234946\n",
      "epoch 418 loss = 1.216955\n",
      "epoch 419 loss = 1.142266\n",
      "epoch 420 loss = 1.282676\n",
      "epoch 421 loss = 1.229174\n",
      "epoch 422 loss = 1.142727\n",
      "epoch 423 loss = 1.383369\n",
      "epoch 424 loss = 1.265917\n",
      "epoch 425 loss = 1.079029\n",
      "epoch 426 loss = 1.261267\n",
      "epoch 427 loss = 1.115169\n",
      "epoch 428 loss = 1.259793\n",
      "epoch 429 loss = 1.315119\n",
      "epoch 430 loss = 1.030706\n",
      "epoch 431 loss = 1.057685\n",
      "epoch 432 loss = 1.052195\n",
      "epoch 433 loss = 1.210770\n",
      "epoch 434 loss = 1.235039\n",
      "epoch 435 loss = 1.149375\n",
      "epoch 436 loss = 1.105927\n",
      "epoch 437 loss = 1.141134\n",
      "epoch 438 loss = 1.282682\n",
      "epoch 439 loss = 1.102751\n",
      "epoch 440 loss = 1.183214\n",
      "epoch 441 loss = 1.123858\n",
      "epoch 442 loss = 1.166443\n",
      "epoch 443 loss = 1.318064\n",
      "epoch 444 loss = 1.059780\n",
      "epoch 445 loss = 1.020896\n",
      "epoch 446 loss = 1.209568\n",
      "epoch 447 loss = 1.160001\n",
      "epoch 448 loss = 1.110777\n",
      "epoch 449 loss = 1.131495\n",
      "epoch 450 loss = 1.161226\n",
      "epoch 451 loss = 1.031761\n",
      "epoch 452 loss = 1.421796\n",
      "epoch 453 loss = 1.044559\n",
      "epoch 454 loss = 1.054124\n",
      "epoch 455 loss = 1.136413\n",
      "epoch 456 loss = 1.174299\n",
      "epoch 457 loss = 1.289607\n",
      "epoch 458 loss = 0.981359\n",
      "epoch 459 loss = 1.137340\n",
      "epoch 460 loss = 1.479461\n",
      "epoch 461 loss = 1.045673\n",
      "epoch 462 loss = 0.987424\n",
      "epoch 463 loss = 1.165276\n",
      "epoch 464 loss = 1.193992\n",
      "epoch 465 loss = 1.098241\n",
      "epoch 466 loss = 1.147297\n",
      "epoch 467 loss = 1.152667\n",
      "epoch 468 loss = 1.304943\n",
      "epoch 469 loss = 1.167485\n",
      "epoch 470 loss = 1.469040\n",
      "epoch 471 loss = 1.124662\n",
      "epoch 472 loss = 1.212369\n",
      "epoch 473 loss = 1.139104\n",
      "epoch 474 loss = 1.161834\n",
      "epoch 475 loss = 1.299010\n",
      "epoch 476 loss = 1.108133\n",
      "epoch 477 loss = 1.083144\n",
      "epoch 478 loss = 0.994169\n",
      "epoch 479 loss = 1.172068\n",
      "epoch 480 loss = 1.111180\n",
      "epoch 481 loss = 1.345349\n",
      "epoch 482 loss = 1.142558\n",
      "epoch 483 loss = 0.998020\n",
      "epoch 484 loss = 1.276380\n",
      "epoch 485 loss = 1.093328\n",
      "epoch 486 loss = 1.377377\n",
      "epoch 487 loss = 1.166398\n",
      "epoch 488 loss = 1.359548\n",
      "epoch 489 loss = 1.128299\n",
      "epoch 490 loss = 1.217059\n",
      "epoch 491 loss = 1.244791\n",
      "epoch 492 loss = 1.188784\n",
      "epoch 493 loss = 1.216211\n",
      "epoch 494 loss = 1.292376\n",
      "epoch 495 loss = 1.206071\n",
      "epoch 496 loss = 1.306236\n",
      "epoch 497 loss = 1.083512\n",
      "epoch 498 loss = 1.069864\n",
      "epoch 499 loss = 1.235605\n",
      "final loss = 1.235605\n",
      "accuracy_mc = tensor(0.4693, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4726, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6061, device='cuda:0')\n",
      "training time = 170.24698162078857 seconds\n",
      "testing time = 1.8272581100463867 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.282334\n",
      "epoch 1 loss = 2.267128\n",
      "epoch 2 loss = 2.214388\n",
      "epoch 3 loss = 2.171804\n",
      "epoch 4 loss = 2.081778\n",
      "epoch 5 loss = 1.980536\n",
      "epoch 6 loss = 1.985640\n",
      "epoch 7 loss = 1.867608\n",
      "epoch 8 loss = 1.891011\n",
      "epoch 9 loss = 1.911069\n",
      "epoch 10 loss = 1.907236\n",
      "epoch 11 loss = 1.883273\n",
      "epoch 12 loss = 2.003714\n",
      "epoch 13 loss = 1.944194\n",
      "epoch 14 loss = 1.885656\n",
      "epoch 15 loss = 1.788540\n",
      "epoch 16 loss = 1.862461\n",
      "epoch 17 loss = 1.850888\n",
      "epoch 18 loss = 1.800936\n",
      "epoch 19 loss = 1.869428\n",
      "epoch 20 loss = 1.824875\n",
      "epoch 21 loss = 1.834689\n",
      "epoch 22 loss = 1.750547\n",
      "epoch 23 loss = 1.916295\n",
      "epoch 24 loss = 1.878596\n",
      "epoch 25 loss = 1.850559\n",
      "epoch 26 loss = 1.837757\n",
      "epoch 27 loss = 1.804728\n",
      "epoch 28 loss = 1.857623\n",
      "epoch 29 loss = 1.782165\n",
      "epoch 30 loss = 1.723851\n",
      "epoch 31 loss = 1.793505\n",
      "epoch 32 loss = 1.878050\n",
      "epoch 33 loss = 1.790069\n",
      "epoch 34 loss = 1.799969\n",
      "epoch 35 loss = 1.713219\n",
      "epoch 36 loss = 1.653672\n",
      "epoch 37 loss = 1.743255\n",
      "epoch 38 loss = 1.780720\n",
      "epoch 39 loss = 1.706919\n",
      "epoch 40 loss = 1.690100\n",
      "epoch 41 loss = 1.742849\n",
      "epoch 42 loss = 1.738789\n",
      "epoch 43 loss = 1.720940\n",
      "epoch 44 loss = 1.844086\n",
      "epoch 45 loss = 1.734667\n",
      "epoch 46 loss = 1.788100\n",
      "epoch 47 loss = 1.736194\n",
      "epoch 48 loss = 1.674271\n",
      "epoch 49 loss = 1.685333\n",
      "epoch 50 loss = 1.664978\n",
      "epoch 51 loss = 1.656880\n",
      "epoch 52 loss = 1.760062\n",
      "epoch 53 loss = 1.660673\n",
      "epoch 54 loss = 1.767613\n",
      "epoch 55 loss = 1.618444\n",
      "epoch 56 loss = 1.674915\n",
      "epoch 57 loss = 1.751603\n",
      "epoch 58 loss = 1.597169\n",
      "epoch 59 loss = 1.710597\n",
      "epoch 60 loss = 1.775075\n",
      "epoch 61 loss = 1.656831\n",
      "epoch 62 loss = 1.721538\n",
      "epoch 63 loss = 1.876820\n",
      "epoch 64 loss = 2.011608\n",
      "epoch 65 loss = 1.688871\n",
      "epoch 66 loss = 1.706609\n",
      "epoch 67 loss = 1.732276\n",
      "epoch 68 loss = 1.690115\n",
      "epoch 69 loss = 1.787936\n",
      "epoch 70 loss = 1.838457\n",
      "epoch 71 loss = 1.727569\n",
      "epoch 72 loss = 1.639030\n",
      "epoch 73 loss = 1.827887\n",
      "epoch 74 loss = 1.863384\n",
      "epoch 75 loss = 1.629776\n",
      "epoch 76 loss = 1.656958\n",
      "epoch 77 loss = 1.750476\n",
      "epoch 78 loss = 1.827702\n",
      "epoch 79 loss = 1.852207\n",
      "epoch 80 loss = 1.827379\n",
      "epoch 81 loss = 1.620301\n",
      "epoch 82 loss = 1.609989\n",
      "epoch 83 loss = 1.603785\n",
      "epoch 84 loss = 1.640005\n",
      "epoch 85 loss = 1.691548\n",
      "epoch 86 loss = 1.737937\n",
      "epoch 87 loss = 1.641789\n",
      "epoch 88 loss = 1.565692\n",
      "epoch 89 loss = 1.845585\n",
      "epoch 90 loss = 1.647398\n",
      "epoch 91 loss = 1.680256\n",
      "epoch 92 loss = 1.604912\n",
      "epoch 93 loss = 1.575567\n",
      "epoch 94 loss = 1.550653\n",
      "epoch 95 loss = 1.769109\n",
      "epoch 96 loss = 1.590724\n",
      "epoch 97 loss = 1.537742\n",
      "epoch 98 loss = 1.648099\n",
      "epoch 99 loss = 1.720132\n",
      "epoch 100 loss = 1.631553\n",
      "epoch 101 loss = 1.507305\n",
      "epoch 102 loss = 1.601610\n",
      "epoch 103 loss = 1.607102\n",
      "epoch 104 loss = 1.569596\n",
      "epoch 105 loss = 1.606536\n",
      "epoch 106 loss = 1.834632\n",
      "epoch 107 loss = 1.710616\n",
      "epoch 108 loss = 1.612441\n",
      "epoch 109 loss = 1.642055\n",
      "epoch 110 loss = 1.542820\n",
      "epoch 111 loss = 1.642973\n",
      "epoch 112 loss = 1.666538\n",
      "epoch 113 loss = 1.565308\n",
      "epoch 114 loss = 1.533975\n",
      "epoch 115 loss = 1.432413\n",
      "epoch 116 loss = 1.627577\n",
      "epoch 117 loss = 1.691053\n",
      "epoch 118 loss = 1.599333\n",
      "epoch 119 loss = 1.496180\n",
      "epoch 120 loss = 1.549076\n",
      "epoch 121 loss = 1.513581\n",
      "epoch 122 loss = 1.446464\n",
      "epoch 123 loss = 1.597061\n",
      "epoch 124 loss = 1.498221\n",
      "epoch 125 loss = 1.500607\n",
      "epoch 126 loss = 1.382456\n",
      "epoch 127 loss = 1.511352\n",
      "epoch 128 loss = 1.678814\n",
      "epoch 129 loss = 1.467121\n",
      "epoch 130 loss = 1.434986\n",
      "epoch 131 loss = 1.417969\n",
      "epoch 132 loss = 1.468246\n",
      "epoch 133 loss = 1.489476\n",
      "epoch 134 loss = 1.350195\n",
      "epoch 135 loss = 1.364227\n",
      "epoch 136 loss = 1.465340\n",
      "epoch 137 loss = 1.388444\n",
      "epoch 138 loss = 1.570891\n",
      "epoch 139 loss = 1.337907\n",
      "epoch 140 loss = 1.488060\n",
      "epoch 141 loss = 1.536443\n",
      "epoch 142 loss = 1.349673\n",
      "epoch 143 loss = 1.383995\n",
      "epoch 144 loss = 1.409817\n",
      "epoch 145 loss = 1.360914\n",
      "epoch 146 loss = 1.545213\n",
      "epoch 147 loss = 1.462532\n",
      "epoch 148 loss = 1.345669\n",
      "epoch 149 loss = 1.387822\n",
      "epoch 150 loss = 1.636120\n",
      "epoch 151 loss = 1.353781\n",
      "epoch 152 loss = 1.369430\n",
      "epoch 153 loss = 1.403433\n",
      "epoch 154 loss = 1.556533\n",
      "epoch 155 loss = 1.456482\n",
      "epoch 156 loss = 1.412079\n",
      "epoch 157 loss = 1.452771\n",
      "epoch 158 loss = 1.443469\n",
      "epoch 159 loss = 1.414799\n",
      "epoch 160 loss = 1.479265\n",
      "epoch 161 loss = 1.303921\n",
      "epoch 162 loss = 1.422893\n",
      "epoch 163 loss = 1.304202\n",
      "epoch 164 loss = 1.459483\n",
      "epoch 165 loss = 1.348930\n",
      "epoch 166 loss = 1.216159\n",
      "epoch 167 loss = 1.346937\n",
      "epoch 168 loss = 1.475767\n",
      "epoch 169 loss = 1.309271\n",
      "epoch 170 loss = 1.291821\n",
      "epoch 171 loss = 1.328139\n",
      "epoch 172 loss = 1.413894\n",
      "epoch 173 loss = 1.299871\n",
      "epoch 174 loss = 1.443661\n",
      "epoch 175 loss = 1.313317\n",
      "epoch 176 loss = 1.246485\n",
      "epoch 177 loss = 1.205613\n",
      "epoch 178 loss = 1.229600\n",
      "epoch 179 loss = 1.343882\n",
      "epoch 180 loss = 1.330857\n",
      "epoch 181 loss = 1.343572\n",
      "epoch 182 loss = 1.278035\n",
      "epoch 183 loss = 1.176780\n",
      "epoch 184 loss = 1.291922\n",
      "epoch 185 loss = 1.204103\n",
      "epoch 186 loss = 1.472836\n",
      "epoch 187 loss = 1.303057\n",
      "epoch 188 loss = 1.337523\n",
      "epoch 189 loss = 1.286443\n",
      "epoch 190 loss = 1.322466\n",
      "epoch 191 loss = 1.180438\n",
      "epoch 192 loss = 1.314882\n",
      "epoch 193 loss = 1.237240\n",
      "epoch 194 loss = 1.444983\n",
      "epoch 195 loss = 1.274095\n",
      "epoch 196 loss = 1.497838\n",
      "epoch 197 loss = 1.264972\n",
      "epoch 198 loss = 1.356956\n",
      "epoch 199 loss = 1.219717\n",
      "epoch 200 loss = 1.207497\n",
      "epoch 201 loss = 1.371899\n",
      "epoch 202 loss = 1.410291\n",
      "epoch 203 loss = 1.294263\n",
      "epoch 204 loss = 1.547661\n",
      "epoch 205 loss = 1.270367\n",
      "epoch 206 loss = 1.334664\n",
      "epoch 207 loss = 1.612286\n",
      "epoch 208 loss = 1.298076\n",
      "epoch 209 loss = 1.282755\n",
      "epoch 210 loss = 1.251703\n",
      "epoch 211 loss = 1.257579\n",
      "epoch 212 loss = 1.294922\n",
      "epoch 213 loss = 1.326770\n",
      "epoch 214 loss = 1.505941\n",
      "epoch 215 loss = 1.256693\n",
      "epoch 216 loss = 1.655177\n",
      "epoch 217 loss = 1.372124\n",
      "epoch 218 loss = 1.352805\n",
      "epoch 219 loss = 1.345445\n",
      "epoch 220 loss = 1.298378\n",
      "epoch 221 loss = 1.269242\n",
      "epoch 222 loss = 1.437907\n",
      "epoch 223 loss = 1.360982\n",
      "epoch 224 loss = 1.564415\n",
      "epoch 225 loss = 1.530917\n",
      "epoch 226 loss = 1.417503\n",
      "epoch 227 loss = 1.218574\n",
      "epoch 228 loss = 1.366633\n",
      "epoch 229 loss = 1.302934\n",
      "epoch 230 loss = 1.304277\n",
      "epoch 231 loss = 1.373660\n",
      "epoch 232 loss = 1.354984\n",
      "epoch 233 loss = 1.235705\n",
      "epoch 234 loss = 1.343930\n",
      "epoch 235 loss = 1.317499\n",
      "epoch 236 loss = 1.440347\n",
      "epoch 237 loss = 1.275876\n",
      "epoch 238 loss = 1.346741\n",
      "epoch 239 loss = 1.431893\n",
      "epoch 240 loss = 1.491005\n",
      "epoch 241 loss = 1.237616\n",
      "epoch 242 loss = 1.314073\n",
      "epoch 243 loss = 1.306868\n",
      "epoch 244 loss = 1.253780\n",
      "epoch 245 loss = 1.269844\n",
      "epoch 246 loss = 1.351062\n",
      "epoch 247 loss = 1.336175\n",
      "epoch 248 loss = 1.370945\n",
      "epoch 249 loss = 1.316519\n",
      "epoch 250 loss = 1.372751\n",
      "epoch 251 loss = 1.215717\n",
      "epoch 252 loss = 1.333067\n",
      "epoch 253 loss = 1.308905\n",
      "epoch 254 loss = 1.385084\n",
      "epoch 255 loss = 1.384133\n",
      "epoch 256 loss = 1.297087\n",
      "epoch 257 loss = 1.216008\n",
      "epoch 258 loss = 1.305644\n",
      "epoch 259 loss = 1.264491\n",
      "epoch 260 loss = 1.191841\n",
      "epoch 261 loss = 1.322841\n",
      "epoch 262 loss = 1.444917\n",
      "epoch 263 loss = 1.418666\n",
      "epoch 264 loss = 1.528127\n",
      "epoch 265 loss = 1.320178\n",
      "epoch 266 loss = 1.244305\n",
      "epoch 267 loss = 1.134324\n",
      "epoch 268 loss = 1.559866\n",
      "epoch 269 loss = 1.308320\n",
      "epoch 270 loss = 1.263276\n",
      "epoch 271 loss = 1.491380\n",
      "epoch 272 loss = 1.180331\n",
      "epoch 273 loss = 1.271198\n",
      "epoch 274 loss = 1.416718\n",
      "epoch 275 loss = 1.354048\n",
      "epoch 276 loss = 1.350572\n",
      "epoch 277 loss = 1.524472\n",
      "epoch 278 loss = 1.158993\n",
      "epoch 279 loss = 1.230424\n",
      "epoch 280 loss = 1.344774\n",
      "epoch 281 loss = 1.290606\n",
      "epoch 282 loss = 1.323118\n",
      "epoch 283 loss = 1.144095\n",
      "epoch 284 loss = 1.318635\n",
      "epoch 285 loss = 1.168096\n",
      "epoch 286 loss = 1.296178\n",
      "epoch 287 loss = 1.333433\n",
      "epoch 288 loss = 1.342372\n",
      "epoch 289 loss = 1.252582\n",
      "epoch 290 loss = 1.188797\n",
      "epoch 291 loss = 1.324418\n",
      "epoch 292 loss = 1.230730\n",
      "epoch 293 loss = 1.225044\n",
      "epoch 294 loss = 1.305927\n",
      "epoch 295 loss = 1.279203\n",
      "epoch 296 loss = 1.212713\n",
      "epoch 297 loss = 1.235920\n",
      "epoch 298 loss = 1.328885\n",
      "epoch 299 loss = 1.156008\n",
      "epoch 300 loss = 1.164117\n",
      "epoch 301 loss = 1.288222\n",
      "epoch 302 loss = 1.217015\n",
      "epoch 303 loss = 1.183536\n",
      "epoch 304 loss = 1.290787\n",
      "epoch 305 loss = 1.253144\n",
      "epoch 306 loss = 1.337312\n",
      "epoch 307 loss = 1.235078\n",
      "epoch 308 loss = 1.217394\n",
      "epoch 309 loss = 1.387041\n",
      "epoch 310 loss = 1.430138\n",
      "epoch 311 loss = 1.363689\n",
      "epoch 312 loss = 1.282465\n",
      "epoch 313 loss = 1.384043\n",
      "epoch 314 loss = 1.304756\n",
      "epoch 315 loss = 1.444611\n",
      "epoch 316 loss = 1.251451\n",
      "epoch 317 loss = 1.107120\n",
      "epoch 318 loss = 1.170311\n",
      "epoch 319 loss = 1.234384\n",
      "epoch 320 loss = 1.080835\n",
      "epoch 321 loss = 1.101595\n",
      "epoch 322 loss = 1.470283\n",
      "epoch 323 loss = 1.273121\n",
      "epoch 324 loss = 1.356189\n",
      "epoch 325 loss = 1.368981\n",
      "epoch 326 loss = 1.367139\n",
      "epoch 327 loss = 1.094767\n",
      "epoch 328 loss = 1.368080\n",
      "epoch 329 loss = 1.272654\n",
      "epoch 330 loss = 1.329005\n",
      "epoch 331 loss = 1.367853\n",
      "epoch 332 loss = 1.285462\n",
      "epoch 333 loss = 1.293643\n",
      "epoch 334 loss = 1.166760\n",
      "epoch 335 loss = 1.414773\n",
      "epoch 336 loss = 1.248198\n",
      "epoch 337 loss = 1.284369\n",
      "epoch 338 loss = 1.219749\n",
      "epoch 339 loss = 1.286862\n",
      "epoch 340 loss = 1.238875\n",
      "epoch 341 loss = 1.357221\n",
      "epoch 342 loss = 1.308740\n",
      "epoch 343 loss = 1.192741\n",
      "epoch 344 loss = 1.251590\n",
      "epoch 345 loss = 1.234211\n",
      "epoch 346 loss = 1.377907\n",
      "epoch 347 loss = 1.176959\n",
      "epoch 348 loss = 1.376960\n",
      "epoch 349 loss = 1.435199\n",
      "epoch 350 loss = 1.162377\n",
      "epoch 351 loss = 1.187186\n",
      "epoch 352 loss = 1.401577\n",
      "epoch 353 loss = 1.138882\n",
      "epoch 354 loss = 1.225158\n",
      "epoch 355 loss = 1.323399\n",
      "epoch 356 loss = 1.247517\n",
      "epoch 357 loss = 1.215908\n",
      "epoch 358 loss = 1.263463\n",
      "epoch 359 loss = 1.373156\n",
      "epoch 360 loss = 1.215288\n",
      "epoch 361 loss = 1.156091\n",
      "epoch 362 loss = 1.193270\n",
      "epoch 363 loss = 1.538571\n",
      "epoch 364 loss = 1.242561\n",
      "epoch 365 loss = 1.228084\n",
      "epoch 366 loss = 1.508858\n",
      "epoch 367 loss = 1.341570\n",
      "epoch 368 loss = 1.284508\n",
      "epoch 369 loss = 1.505578\n",
      "epoch 370 loss = 1.475038\n",
      "epoch 371 loss = 1.103980\n",
      "epoch 372 loss = 1.258104\n",
      "epoch 373 loss = 1.138859\n",
      "epoch 374 loss = 1.165069\n",
      "epoch 375 loss = 1.183704\n",
      "epoch 376 loss = 1.058171\n",
      "epoch 377 loss = 1.322836\n",
      "epoch 378 loss = 1.286410\n",
      "epoch 379 loss = 1.112857\n",
      "epoch 380 loss = 1.277895\n",
      "epoch 381 loss = 1.352479\n",
      "epoch 382 loss = 1.145383\n",
      "epoch 383 loss = 1.219853\n",
      "epoch 384 loss = 1.280749\n",
      "epoch 385 loss = 1.203615\n",
      "epoch 386 loss = 1.184780\n",
      "epoch 387 loss = 1.369168\n",
      "epoch 388 loss = 1.350297\n",
      "epoch 389 loss = 1.220941\n",
      "epoch 390 loss = 1.246123\n",
      "epoch 391 loss = 1.180810\n",
      "epoch 392 loss = 1.242750\n",
      "epoch 393 loss = 1.110097\n",
      "epoch 394 loss = 1.173059\n",
      "epoch 395 loss = 1.224816\n",
      "epoch 396 loss = 1.339557\n",
      "epoch 397 loss = 1.213614\n",
      "epoch 398 loss = 1.130770\n",
      "epoch 399 loss = 1.144212\n",
      "epoch 400 loss = 1.163409\n",
      "epoch 401 loss = 1.263560\n",
      "epoch 402 loss = 1.005883\n",
      "epoch 403 loss = 1.154878\n",
      "epoch 404 loss = 1.289083\n",
      "epoch 405 loss = 1.372043\n",
      "epoch 406 loss = 1.196465\n",
      "epoch 407 loss = 1.009547\n",
      "epoch 408 loss = 1.314985\n",
      "epoch 409 loss = 1.157984\n",
      "epoch 410 loss = 1.203063\n",
      "epoch 411 loss = 1.222721\n",
      "epoch 412 loss = 1.227236\n",
      "epoch 413 loss = 1.203088\n",
      "epoch 414 loss = 1.050389\n",
      "epoch 415 loss = 1.154099\n",
      "epoch 416 loss = 1.144933\n",
      "epoch 417 loss = 1.202271\n",
      "epoch 418 loss = 1.310774\n",
      "epoch 419 loss = 1.293393\n",
      "epoch 420 loss = 1.081524\n",
      "epoch 421 loss = 1.127294\n",
      "epoch 422 loss = 1.116667\n",
      "epoch 423 loss = 1.338463\n",
      "epoch 424 loss = 1.378015\n",
      "epoch 425 loss = 1.209835\n",
      "epoch 426 loss = 1.151776\n",
      "epoch 427 loss = 1.306863\n",
      "epoch 428 loss = 1.376526\n",
      "epoch 429 loss = 1.042023\n",
      "epoch 430 loss = 1.123145\n",
      "epoch 431 loss = 1.235735\n",
      "epoch 432 loss = 1.279368\n",
      "epoch 433 loss = 1.322864\n",
      "epoch 434 loss = 1.153477\n",
      "epoch 435 loss = 1.101490\n",
      "epoch 436 loss = 1.186211\n",
      "epoch 437 loss = 1.271676\n",
      "epoch 438 loss = 1.157735\n",
      "epoch 439 loss = 1.141003\n",
      "epoch 440 loss = 1.159467\n",
      "epoch 441 loss = 1.169908\n",
      "epoch 442 loss = 1.137849\n",
      "epoch 443 loss = 1.229572\n",
      "epoch 444 loss = 1.227023\n",
      "epoch 445 loss = 1.065925\n",
      "epoch 446 loss = 1.151021\n",
      "epoch 447 loss = 1.292331\n",
      "epoch 448 loss = 1.154150\n",
      "epoch 449 loss = 1.202746\n",
      "epoch 450 loss = 1.206784\n",
      "epoch 451 loss = 1.240834\n",
      "epoch 452 loss = 1.188135\n",
      "epoch 453 loss = 1.158710\n",
      "epoch 454 loss = 0.993383\n",
      "epoch 455 loss = 0.987635\n",
      "epoch 456 loss = 1.100907\n",
      "epoch 457 loss = 1.318809\n",
      "epoch 458 loss = 1.246247\n",
      "epoch 459 loss = 1.055972\n",
      "epoch 460 loss = 1.014168\n",
      "epoch 461 loss = 1.095069\n",
      "epoch 462 loss = 1.032114\n",
      "epoch 463 loss = 1.013963\n",
      "epoch 464 loss = 1.054263\n",
      "epoch 465 loss = 1.050535\n",
      "epoch 466 loss = 1.115869\n",
      "epoch 467 loss = 1.069719\n",
      "epoch 468 loss = 1.099876\n",
      "epoch 469 loss = 1.353681\n",
      "epoch 470 loss = 1.300315\n",
      "epoch 471 loss = 1.098889\n",
      "epoch 472 loss = 1.165322\n",
      "epoch 473 loss = 1.189662\n",
      "epoch 474 loss = 1.063038\n",
      "epoch 475 loss = 1.036843\n",
      "epoch 476 loss = 1.101830\n",
      "epoch 477 loss = 1.080344\n",
      "epoch 478 loss = 1.184125\n",
      "epoch 479 loss = 1.143315\n",
      "epoch 480 loss = 1.102872\n",
      "epoch 481 loss = 1.035624\n",
      "epoch 482 loss = 1.155095\n",
      "epoch 483 loss = 1.134643\n",
      "epoch 484 loss = 1.204007\n",
      "epoch 485 loss = 1.204221\n",
      "epoch 486 loss = 1.282164\n",
      "epoch 487 loss = 1.103553\n",
      "epoch 488 loss = 1.300610\n",
      "epoch 489 loss = 1.216551\n",
      "epoch 490 loss = 1.002620\n",
      "epoch 491 loss = 1.109278\n",
      "epoch 492 loss = 1.077072\n",
      "epoch 493 loss = 1.010808\n",
      "epoch 494 loss = 1.223608\n",
      "epoch 495 loss = 1.122186\n",
      "epoch 496 loss = 1.021671\n",
      "epoch 497 loss = 1.143308\n",
      "epoch 498 loss = 1.048003\n",
      "epoch 499 loss = 1.063717\n",
      "final loss = 1.063717\n",
      "accuracy_mc = tensor(0.3757, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3738, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6312, device='cuda:0')\n",
      "training time = 170.09120917320251 seconds\n",
      "testing time = 1.8479392528533936 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.274773\n",
      "epoch 1 loss = 2.227073\n",
      "epoch 2 loss = 2.151856\n",
      "epoch 3 loss = 1.954994\n",
      "epoch 4 loss = 1.946905\n",
      "epoch 5 loss = 1.949658\n",
      "epoch 6 loss = 1.852386\n",
      "epoch 7 loss = 1.811804\n",
      "epoch 8 loss = 1.817024\n",
      "epoch 9 loss = 1.806715\n",
      "epoch 10 loss = 1.887881\n",
      "epoch 11 loss = 1.816217\n",
      "epoch 12 loss = 1.651753\n",
      "epoch 13 loss = 1.770365\n",
      "epoch 14 loss = 1.712505\n",
      "epoch 15 loss = 1.756891\n",
      "epoch 16 loss = 1.613341\n",
      "epoch 17 loss = 1.623300\n",
      "epoch 18 loss = 1.785495\n",
      "epoch 19 loss = 1.655931\n",
      "epoch 20 loss = 1.618158\n",
      "epoch 21 loss = 1.516306\n",
      "epoch 22 loss = 1.693478\n",
      "epoch 23 loss = 1.627836\n",
      "epoch 24 loss = 1.673965\n",
      "epoch 25 loss = 1.593337\n",
      "epoch 26 loss = 1.590114\n",
      "epoch 27 loss = 1.693863\n",
      "epoch 28 loss = 1.617811\n",
      "epoch 29 loss = 1.649104\n",
      "epoch 30 loss = 1.542114\n",
      "epoch 31 loss = 1.562401\n",
      "epoch 32 loss = 1.719015\n",
      "epoch 33 loss = 1.455406\n",
      "epoch 34 loss = 1.593978\n",
      "epoch 35 loss = 1.370898\n",
      "epoch 36 loss = 1.446342\n",
      "epoch 37 loss = 1.566239\n",
      "epoch 38 loss = 1.544077\n",
      "epoch 39 loss = 1.527802\n",
      "epoch 40 loss = 1.403637\n",
      "epoch 41 loss = 1.493235\n",
      "epoch 42 loss = 1.604807\n",
      "epoch 43 loss = 1.551777\n",
      "epoch 44 loss = 1.535336\n",
      "epoch 45 loss = 1.352184\n",
      "epoch 46 loss = 1.469680\n",
      "epoch 47 loss = 1.565977\n",
      "epoch 48 loss = 1.365087\n",
      "epoch 49 loss = 1.459598\n",
      "epoch 50 loss = 1.432996\n",
      "epoch 51 loss = 1.417763\n",
      "epoch 52 loss = 1.303244\n",
      "epoch 53 loss = 1.588699\n",
      "epoch 54 loss = 1.448910\n",
      "epoch 55 loss = 1.278672\n",
      "epoch 56 loss = 1.522493\n",
      "epoch 57 loss = 1.431285\n",
      "epoch 58 loss = 1.437680\n",
      "epoch 59 loss = 1.451972\n",
      "epoch 60 loss = 1.403892\n",
      "epoch 61 loss = 1.546707\n",
      "epoch 62 loss = 1.387825\n",
      "epoch 63 loss = 1.346755\n",
      "epoch 64 loss = 1.479412\n",
      "epoch 65 loss = 1.413649\n",
      "epoch 66 loss = 1.428546\n",
      "epoch 67 loss = 1.359510\n",
      "epoch 68 loss = 1.339673\n",
      "epoch 69 loss = 1.343543\n",
      "epoch 70 loss = 1.513382\n",
      "epoch 71 loss = 1.444486\n",
      "epoch 72 loss = 1.415980\n",
      "epoch 73 loss = 1.518685\n",
      "epoch 74 loss = 1.339629\n",
      "epoch 75 loss = 1.426757\n",
      "epoch 76 loss = 1.580370\n",
      "epoch 77 loss = 1.175542\n",
      "epoch 78 loss = 1.421904\n",
      "epoch 79 loss = 1.527227\n",
      "epoch 80 loss = 1.412128\n",
      "epoch 81 loss = 1.269705\n",
      "epoch 82 loss = 1.340908\n",
      "epoch 83 loss = 1.327586\n",
      "epoch 84 loss = 1.353938\n",
      "epoch 85 loss = 1.451253\n",
      "epoch 86 loss = 1.311592\n",
      "epoch 87 loss = 1.513727\n",
      "epoch 88 loss = 1.427117\n",
      "epoch 89 loss = 1.221880\n",
      "epoch 90 loss = 1.326578\n",
      "epoch 91 loss = 1.645922\n",
      "epoch 92 loss = 1.394131\n",
      "epoch 93 loss = 1.382885\n",
      "epoch 94 loss = 1.214683\n",
      "epoch 95 loss = 1.390879\n",
      "epoch 96 loss = 1.194232\n",
      "epoch 97 loss = 1.248610\n",
      "epoch 98 loss = 1.339949\n",
      "epoch 99 loss = 1.273864\n",
      "epoch 100 loss = 1.423644\n",
      "epoch 101 loss = 1.299740\n",
      "epoch 102 loss = 1.295289\n",
      "epoch 103 loss = 1.440568\n",
      "epoch 104 loss = 1.307185\n",
      "epoch 105 loss = 1.281240\n",
      "epoch 106 loss = 1.171932\n",
      "epoch 107 loss = 1.221372\n",
      "epoch 108 loss = 1.370731\n",
      "epoch 109 loss = 1.406834\n",
      "epoch 110 loss = 1.280229\n",
      "epoch 111 loss = 1.225443\n",
      "epoch 112 loss = 1.399105\n",
      "epoch 113 loss = 1.305433\n",
      "epoch 114 loss = 1.149743\n",
      "epoch 115 loss = 1.486780\n",
      "epoch 116 loss = 1.410072\n",
      "epoch 117 loss = 1.193132\n",
      "epoch 118 loss = 1.359967\n",
      "epoch 119 loss = 1.408157\n",
      "epoch 120 loss = 1.144732\n",
      "epoch 121 loss = 1.348639\n",
      "epoch 122 loss = 1.210157\n",
      "epoch 123 loss = 1.401623\n",
      "epoch 124 loss = 1.100245\n",
      "epoch 125 loss = 1.151200\n",
      "epoch 126 loss = 1.294749\n",
      "epoch 127 loss = 1.176173\n",
      "epoch 128 loss = 1.109364\n",
      "epoch 129 loss = 1.301565\n",
      "epoch 130 loss = 1.365925\n",
      "epoch 131 loss = 1.154815\n",
      "epoch 132 loss = 1.135990\n",
      "epoch 133 loss = 1.233941\n",
      "epoch 134 loss = 1.117203\n",
      "epoch 135 loss = 1.143525\n",
      "epoch 136 loss = 1.279923\n",
      "epoch 137 loss = 1.157366\n",
      "epoch 138 loss = 1.148852\n",
      "epoch 139 loss = 1.178054\n",
      "epoch 140 loss = 1.197003\n",
      "epoch 141 loss = 1.180129\n",
      "epoch 142 loss = 1.182705\n",
      "epoch 143 loss = 1.251581\n",
      "epoch 144 loss = 1.130569\n",
      "epoch 145 loss = 1.178718\n",
      "epoch 146 loss = 1.200443\n",
      "epoch 147 loss = 1.322793\n",
      "epoch 148 loss = 1.097765\n",
      "epoch 149 loss = 1.129824\n",
      "epoch 150 loss = 1.428519\n",
      "epoch 151 loss = 0.958385\n",
      "epoch 152 loss = 1.065030\n",
      "epoch 153 loss = 1.222718\n",
      "epoch 154 loss = 1.240557\n",
      "epoch 155 loss = 1.217927\n",
      "epoch 156 loss = 1.422231\n",
      "epoch 157 loss = 1.099856\n",
      "epoch 158 loss = 1.229792\n",
      "epoch 159 loss = 1.336796\n",
      "epoch 160 loss = 1.264403\n",
      "epoch 161 loss = 1.091870\n",
      "epoch 162 loss = 1.187952\n",
      "epoch 163 loss = 1.271407\n",
      "epoch 164 loss = 1.271492\n",
      "epoch 165 loss = 1.158370\n",
      "epoch 166 loss = 1.029944\n",
      "epoch 167 loss = 1.114593\n",
      "epoch 168 loss = 1.111750\n",
      "epoch 169 loss = 1.375715\n",
      "epoch 170 loss = 1.221857\n",
      "epoch 171 loss = 1.042541\n",
      "epoch 172 loss = 1.035019\n",
      "epoch 173 loss = 1.227974\n",
      "epoch 174 loss = 1.051521\n",
      "epoch 175 loss = 0.978555\n",
      "epoch 176 loss = 1.174213\n",
      "epoch 177 loss = 1.096215\n",
      "epoch 178 loss = 0.983856\n",
      "epoch 179 loss = 1.164948\n",
      "epoch 180 loss = 1.081599\n",
      "epoch 181 loss = 1.067713\n",
      "epoch 182 loss = 1.109140\n",
      "epoch 183 loss = 1.275664\n",
      "epoch 184 loss = 1.167091\n",
      "epoch 185 loss = 1.244775\n",
      "epoch 186 loss = 1.033657\n",
      "epoch 187 loss = 1.226917\n",
      "epoch 188 loss = 1.297018\n",
      "epoch 189 loss = 1.203962\n",
      "epoch 190 loss = 1.208783\n",
      "epoch 191 loss = 1.372700\n",
      "epoch 192 loss = 1.110861\n",
      "epoch 193 loss = 1.516781\n",
      "epoch 194 loss = 1.293685\n",
      "epoch 195 loss = 0.964582\n",
      "epoch 196 loss = 1.263224\n",
      "epoch 197 loss = 1.209511\n",
      "epoch 198 loss = 1.181576\n",
      "epoch 199 loss = 1.109746\n",
      "epoch 200 loss = 1.138686\n",
      "epoch 201 loss = 1.238034\n",
      "epoch 202 loss = 1.232983\n",
      "epoch 203 loss = 1.193239\n",
      "epoch 204 loss = 1.158552\n",
      "epoch 205 loss = 1.208937\n",
      "epoch 206 loss = 1.138799\n",
      "epoch 207 loss = 1.045296\n",
      "epoch 208 loss = 1.182801\n",
      "epoch 209 loss = 1.065051\n",
      "epoch 210 loss = 1.326862\n",
      "epoch 211 loss = 1.235101\n",
      "epoch 212 loss = 1.071372\n",
      "epoch 213 loss = 1.257696\n",
      "epoch 214 loss = 1.254527\n",
      "epoch 215 loss = 1.134541\n",
      "epoch 216 loss = 1.087076\n",
      "epoch 217 loss = 1.096316\n",
      "epoch 218 loss = 1.182595\n",
      "epoch 219 loss = 1.118506\n",
      "epoch 220 loss = 1.220906\n",
      "epoch 221 loss = 1.079009\n",
      "epoch 222 loss = 1.115745\n",
      "epoch 223 loss = 1.238898\n",
      "epoch 224 loss = 1.039794\n",
      "epoch 225 loss = 1.178211\n",
      "epoch 226 loss = 1.268289\n",
      "epoch 227 loss = 1.315800\n",
      "epoch 228 loss = 1.016794\n",
      "epoch 229 loss = 1.150401\n",
      "epoch 230 loss = 1.284380\n",
      "epoch 231 loss = 1.007826\n",
      "epoch 232 loss = 1.059785\n",
      "epoch 233 loss = 1.288236\n",
      "epoch 234 loss = 1.023006\n",
      "epoch 235 loss = 1.195445\n",
      "epoch 236 loss = 1.140410\n",
      "epoch 237 loss = 0.997979\n",
      "epoch 238 loss = 1.068566\n",
      "epoch 239 loss = 1.113308\n",
      "epoch 240 loss = 1.098267\n",
      "epoch 241 loss = 0.975149\n",
      "epoch 242 loss = 1.115689\n",
      "epoch 243 loss = 1.166307\n",
      "epoch 244 loss = 1.206929\n",
      "epoch 245 loss = 1.205479\n",
      "epoch 246 loss = 1.229024\n",
      "epoch 247 loss = 1.098862\n",
      "epoch 248 loss = 0.985484\n",
      "epoch 249 loss = 1.095838\n",
      "epoch 250 loss = 1.044723\n",
      "epoch 251 loss = 1.091652\n",
      "epoch 252 loss = 1.177654\n",
      "epoch 253 loss = 1.123428\n",
      "epoch 254 loss = 1.217638\n",
      "epoch 255 loss = 1.258983\n",
      "epoch 256 loss = 1.012181\n",
      "epoch 257 loss = 1.201634\n",
      "epoch 258 loss = 1.019298\n",
      "epoch 259 loss = 1.274736\n",
      "epoch 260 loss = 0.975651\n",
      "epoch 261 loss = 0.907674\n",
      "epoch 262 loss = 1.265200\n",
      "epoch 263 loss = 1.035118\n",
      "epoch 264 loss = 0.971250\n",
      "epoch 265 loss = 1.107709\n",
      "epoch 266 loss = 1.081540\n",
      "epoch 267 loss = 1.055701\n",
      "epoch 268 loss = 1.243601\n",
      "epoch 269 loss = 1.082901\n",
      "epoch 270 loss = 1.082768\n",
      "epoch 271 loss = 1.065276\n",
      "epoch 272 loss = 1.091654\n",
      "epoch 273 loss = 1.040554\n",
      "epoch 274 loss = 1.010431\n",
      "epoch 275 loss = 1.193675\n",
      "epoch 276 loss = 1.044200\n",
      "epoch 277 loss = 0.998399\n",
      "epoch 278 loss = 1.102131\n",
      "epoch 279 loss = 1.234530\n",
      "epoch 280 loss = 1.019844\n",
      "epoch 281 loss = 0.926798\n",
      "epoch 282 loss = 1.060089\n",
      "epoch 283 loss = 0.959395\n",
      "epoch 284 loss = 0.976066\n",
      "epoch 285 loss = 0.977338\n",
      "epoch 286 loss = 1.175866\n",
      "epoch 287 loss = 1.076736\n",
      "epoch 288 loss = 0.962287\n",
      "epoch 289 loss = 1.062800\n",
      "epoch 290 loss = 1.145882\n",
      "epoch 291 loss = 0.854697\n",
      "epoch 292 loss = 1.075230\n",
      "epoch 293 loss = 1.003802\n",
      "epoch 294 loss = 1.105724\n",
      "epoch 295 loss = 0.957792\n",
      "epoch 296 loss = 1.094988\n",
      "epoch 297 loss = 0.913393\n",
      "epoch 298 loss = 0.982261\n",
      "epoch 299 loss = 0.960540\n",
      "epoch 300 loss = 1.198470\n",
      "epoch 301 loss = 1.155644\n",
      "epoch 302 loss = 1.073231\n",
      "epoch 303 loss = 1.039880\n",
      "epoch 304 loss = 0.946993\n",
      "epoch 305 loss = 0.847455\n",
      "epoch 306 loss = 0.929848\n",
      "epoch 307 loss = 0.965611\n",
      "epoch 308 loss = 1.041388\n",
      "epoch 309 loss = 1.315035\n",
      "epoch 310 loss = 1.197593\n",
      "epoch 311 loss = 0.848921\n",
      "epoch 312 loss = 1.039631\n",
      "epoch 313 loss = 0.907863\n",
      "epoch 314 loss = 0.902952\n",
      "epoch 315 loss = 1.072575\n",
      "epoch 316 loss = 1.069620\n",
      "epoch 317 loss = 1.100519\n",
      "epoch 318 loss = 1.099634\n",
      "epoch 319 loss = 1.007851\n",
      "epoch 320 loss = 1.527379\n",
      "epoch 321 loss = 1.029619\n",
      "epoch 322 loss = 1.167931\n",
      "epoch 323 loss = 1.189919\n",
      "epoch 324 loss = 1.227405\n",
      "epoch 325 loss = 1.069473\n",
      "epoch 326 loss = 0.899460\n",
      "epoch 327 loss = 0.960489\n",
      "epoch 328 loss = 1.067781\n",
      "epoch 329 loss = 0.918974\n",
      "epoch 330 loss = 1.062837\n",
      "epoch 331 loss = 0.951899\n",
      "epoch 332 loss = 1.044620\n",
      "epoch 333 loss = 0.915263\n",
      "epoch 334 loss = 1.099699\n",
      "epoch 335 loss = 1.131828\n",
      "epoch 336 loss = 1.185835\n",
      "epoch 337 loss = 1.080146\n",
      "epoch 338 loss = 1.054354\n",
      "epoch 339 loss = 1.217709\n",
      "epoch 340 loss = 0.857249\n",
      "epoch 341 loss = 0.846422\n",
      "epoch 342 loss = 1.008400\n",
      "epoch 343 loss = 1.033522\n",
      "epoch 344 loss = 0.849937\n",
      "epoch 345 loss = 0.942952\n",
      "epoch 346 loss = 0.790503\n",
      "epoch 347 loss = 1.156329\n",
      "epoch 348 loss = 1.150051\n",
      "epoch 349 loss = 1.106645\n",
      "epoch 350 loss = 1.195045\n",
      "epoch 351 loss = 0.958260\n",
      "epoch 352 loss = 1.187913\n",
      "epoch 353 loss = 1.017526\n",
      "epoch 354 loss = 0.918607\n",
      "epoch 355 loss = 0.989197\n",
      "epoch 356 loss = 0.956651\n",
      "epoch 357 loss = 0.910300\n",
      "epoch 358 loss = 0.966854\n",
      "epoch 359 loss = 0.922152\n",
      "epoch 360 loss = 0.912600\n",
      "epoch 361 loss = 1.053481\n",
      "epoch 362 loss = 0.950837\n",
      "epoch 363 loss = 0.999779\n",
      "epoch 364 loss = 1.175467\n",
      "epoch 365 loss = 0.985749\n",
      "epoch 366 loss = 1.015256\n",
      "epoch 367 loss = 1.042465\n",
      "epoch 368 loss = 0.896477\n",
      "epoch 369 loss = 1.115674\n",
      "epoch 370 loss = 0.904891\n",
      "epoch 371 loss = 1.109290\n",
      "epoch 372 loss = 1.206276\n",
      "epoch 373 loss = 1.266002\n",
      "epoch 374 loss = 0.950621\n",
      "epoch 375 loss = 0.885156\n",
      "epoch 376 loss = 0.844020\n",
      "epoch 377 loss = 1.043560\n",
      "epoch 378 loss = 0.913953\n",
      "epoch 379 loss = 1.000666\n",
      "epoch 380 loss = 1.446561\n",
      "epoch 381 loss = 0.984998\n",
      "epoch 382 loss = 0.987931\n",
      "epoch 383 loss = 1.060258\n",
      "epoch 384 loss = 1.042370\n",
      "epoch 385 loss = 0.903260\n",
      "epoch 386 loss = 1.020652\n",
      "epoch 387 loss = 0.991768\n",
      "epoch 388 loss = 0.951923\n",
      "epoch 389 loss = 0.973010\n",
      "epoch 390 loss = 0.878911\n",
      "epoch 391 loss = 1.227863\n",
      "epoch 392 loss = 0.962463\n",
      "epoch 393 loss = 0.945796\n",
      "epoch 394 loss = 1.046998\n",
      "epoch 395 loss = 0.986491\n",
      "epoch 396 loss = 0.934856\n",
      "epoch 397 loss = 1.238331\n",
      "epoch 398 loss = 1.056780\n",
      "epoch 399 loss = 0.960645\n",
      "epoch 400 loss = 1.057694\n",
      "epoch 401 loss = 0.964085\n",
      "epoch 402 loss = 0.959324\n",
      "epoch 403 loss = 0.877705\n",
      "epoch 404 loss = 1.105320\n",
      "epoch 405 loss = 0.905070\n",
      "epoch 406 loss = 1.058202\n",
      "epoch 407 loss = 1.328073\n",
      "epoch 408 loss = 1.133327\n",
      "epoch 409 loss = 0.989325\n",
      "epoch 410 loss = 1.133623\n",
      "epoch 411 loss = 1.072623\n",
      "epoch 412 loss = 0.852477\n",
      "epoch 413 loss = 0.923066\n",
      "epoch 414 loss = 1.078030\n",
      "epoch 415 loss = 1.019382\n",
      "epoch 416 loss = 1.104050\n",
      "epoch 417 loss = 0.884725\n",
      "epoch 418 loss = 0.964618\n",
      "epoch 419 loss = 1.024544\n",
      "epoch 420 loss = 0.999327\n",
      "epoch 421 loss = 1.014380\n",
      "epoch 422 loss = 1.029615\n",
      "epoch 423 loss = 0.943777\n",
      "epoch 424 loss = 1.041024\n",
      "epoch 425 loss = 0.960387\n",
      "epoch 426 loss = 1.005654\n",
      "epoch 427 loss = 1.033412\n",
      "epoch 428 loss = 1.089036\n",
      "epoch 429 loss = 0.947689\n",
      "epoch 430 loss = 1.000396\n",
      "epoch 431 loss = 0.878701\n",
      "epoch 432 loss = 0.925397\n",
      "epoch 433 loss = 0.937314\n",
      "epoch 434 loss = 0.905985\n",
      "epoch 435 loss = 0.950423\n",
      "epoch 436 loss = 1.042494\n",
      "epoch 437 loss = 1.008145\n",
      "epoch 438 loss = 0.889702\n",
      "epoch 439 loss = 0.850514\n",
      "epoch 440 loss = 1.087645\n",
      "epoch 441 loss = 1.105714\n",
      "epoch 442 loss = 1.294098\n",
      "epoch 443 loss = 0.917934\n",
      "epoch 444 loss = 1.357837\n",
      "epoch 445 loss = 0.770965\n",
      "epoch 446 loss = 0.878491\n",
      "epoch 447 loss = 0.864923\n",
      "epoch 448 loss = 1.179203\n",
      "epoch 449 loss = 0.820021\n",
      "epoch 450 loss = 0.878379\n",
      "epoch 451 loss = 0.902452\n",
      "epoch 452 loss = 0.932619\n",
      "epoch 453 loss = 1.331471\n",
      "epoch 454 loss = 0.886178\n",
      "epoch 455 loss = 1.003535\n",
      "epoch 456 loss = 0.853757\n",
      "epoch 457 loss = 1.016567\n",
      "epoch 458 loss = 1.069506\n",
      "epoch 459 loss = 1.125490\n",
      "epoch 460 loss = 0.917088\n",
      "epoch 461 loss = 1.165799\n",
      "epoch 462 loss = 0.946370\n",
      "epoch 463 loss = 1.111684\n",
      "epoch 464 loss = 0.745893\n",
      "epoch 465 loss = 0.889156\n",
      "epoch 466 loss = 0.947191\n",
      "epoch 467 loss = 1.115375\n",
      "epoch 468 loss = 0.974295\n",
      "epoch 469 loss = 1.142304\n",
      "epoch 470 loss = 0.790665\n",
      "epoch 471 loss = 1.044432\n",
      "epoch 472 loss = 0.903061\n",
      "epoch 473 loss = 1.016444\n",
      "epoch 474 loss = 0.813236\n",
      "epoch 475 loss = 0.856842\n",
      "epoch 476 loss = 0.984691\n",
      "epoch 477 loss = 1.086595\n",
      "epoch 478 loss = 0.845759\n",
      "epoch 479 loss = 1.042834\n",
      "epoch 480 loss = 1.031261\n",
      "epoch 481 loss = 0.833309\n",
      "epoch 482 loss = 1.088269\n",
      "epoch 483 loss = 0.966012\n",
      "epoch 484 loss = 1.003616\n",
      "epoch 485 loss = 0.899260\n",
      "epoch 486 loss = 0.898727\n",
      "epoch 487 loss = 0.902535\n",
      "epoch 488 loss = 0.825450\n",
      "epoch 489 loss = 1.041238\n",
      "epoch 490 loss = 0.962307\n",
      "epoch 491 loss = 1.007014\n",
      "epoch 492 loss = 1.050239\n",
      "epoch 493 loss = 1.035738\n",
      "epoch 494 loss = 0.835050\n",
      "epoch 495 loss = 0.900354\n",
      "epoch 496 loss = 0.897776\n",
      "epoch 497 loss = 0.916154\n",
      "epoch 498 loss = 0.833648\n",
      "epoch 499 loss = 0.890773\n",
      "final loss = 0.890773\n",
      "accuracy_mc = tensor(0.3934, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4053, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6294, device='cuda:0')\n",
      "training time = 170.15856766700745 seconds\n",
      "testing time = 1.8911216259002686 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 2.270210\n",
      "epoch 1 loss = 2.216937\n",
      "epoch 2 loss = 2.105033\n",
      "epoch 3 loss = 2.035171\n",
      "epoch 4 loss = 1.864773\n",
      "epoch 5 loss = 1.920787\n",
      "epoch 6 loss = 1.900923\n",
      "epoch 7 loss = 1.907363\n",
      "epoch 8 loss = 1.730450\n",
      "epoch 9 loss = 1.900172\n",
      "epoch 10 loss = 1.874949\n",
      "epoch 11 loss = 1.763353\n",
      "epoch 12 loss = 1.865546\n",
      "epoch 13 loss = 1.775675\n",
      "epoch 14 loss = 1.728826\n",
      "epoch 15 loss = 1.889243\n",
      "epoch 16 loss = 1.697579\n",
      "epoch 17 loss = 1.804948\n",
      "epoch 18 loss = 1.717410\n",
      "epoch 19 loss = 1.720859\n",
      "epoch 20 loss = 1.767098\n",
      "epoch 21 loss = 1.665183\n",
      "epoch 22 loss = 1.821642\n",
      "epoch 23 loss = 1.759521\n",
      "epoch 24 loss = 1.828244\n",
      "epoch 25 loss = 1.589599\n",
      "epoch 26 loss = 1.881506\n",
      "epoch 27 loss = 1.691170\n",
      "epoch 28 loss = 1.738635\n",
      "epoch 29 loss = 1.895827\n",
      "epoch 30 loss = 1.627179\n",
      "epoch 31 loss = 1.736956\n",
      "epoch 32 loss = 1.644687\n",
      "epoch 33 loss = 1.760243\n",
      "epoch 34 loss = 1.555389\n",
      "epoch 35 loss = 1.695322\n",
      "epoch 36 loss = 1.666103\n",
      "epoch 37 loss = 1.565484\n",
      "epoch 38 loss = 1.576498\n",
      "epoch 39 loss = 1.655162\n",
      "epoch 40 loss = 1.507937\n",
      "epoch 41 loss = 1.543774\n",
      "epoch 42 loss = 1.639100\n",
      "epoch 43 loss = 1.673654\n",
      "epoch 44 loss = 1.705025\n",
      "epoch 45 loss = 1.600960\n",
      "epoch 46 loss = 1.577254\n",
      "epoch 47 loss = 1.619547\n",
      "epoch 48 loss = 1.694393\n",
      "epoch 49 loss = 1.569332\n",
      "epoch 50 loss = 1.529706\n",
      "epoch 51 loss = 1.441197\n",
      "epoch 52 loss = 1.456286\n",
      "epoch 53 loss = 1.468838\n",
      "epoch 54 loss = 1.714791\n",
      "epoch 55 loss = 1.577757\n",
      "epoch 56 loss = 1.421759\n",
      "epoch 57 loss = 1.612708\n",
      "epoch 58 loss = 1.502924\n",
      "epoch 59 loss = 1.492092\n",
      "epoch 60 loss = 1.493658\n",
      "epoch 61 loss = 1.359356\n",
      "epoch 62 loss = 1.630576\n",
      "epoch 63 loss = 1.487430\n",
      "epoch 64 loss = 1.545072\n",
      "epoch 65 loss = 1.560028\n",
      "epoch 66 loss = 1.449795\n",
      "epoch 67 loss = 1.371717\n",
      "epoch 68 loss = 1.573907\n",
      "epoch 69 loss = 1.532265\n",
      "epoch 70 loss = 1.505727\n",
      "epoch 71 loss = 1.390381\n",
      "epoch 72 loss = 1.442838\n",
      "epoch 73 loss = 1.396819\n",
      "epoch 74 loss = 1.469326\n",
      "epoch 75 loss = 1.409389\n",
      "epoch 76 loss = 1.465223\n",
      "epoch 77 loss = 1.446111\n",
      "epoch 78 loss = 1.611174\n",
      "epoch 79 loss = 1.507636\n",
      "epoch 80 loss = 1.431988\n",
      "epoch 81 loss = 1.367136\n",
      "epoch 82 loss = 1.392431\n",
      "epoch 83 loss = 1.406884\n",
      "epoch 84 loss = 1.334869\n",
      "epoch 85 loss = 1.493526\n",
      "epoch 86 loss = 1.537041\n",
      "epoch 87 loss = 1.312327\n",
      "epoch 88 loss = 1.418384\n",
      "epoch 89 loss = 1.490852\n",
      "epoch 90 loss = 1.510959\n",
      "epoch 91 loss = 1.417258\n",
      "epoch 92 loss = 1.545681\n",
      "epoch 93 loss = 1.336942\n",
      "epoch 94 loss = 1.392114\n",
      "epoch 95 loss = 1.355708\n",
      "epoch 96 loss = 1.416659\n",
      "epoch 97 loss = 1.432625\n",
      "epoch 98 loss = 1.401338\n",
      "epoch 99 loss = 1.433062\n",
      "epoch 100 loss = 1.395273\n",
      "epoch 101 loss = 1.373572\n",
      "epoch 102 loss = 1.377665\n",
      "epoch 103 loss = 1.268829\n",
      "epoch 104 loss = 1.633482\n",
      "epoch 105 loss = 1.341985\n",
      "epoch 106 loss = 1.343070\n",
      "epoch 107 loss = 1.449270\n",
      "epoch 108 loss = 1.467208\n",
      "epoch 109 loss = 1.427380\n",
      "epoch 110 loss = 1.487362\n",
      "epoch 111 loss = 1.387539\n",
      "epoch 112 loss = 1.354984\n",
      "epoch 113 loss = 1.576547\n",
      "epoch 114 loss = 1.357748\n",
      "epoch 115 loss = 1.299371\n",
      "epoch 116 loss = 1.337533\n",
      "epoch 117 loss = 1.313546\n",
      "epoch 118 loss = 1.484960\n",
      "epoch 119 loss = 1.306907\n",
      "epoch 120 loss = 1.473587\n",
      "epoch 121 loss = 1.458988\n",
      "epoch 122 loss = 1.450085\n",
      "epoch 123 loss = 1.361262\n",
      "epoch 124 loss = 1.451436\n",
      "epoch 125 loss = 1.479933\n",
      "epoch 126 loss = 1.580642\n",
      "epoch 127 loss = 1.532468\n",
      "epoch 128 loss = 1.348684\n",
      "epoch 129 loss = 1.258538\n",
      "epoch 130 loss = 1.429366\n",
      "epoch 131 loss = 1.180893\n",
      "epoch 132 loss = 1.247596\n",
      "epoch 133 loss = 1.376266\n",
      "epoch 134 loss = 1.381239\n",
      "epoch 135 loss = 1.428186\n",
      "epoch 136 loss = 1.394323\n",
      "epoch 137 loss = 1.388843\n",
      "epoch 138 loss = 1.429442\n",
      "epoch 139 loss = 1.195749\n",
      "epoch 140 loss = 1.322505\n",
      "epoch 141 loss = 1.475246\n",
      "epoch 142 loss = 1.356980\n",
      "epoch 143 loss = 1.342561\n",
      "epoch 144 loss = 1.376845\n",
      "epoch 145 loss = 1.274515\n",
      "epoch 146 loss = 1.162393\n",
      "epoch 147 loss = 1.434270\n",
      "epoch 148 loss = 1.289078\n",
      "epoch 149 loss = 1.309869\n",
      "epoch 150 loss = 1.385599\n",
      "epoch 151 loss = 1.369977\n",
      "epoch 152 loss = 1.473535\n",
      "epoch 153 loss = 1.539225\n",
      "epoch 154 loss = 1.192461\n",
      "epoch 155 loss = 1.309178\n",
      "epoch 156 loss = 1.320482\n",
      "epoch 157 loss = 1.220217\n",
      "epoch 158 loss = 1.486097\n",
      "epoch 159 loss = 1.343410\n",
      "epoch 160 loss = 1.390024\n",
      "epoch 161 loss = 1.252765\n",
      "epoch 162 loss = 1.367200\n",
      "epoch 163 loss = 1.337540\n",
      "epoch 164 loss = 1.456788\n",
      "epoch 165 loss = 1.359513\n",
      "epoch 166 loss = 1.408400\n",
      "epoch 167 loss = 1.321652\n",
      "epoch 168 loss = 1.194097\n",
      "epoch 169 loss = 1.237707\n",
      "epoch 170 loss = 1.434176\n",
      "epoch 171 loss = 1.351352\n",
      "epoch 172 loss = 1.257796\n",
      "epoch 173 loss = 1.321959\n",
      "epoch 174 loss = 1.242448\n",
      "epoch 175 loss = 1.298201\n",
      "epoch 176 loss = 1.375323\n",
      "epoch 177 loss = 1.315637\n",
      "epoch 178 loss = 1.436685\n",
      "epoch 179 loss = 1.203853\n",
      "epoch 180 loss = 1.527615\n",
      "epoch 181 loss = 1.221282\n",
      "epoch 182 loss = 1.306494\n",
      "epoch 183 loss = 1.210996\n",
      "epoch 184 loss = 1.307705\n",
      "epoch 185 loss = 1.391531\n",
      "epoch 186 loss = 1.383985\n",
      "epoch 187 loss = 1.250415\n",
      "epoch 188 loss = 1.318752\n",
      "epoch 189 loss = 1.445022\n",
      "epoch 190 loss = 1.468336\n",
      "epoch 191 loss = 1.332029\n",
      "epoch 192 loss = 1.278047\n",
      "epoch 193 loss = 1.431226\n",
      "epoch 194 loss = 1.431656\n",
      "epoch 195 loss = 1.331874\n",
      "epoch 196 loss = 1.211351\n",
      "epoch 197 loss = 1.305510\n",
      "epoch 198 loss = 1.163091\n",
      "epoch 199 loss = 1.062509\n",
      "epoch 200 loss = 1.166145\n",
      "epoch 201 loss = 1.381972\n",
      "epoch 202 loss = 1.370207\n",
      "epoch 203 loss = 1.156754\n",
      "epoch 204 loss = 1.293761\n",
      "epoch 205 loss = 1.454311\n",
      "epoch 206 loss = 1.462412\n",
      "epoch 207 loss = 1.192896\n",
      "epoch 208 loss = 1.464591\n",
      "epoch 209 loss = 1.513748\n",
      "epoch 210 loss = 1.264704\n",
      "epoch 211 loss = 1.240137\n",
      "epoch 212 loss = 1.275415\n",
      "epoch 213 loss = 1.392105\n",
      "epoch 214 loss = 1.352404\n",
      "epoch 215 loss = 1.113246\n",
      "epoch 216 loss = 1.364468\n",
      "epoch 217 loss = 1.298048\n",
      "epoch 218 loss = 1.308092\n",
      "epoch 219 loss = 1.305182\n",
      "epoch 220 loss = 1.355061\n",
      "epoch 221 loss = 1.417282\n",
      "epoch 222 loss = 1.164115\n",
      "epoch 223 loss = 1.243639\n",
      "epoch 224 loss = 1.263756\n",
      "epoch 225 loss = 1.347079\n",
      "epoch 226 loss = 1.299070\n",
      "epoch 227 loss = 1.182384\n",
      "epoch 228 loss = 1.183543\n",
      "epoch 229 loss = 1.246086\n",
      "epoch 230 loss = 1.188208\n",
      "epoch 231 loss = 1.149724\n",
      "epoch 232 loss = 1.385164\n",
      "epoch 233 loss = 1.430507\n",
      "epoch 234 loss = 1.202308\n",
      "epoch 235 loss = 1.072795\n",
      "epoch 236 loss = 1.130832\n",
      "epoch 237 loss = 1.073398\n",
      "epoch 238 loss = 1.342886\n",
      "epoch 239 loss = 1.347009\n",
      "epoch 240 loss = 1.162620\n",
      "epoch 241 loss = 1.624599\n",
      "epoch 242 loss = 1.146440\n",
      "epoch 243 loss = 1.230605\n",
      "epoch 244 loss = 1.157912\n",
      "epoch 245 loss = 1.163455\n",
      "epoch 246 loss = 1.107460\n",
      "epoch 247 loss = 1.241312\n",
      "epoch 248 loss = 1.236010\n",
      "epoch 249 loss = 1.319215\n",
      "epoch 250 loss = 1.438267\n",
      "epoch 251 loss = 1.193349\n",
      "epoch 252 loss = 1.154113\n",
      "epoch 253 loss = 1.135402\n",
      "epoch 254 loss = 1.426241\n",
      "epoch 255 loss = 1.358114\n",
      "epoch 256 loss = 1.054522\n",
      "epoch 257 loss = 1.275182\n",
      "epoch 258 loss = 1.491204\n",
      "epoch 259 loss = 1.143965\n",
      "epoch 260 loss = 1.296131\n",
      "epoch 261 loss = 1.321944\n",
      "epoch 262 loss = 1.185166\n",
      "epoch 263 loss = 1.136326\n",
      "epoch 264 loss = 1.097062\n",
      "epoch 265 loss = 1.073525\n",
      "epoch 266 loss = 1.112881\n",
      "epoch 267 loss = 1.223058\n",
      "epoch 268 loss = 1.234992\n",
      "epoch 269 loss = 1.378121\n",
      "epoch 270 loss = 1.345413\n",
      "epoch 271 loss = 1.127620\n",
      "epoch 272 loss = 1.119465\n",
      "epoch 273 loss = 1.077198\n",
      "epoch 274 loss = 1.217218\n",
      "epoch 275 loss = 1.218584\n",
      "epoch 276 loss = 1.321724\n",
      "epoch 277 loss = 0.959064\n",
      "epoch 278 loss = 1.153828\n",
      "epoch 279 loss = 1.211246\n",
      "epoch 280 loss = 1.243544\n",
      "epoch 281 loss = 1.249595\n",
      "epoch 282 loss = 1.188249\n",
      "epoch 283 loss = 1.296305\n",
      "epoch 284 loss = 1.267025\n",
      "epoch 285 loss = 1.063781\n",
      "epoch 286 loss = 1.286031\n",
      "epoch 287 loss = 1.106940\n",
      "epoch 288 loss = 1.107497\n",
      "epoch 289 loss = 1.267683\n",
      "epoch 290 loss = 1.319883\n",
      "epoch 291 loss = 1.271567\n",
      "epoch 292 loss = 1.202075\n",
      "epoch 293 loss = 1.199375\n",
      "epoch 294 loss = 1.181867\n",
      "epoch 295 loss = 1.064870\n",
      "epoch 296 loss = 1.284115\n",
      "epoch 297 loss = 1.418160\n",
      "epoch 298 loss = 1.132928\n",
      "epoch 299 loss = 1.320186\n",
      "epoch 300 loss = 1.085321\n",
      "epoch 301 loss = 1.164039\n",
      "epoch 302 loss = 1.064815\n",
      "epoch 303 loss = 1.132482\n",
      "epoch 304 loss = 1.238696\n",
      "epoch 305 loss = 1.332603\n",
      "epoch 306 loss = 1.378614\n",
      "epoch 307 loss = 1.096091\n",
      "epoch 308 loss = 1.109033\n",
      "epoch 309 loss = 1.188312\n",
      "epoch 310 loss = 1.280297\n",
      "epoch 311 loss = 1.080873\n",
      "epoch 312 loss = 1.290612\n",
      "epoch 313 loss = 1.068056\n",
      "epoch 314 loss = 1.104288\n",
      "epoch 315 loss = 1.260579\n",
      "epoch 316 loss = 1.340363\n",
      "epoch 317 loss = 1.030293\n",
      "epoch 318 loss = 1.022350\n",
      "epoch 319 loss = 1.294173\n",
      "epoch 320 loss = 1.244387\n",
      "epoch 321 loss = 1.255079\n",
      "epoch 322 loss = 1.232991\n",
      "epoch 323 loss = 1.125228\n",
      "epoch 324 loss = 1.353418\n",
      "epoch 325 loss = 1.142231\n",
      "epoch 326 loss = 1.399631\n",
      "epoch 327 loss = 1.108027\n",
      "epoch 328 loss = 1.172525\n",
      "epoch 329 loss = 1.004568\n",
      "epoch 330 loss = 1.146787\n",
      "epoch 331 loss = 1.277241\n",
      "epoch 332 loss = 1.198939\n",
      "epoch 333 loss = 1.125389\n",
      "epoch 334 loss = 1.177303\n",
      "epoch 335 loss = 1.416424\n",
      "epoch 336 loss = 1.423954\n",
      "epoch 337 loss = 1.183015\n",
      "epoch 338 loss = 1.161864\n",
      "epoch 339 loss = 1.080170\n",
      "epoch 340 loss = 1.095011\n",
      "epoch 341 loss = 1.127347\n",
      "epoch 342 loss = 1.185500\n",
      "epoch 343 loss = 1.115006\n",
      "epoch 344 loss = 1.055598\n",
      "epoch 345 loss = 1.159019\n",
      "epoch 346 loss = 1.016786\n",
      "epoch 347 loss = 1.119136\n",
      "epoch 348 loss = 1.011152\n",
      "epoch 349 loss = 1.146856\n",
      "epoch 350 loss = 1.159097\n",
      "epoch 351 loss = 1.279463\n",
      "epoch 352 loss = 1.237792\n",
      "epoch 353 loss = 1.187359\n",
      "epoch 354 loss = 1.092545\n",
      "epoch 355 loss = 0.978848\n",
      "epoch 356 loss = 1.095448\n",
      "epoch 357 loss = 0.994699\n",
      "epoch 358 loss = 0.904683\n",
      "epoch 359 loss = 0.992899\n",
      "epoch 360 loss = 1.065826\n",
      "epoch 361 loss = 0.928043\n",
      "epoch 362 loss = 1.230176\n",
      "epoch 363 loss = 0.982276\n",
      "epoch 364 loss = 1.194043\n",
      "epoch 365 loss = 1.109837\n",
      "epoch 366 loss = 0.985545\n",
      "epoch 367 loss = 1.217258\n",
      "epoch 368 loss = 1.143870\n",
      "epoch 369 loss = 1.180214\n",
      "epoch 370 loss = 1.158876\n",
      "epoch 371 loss = 1.004984\n",
      "epoch 372 loss = 1.004053\n",
      "epoch 373 loss = 1.177131\n",
      "epoch 374 loss = 0.964633\n",
      "epoch 375 loss = 1.381382\n",
      "epoch 376 loss = 1.018073\n",
      "epoch 377 loss = 1.074336\n",
      "epoch 378 loss = 1.215985\n",
      "epoch 379 loss = 0.992073\n",
      "epoch 380 loss = 0.987813\n",
      "epoch 381 loss = 1.257588\n",
      "epoch 382 loss = 1.048314\n",
      "epoch 383 loss = 0.966711\n",
      "epoch 384 loss = 0.945213\n",
      "epoch 385 loss = 1.403653\n",
      "epoch 386 loss = 1.028449\n",
      "epoch 387 loss = 1.031406\n",
      "epoch 388 loss = 1.055981\n",
      "epoch 389 loss = 1.140463\n",
      "epoch 390 loss = 0.944663\n",
      "epoch 391 loss = 0.952893\n",
      "epoch 392 loss = 1.006573\n",
      "epoch 393 loss = 0.997801\n",
      "epoch 394 loss = 1.176306\n",
      "epoch 395 loss = 1.088901\n",
      "epoch 396 loss = 1.109459\n",
      "epoch 397 loss = 1.029972\n",
      "epoch 398 loss = 1.170477\n",
      "epoch 399 loss = 0.895310\n",
      "epoch 400 loss = 0.927768\n",
      "epoch 401 loss = 1.191656\n",
      "epoch 402 loss = 1.122634\n",
      "epoch 403 loss = 1.129484\n",
      "epoch 404 loss = 1.145074\n",
      "epoch 405 loss = 1.034644\n",
      "epoch 406 loss = 1.157714\n",
      "epoch 407 loss = 1.171637\n",
      "epoch 408 loss = 1.169637\n",
      "epoch 409 loss = 1.195746\n",
      "epoch 410 loss = 0.932828\n",
      "epoch 411 loss = 0.926587\n",
      "epoch 412 loss = 1.101946\n",
      "epoch 413 loss = 1.007206\n",
      "epoch 414 loss = 0.965883\n",
      "epoch 415 loss = 1.076428\n",
      "epoch 416 loss = 0.832259\n",
      "epoch 417 loss = 1.003825\n",
      "epoch 418 loss = 1.150638\n",
      "epoch 419 loss = 1.052512\n",
      "epoch 420 loss = 0.976875\n",
      "epoch 421 loss = 1.123876\n",
      "epoch 422 loss = 0.922478\n",
      "epoch 423 loss = 1.054779\n",
      "epoch 424 loss = 1.346839\n",
      "epoch 425 loss = 1.108366\n",
      "epoch 426 loss = 0.974870\n",
      "epoch 427 loss = 1.027708\n",
      "epoch 428 loss = 0.834731\n",
      "epoch 429 loss = 1.022686\n",
      "epoch 430 loss = 0.922445\n",
      "epoch 431 loss = 0.975811\n",
      "epoch 432 loss = 0.988578\n",
      "epoch 433 loss = 0.870480\n",
      "epoch 434 loss = 0.980894\n",
      "epoch 435 loss = 0.906278\n",
      "epoch 436 loss = 0.987628\n",
      "epoch 437 loss = 1.045336\n",
      "epoch 438 loss = 0.997790\n",
      "epoch 439 loss = 0.909557\n",
      "epoch 440 loss = 0.979520\n",
      "epoch 441 loss = 0.897293\n",
      "epoch 442 loss = 1.105478\n",
      "epoch 443 loss = 1.173167\n",
      "epoch 444 loss = 1.018132\n",
      "epoch 445 loss = 0.959685\n",
      "epoch 446 loss = 0.876548\n",
      "epoch 447 loss = 0.831914\n",
      "epoch 448 loss = 1.011660\n",
      "epoch 449 loss = 0.873402\n",
      "epoch 450 loss = 0.934773\n",
      "epoch 451 loss = 1.013049\n",
      "epoch 452 loss = 1.193089\n",
      "epoch 453 loss = 0.835235\n",
      "epoch 454 loss = 1.095909\n",
      "epoch 455 loss = 1.094933\n",
      "epoch 456 loss = 0.868378\n",
      "epoch 457 loss = 0.985294\n",
      "epoch 458 loss = 0.916891\n",
      "epoch 459 loss = 0.837945\n",
      "epoch 460 loss = 0.980725\n",
      "epoch 461 loss = 1.054866\n",
      "epoch 462 loss = 0.909091\n",
      "epoch 463 loss = 0.820617\n",
      "epoch 464 loss = 0.857248\n",
      "epoch 465 loss = 1.050812\n",
      "epoch 466 loss = 0.913817\n",
      "epoch 467 loss = 0.803761\n",
      "epoch 468 loss = 1.011212\n",
      "epoch 469 loss = 0.865488\n",
      "epoch 470 loss = 0.978626\n",
      "epoch 471 loss = 0.728433\n",
      "epoch 472 loss = 0.911454\n",
      "epoch 473 loss = 0.899056\n",
      "epoch 474 loss = 0.891995\n",
      "epoch 475 loss = 1.036945\n",
      "epoch 476 loss = 1.092173\n",
      "epoch 477 loss = 0.840136\n",
      "epoch 478 loss = 0.857576\n",
      "epoch 479 loss = 0.849320\n",
      "epoch 480 loss = 0.916532\n",
      "epoch 481 loss = 0.932066\n",
      "epoch 482 loss = 0.952860\n",
      "epoch 483 loss = 0.883750\n",
      "epoch 484 loss = 0.896019\n",
      "epoch 485 loss = 1.029840\n",
      "epoch 486 loss = 1.039625\n",
      "epoch 487 loss = 0.933063\n",
      "epoch 488 loss = 1.015690\n",
      "epoch 489 loss = 0.874751\n",
      "epoch 490 loss = 0.920057\n",
      "epoch 491 loss = 0.816997\n",
      "epoch 492 loss = 0.942255\n",
      "epoch 493 loss = 0.805237\n",
      "epoch 494 loss = 0.967818\n",
      "epoch 495 loss = 0.819905\n",
      "epoch 496 loss = 0.886456\n",
      "epoch 497 loss = 0.943785\n",
      "epoch 498 loss = 0.928302\n",
      "epoch 499 loss = 0.958915\n",
      "final loss = 0.958915\n",
      "accuracy_mc = tensor(0.4134, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4079, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.5427, device='cuda:0')\n",
      "training time = 170.08205103874207 seconds\n",
      "testing time = 1.9088749885559082 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.301667\n",
      "epoch 1 loss = 2.278883\n",
      "epoch 2 loss = 2.250849\n",
      "epoch 3 loss = 2.273470\n",
      "epoch 4 loss = 2.261309\n",
      "epoch 5 loss = 2.227102\n",
      "epoch 6 loss = 2.264131\n",
      "epoch 7 loss = 2.209337\n",
      "epoch 8 loss = 2.129460\n",
      "epoch 9 loss = 2.034239\n",
      "epoch 10 loss = 2.095828\n",
      "epoch 11 loss = 1.844051\n",
      "epoch 12 loss = 2.016514\n",
      "epoch 13 loss = 2.033643\n",
      "epoch 14 loss = 1.997126\n",
      "epoch 15 loss = 1.944443\n",
      "epoch 16 loss = 1.893730\n",
      "epoch 17 loss = 1.943074\n",
      "epoch 18 loss = 2.046136\n",
      "epoch 19 loss = 1.892395\n",
      "epoch 20 loss = 1.981398\n",
      "epoch 21 loss = 1.968439\n",
      "epoch 22 loss = 1.984064\n",
      "epoch 23 loss = 1.925199\n",
      "epoch 24 loss = 1.972366\n",
      "epoch 25 loss = 1.965468\n",
      "epoch 26 loss = 1.900717\n",
      "epoch 27 loss = 1.912789\n",
      "epoch 28 loss = 1.845851\n",
      "epoch 29 loss = 1.892964\n",
      "epoch 30 loss = 1.824415\n",
      "epoch 31 loss = 1.809222\n",
      "epoch 32 loss = 1.830297\n",
      "epoch 33 loss = 1.899359\n",
      "epoch 34 loss = 1.867855\n",
      "epoch 35 loss = 1.815206\n",
      "epoch 36 loss = 1.790639\n",
      "epoch 37 loss = 1.776490\n",
      "epoch 38 loss = 1.753784\n",
      "epoch 39 loss = 1.725526\n",
      "epoch 40 loss = 1.789226\n",
      "epoch 41 loss = 1.761629\n",
      "epoch 42 loss = 1.639137\n",
      "epoch 43 loss = 1.703577\n",
      "epoch 44 loss = 1.644147\n",
      "epoch 45 loss = 1.660839\n",
      "epoch 46 loss = 1.749166\n",
      "epoch 47 loss = 1.787493\n",
      "epoch 48 loss = 1.743027\n",
      "epoch 49 loss = 1.671674\n",
      "epoch 50 loss = 1.740537\n",
      "epoch 51 loss = 1.601413\n",
      "epoch 52 loss = 1.769573\n",
      "epoch 53 loss = 1.597821\n",
      "epoch 54 loss = 1.863519\n",
      "epoch 55 loss = 1.669124\n",
      "epoch 56 loss = 1.515830\n",
      "epoch 57 loss = 1.485122\n",
      "epoch 58 loss = 1.710788\n",
      "epoch 59 loss = 1.706542\n",
      "epoch 60 loss = 1.568325\n",
      "epoch 61 loss = 1.667311\n",
      "epoch 62 loss = 1.606314\n",
      "epoch 63 loss = 1.940511\n",
      "epoch 64 loss = 1.563829\n",
      "epoch 65 loss = 1.561183\n",
      "epoch 66 loss = 1.523125\n",
      "epoch 67 loss = 1.493028\n",
      "epoch 68 loss = 1.648579\n",
      "epoch 69 loss = 1.608411\n",
      "epoch 70 loss = 1.602158\n",
      "epoch 71 loss = 1.584019\n",
      "epoch 72 loss = 1.540396\n",
      "epoch 73 loss = 1.651788\n",
      "epoch 74 loss = 1.720127\n",
      "epoch 75 loss = 1.557699\n",
      "epoch 76 loss = 1.731174\n",
      "epoch 77 loss = 1.595299\n",
      "epoch 78 loss = 1.540046\n",
      "epoch 79 loss = 1.510541\n",
      "epoch 80 loss = 1.651182\n",
      "epoch 81 loss = 1.638347\n",
      "epoch 82 loss = 1.560070\n",
      "epoch 83 loss = 1.462912\n",
      "epoch 84 loss = 1.718821\n",
      "epoch 85 loss = 1.706569\n",
      "epoch 86 loss = 1.663932\n",
      "epoch 87 loss = 1.509730\n",
      "epoch 88 loss = 1.735485\n",
      "epoch 89 loss = 1.462041\n",
      "epoch 90 loss = 1.490653\n",
      "epoch 91 loss = 1.561219\n",
      "epoch 92 loss = 1.559837\n",
      "epoch 93 loss = 1.461067\n",
      "epoch 94 loss = 1.472000\n",
      "epoch 95 loss = 1.583524\n",
      "epoch 96 loss = 1.512358\n",
      "epoch 97 loss = 1.700438\n",
      "epoch 98 loss = 1.319068\n",
      "epoch 99 loss = 1.535668\n",
      "epoch 100 loss = 1.556875\n",
      "epoch 101 loss = 1.622413\n",
      "epoch 102 loss = 1.405596\n",
      "epoch 103 loss = 1.636712\n",
      "epoch 104 loss = 1.542673\n",
      "epoch 105 loss = 1.424313\n",
      "epoch 106 loss = 1.616958\n",
      "epoch 107 loss = 1.372055\n",
      "epoch 108 loss = 1.329491\n",
      "epoch 109 loss = 1.537041\n",
      "epoch 110 loss = 1.464440\n",
      "epoch 111 loss = 1.578654\n",
      "epoch 112 loss = 1.657928\n",
      "epoch 113 loss = 1.456270\n",
      "epoch 114 loss = 1.436990\n",
      "epoch 115 loss = 1.590020\n",
      "epoch 116 loss = 1.461928\n",
      "epoch 117 loss = 1.575308\n",
      "epoch 118 loss = 1.517353\n",
      "epoch 119 loss = 1.598376\n",
      "epoch 120 loss = 1.549859\n",
      "epoch 121 loss = 1.502347\n",
      "epoch 122 loss = 1.593373\n",
      "epoch 123 loss = 1.531442\n",
      "epoch 124 loss = 1.424186\n",
      "epoch 125 loss = 1.456978\n",
      "epoch 126 loss = 1.559834\n",
      "epoch 127 loss = 1.244279\n",
      "epoch 128 loss = 1.439642\n",
      "epoch 129 loss = 1.398653\n",
      "epoch 130 loss = 1.437479\n",
      "epoch 131 loss = 1.377768\n",
      "epoch 132 loss = 1.313587\n",
      "epoch 133 loss = 1.372575\n",
      "epoch 134 loss = 1.457223\n",
      "epoch 135 loss = 1.564765\n",
      "epoch 136 loss = 1.307231\n",
      "epoch 137 loss = 1.616662\n",
      "epoch 138 loss = 1.442597\n",
      "epoch 139 loss = 1.608760\n",
      "epoch 140 loss = 1.308743\n",
      "epoch 141 loss = 1.531286\n",
      "epoch 142 loss = 1.508743\n",
      "epoch 143 loss = 1.324179\n",
      "epoch 144 loss = 1.544313\n",
      "epoch 145 loss = 1.576165\n",
      "epoch 146 loss = 1.334661\n",
      "epoch 147 loss = 1.492369\n",
      "epoch 148 loss = 1.423670\n",
      "epoch 149 loss = 1.479377\n",
      "epoch 150 loss = 1.298864\n",
      "epoch 151 loss = 1.426534\n",
      "epoch 152 loss = 1.446174\n",
      "epoch 153 loss = 1.471368\n",
      "epoch 154 loss = 1.596977\n",
      "epoch 155 loss = 1.233056\n",
      "epoch 156 loss = 1.302463\n",
      "epoch 157 loss = 1.325957\n",
      "epoch 158 loss = 1.260705\n",
      "epoch 159 loss = 1.360788\n",
      "epoch 160 loss = 1.340969\n",
      "epoch 161 loss = 1.428460\n",
      "epoch 162 loss = 1.498697\n",
      "epoch 163 loss = 1.385558\n",
      "epoch 164 loss = 1.407553\n",
      "epoch 165 loss = 1.490750\n",
      "epoch 166 loss = 1.380975\n",
      "epoch 167 loss = 1.351919\n",
      "epoch 168 loss = 1.625389\n",
      "epoch 169 loss = 1.238509\n",
      "epoch 170 loss = 1.381333\n",
      "epoch 171 loss = 1.300583\n",
      "epoch 172 loss = 1.447536\n",
      "epoch 173 loss = 1.529067\n",
      "epoch 174 loss = 1.321384\n",
      "epoch 175 loss = 1.277475\n",
      "epoch 176 loss = 1.495986\n",
      "epoch 177 loss = 1.347249\n",
      "epoch 178 loss = 1.393358\n",
      "epoch 179 loss = 1.563078\n",
      "epoch 180 loss = 1.547175\n",
      "epoch 181 loss = 1.368885\n",
      "epoch 182 loss = 1.468488\n",
      "epoch 183 loss = 1.342264\n",
      "epoch 184 loss = 1.369631\n",
      "epoch 185 loss = 1.268590\n",
      "epoch 186 loss = 1.464306\n",
      "epoch 187 loss = 1.338888\n",
      "epoch 188 loss = 1.305755\n",
      "epoch 189 loss = 1.373406\n",
      "epoch 190 loss = 1.388234\n",
      "epoch 191 loss = 1.256487\n",
      "epoch 192 loss = 1.401039\n",
      "epoch 193 loss = 1.380143\n",
      "epoch 194 loss = 1.460066\n",
      "epoch 195 loss = 1.316167\n",
      "epoch 196 loss = 1.368126\n",
      "epoch 197 loss = 1.532452\n",
      "epoch 198 loss = 1.411374\n",
      "epoch 199 loss = 1.406526\n",
      "epoch 200 loss = 1.277757\n",
      "epoch 201 loss = 1.245013\n",
      "epoch 202 loss = 1.306733\n",
      "epoch 203 loss = 1.229935\n",
      "epoch 204 loss = 1.425008\n",
      "epoch 205 loss = 1.583499\n",
      "epoch 206 loss = 1.428987\n",
      "epoch 207 loss = 1.213142\n",
      "epoch 208 loss = 1.370434\n",
      "epoch 209 loss = 1.430757\n",
      "epoch 210 loss = 1.291899\n",
      "epoch 211 loss = 1.393429\n",
      "epoch 212 loss = 1.275936\n",
      "epoch 213 loss = 1.403093\n",
      "epoch 214 loss = 1.152394\n",
      "epoch 215 loss = 1.218968\n",
      "epoch 216 loss = 1.229327\n",
      "epoch 217 loss = 1.411931\n",
      "epoch 218 loss = 1.189219\n",
      "epoch 219 loss = 1.291148\n",
      "epoch 220 loss = 1.507362\n",
      "epoch 221 loss = 1.376916\n",
      "epoch 222 loss = 1.432068\n",
      "epoch 223 loss = 1.432745\n",
      "epoch 224 loss = 1.580718\n",
      "epoch 225 loss = 1.200566\n",
      "epoch 226 loss = 1.171808\n",
      "epoch 227 loss = 1.298345\n",
      "epoch 228 loss = 1.183206\n",
      "epoch 229 loss = 1.101912\n",
      "epoch 230 loss = 1.214227\n",
      "epoch 231 loss = 1.404233\n",
      "epoch 232 loss = 1.214626\n",
      "epoch 233 loss = 1.177177\n",
      "epoch 234 loss = 1.215370\n",
      "epoch 235 loss = 1.297949\n",
      "epoch 236 loss = 1.263954\n",
      "epoch 237 loss = 1.276388\n",
      "epoch 238 loss = 1.140393\n",
      "epoch 239 loss = 1.276957\n",
      "epoch 240 loss = 1.294009\n",
      "epoch 241 loss = 1.346815\n",
      "epoch 242 loss = 1.465990\n",
      "epoch 243 loss = 1.196884\n",
      "epoch 244 loss = 1.285127\n",
      "epoch 245 loss = 1.365022\n",
      "epoch 246 loss = 1.355547\n",
      "epoch 247 loss = 1.215834\n",
      "epoch 248 loss = 1.252761\n",
      "epoch 249 loss = 1.292796\n",
      "epoch 250 loss = 0.998453\n",
      "epoch 251 loss = 1.162272\n",
      "epoch 252 loss = 1.110552\n",
      "epoch 253 loss = 1.253201\n",
      "epoch 254 loss = 1.327111\n",
      "epoch 255 loss = 1.177602\n",
      "epoch 256 loss = 1.268970\n",
      "epoch 257 loss = 1.197060\n",
      "epoch 258 loss = 1.210757\n",
      "epoch 259 loss = 1.105137\n",
      "epoch 260 loss = 1.326988\n",
      "epoch 261 loss = 1.415649\n",
      "epoch 262 loss = 1.102503\n",
      "epoch 263 loss = 1.263275\n",
      "epoch 264 loss = 1.276452\n",
      "epoch 265 loss = 1.187060\n",
      "epoch 266 loss = 1.429039\n",
      "epoch 267 loss = 1.385733\n",
      "epoch 268 loss = 1.182805\n",
      "epoch 269 loss = 1.188150\n",
      "epoch 270 loss = 1.450298\n",
      "epoch 271 loss = 1.177170\n",
      "epoch 272 loss = 1.175764\n",
      "epoch 273 loss = 1.321010\n",
      "epoch 274 loss = 1.385300\n",
      "epoch 275 loss = 1.188957\n",
      "epoch 276 loss = 1.372273\n",
      "epoch 277 loss = 1.180741\n",
      "epoch 278 loss = 1.009959\n",
      "epoch 279 loss = 1.058699\n",
      "epoch 280 loss = 1.217626\n",
      "epoch 281 loss = 1.158683\n",
      "epoch 282 loss = 1.282891\n",
      "epoch 283 loss = 1.391121\n",
      "epoch 284 loss = 1.192431\n",
      "epoch 285 loss = 1.286429\n",
      "epoch 286 loss = 1.246956\n",
      "epoch 287 loss = 1.135596\n",
      "epoch 288 loss = 1.107167\n",
      "epoch 289 loss = 1.167177\n",
      "epoch 290 loss = 1.249933\n",
      "epoch 291 loss = 1.143551\n",
      "epoch 292 loss = 1.236206\n",
      "epoch 293 loss = 1.199067\n",
      "epoch 294 loss = 1.203929\n",
      "epoch 295 loss = 1.358773\n",
      "epoch 296 loss = 1.064872\n",
      "epoch 297 loss = 1.299591\n",
      "epoch 298 loss = 1.238126\n",
      "epoch 299 loss = 1.201845\n",
      "epoch 300 loss = 1.160816\n",
      "epoch 301 loss = 1.106707\n",
      "epoch 302 loss = 1.247537\n",
      "epoch 303 loss = 1.395260\n",
      "epoch 304 loss = 1.317648\n",
      "epoch 305 loss = 1.218081\n",
      "epoch 306 loss = 1.189805\n",
      "epoch 307 loss = 1.217943\n",
      "epoch 308 loss = 1.478081\n",
      "epoch 309 loss = 1.205485\n",
      "epoch 310 loss = 1.240138\n",
      "epoch 311 loss = 1.290749\n",
      "epoch 312 loss = 1.297755\n",
      "epoch 313 loss = 1.260341\n",
      "epoch 314 loss = 1.091101\n",
      "epoch 315 loss = 1.143828\n",
      "epoch 316 loss = 1.190888\n",
      "epoch 317 loss = 1.033111\n",
      "epoch 318 loss = 1.102986\n",
      "epoch 319 loss = 1.066357\n",
      "epoch 320 loss = 1.160883\n",
      "epoch 321 loss = 1.138542\n",
      "epoch 322 loss = 1.307971\n",
      "epoch 323 loss = 1.209422\n",
      "epoch 324 loss = 1.270586\n",
      "epoch 325 loss = 1.343927\n",
      "epoch 326 loss = 1.187143\n",
      "epoch 327 loss = 1.292607\n",
      "epoch 328 loss = 1.219694\n",
      "epoch 329 loss = 1.110524\n",
      "epoch 330 loss = 1.039637\n",
      "epoch 331 loss = 1.146313\n",
      "epoch 332 loss = 1.194832\n",
      "epoch 333 loss = 1.111625\n",
      "epoch 334 loss = 1.199951\n",
      "epoch 335 loss = 1.171398\n",
      "epoch 336 loss = 1.272561\n",
      "epoch 337 loss = 1.077275\n",
      "epoch 338 loss = 1.201925\n",
      "epoch 339 loss = 1.176413\n",
      "epoch 340 loss = 1.449291\n",
      "epoch 341 loss = 1.343122\n",
      "epoch 342 loss = 1.112531\n",
      "epoch 343 loss = 1.327743\n",
      "epoch 344 loss = 1.178681\n",
      "epoch 345 loss = 1.117660\n",
      "epoch 346 loss = 1.437663\n",
      "epoch 347 loss = 1.076273\n",
      "epoch 348 loss = 1.170850\n",
      "epoch 349 loss = 1.191631\n",
      "epoch 350 loss = 1.232553\n",
      "epoch 351 loss = 1.096834\n",
      "epoch 352 loss = 1.016786\n",
      "epoch 353 loss = 1.022512\n",
      "epoch 354 loss = 1.024939\n",
      "epoch 355 loss = 1.000132\n",
      "epoch 356 loss = 1.186693\n",
      "epoch 357 loss = 1.062728\n",
      "epoch 358 loss = 1.317951\n",
      "epoch 359 loss = 1.101393\n",
      "epoch 360 loss = 1.007888\n",
      "epoch 361 loss = 1.184013\n",
      "epoch 362 loss = 1.097129\n",
      "epoch 363 loss = 1.117520\n",
      "epoch 364 loss = 1.208697\n",
      "epoch 365 loss = 1.037486\n",
      "epoch 366 loss = 1.125049\n",
      "epoch 367 loss = 1.139515\n",
      "epoch 368 loss = 1.267777\n",
      "epoch 369 loss = 1.246055\n",
      "epoch 370 loss = 1.254368\n",
      "epoch 371 loss = 1.060317\n",
      "epoch 372 loss = 1.076154\n",
      "epoch 373 loss = 1.049461\n",
      "epoch 374 loss = 1.024236\n",
      "epoch 375 loss = 1.149304\n",
      "epoch 376 loss = 1.079188\n",
      "epoch 377 loss = 1.090813\n",
      "epoch 378 loss = 1.341366\n",
      "epoch 379 loss = 1.301691\n",
      "epoch 380 loss = 1.093258\n",
      "epoch 381 loss = 1.105617\n",
      "epoch 382 loss = 1.118738\n",
      "epoch 383 loss = 1.232931\n",
      "epoch 384 loss = 1.142929\n",
      "epoch 385 loss = 1.242666\n",
      "epoch 386 loss = 1.203959\n",
      "epoch 387 loss = 1.276110\n",
      "epoch 388 loss = 1.303866\n",
      "epoch 389 loss = 1.240427\n",
      "epoch 390 loss = 1.139439\n",
      "epoch 391 loss = 1.162817\n",
      "epoch 392 loss = 0.991581\n",
      "epoch 393 loss = 1.159388\n",
      "epoch 394 loss = 1.147689\n",
      "epoch 395 loss = 1.081961\n",
      "epoch 396 loss = 1.049517\n",
      "epoch 397 loss = 1.086827\n",
      "epoch 398 loss = 1.224788\n",
      "epoch 399 loss = 1.168856\n",
      "epoch 400 loss = 0.951098\n",
      "epoch 401 loss = 1.059333\n",
      "epoch 402 loss = 1.180442\n",
      "epoch 403 loss = 1.108204\n",
      "epoch 404 loss = 1.288877\n",
      "epoch 405 loss = 1.308575\n",
      "epoch 406 loss = 1.191539\n",
      "epoch 407 loss = 1.309232\n",
      "epoch 408 loss = 1.213428\n",
      "epoch 409 loss = 1.269761\n",
      "epoch 410 loss = 1.176980\n",
      "epoch 411 loss = 1.413179\n",
      "epoch 412 loss = 1.028464\n",
      "epoch 413 loss = 1.020579\n",
      "epoch 414 loss = 1.054284\n",
      "epoch 415 loss = 1.179637\n",
      "epoch 416 loss = 1.141281\n",
      "epoch 417 loss = 1.337427\n",
      "epoch 418 loss = 0.922643\n",
      "epoch 419 loss = 1.172334\n",
      "epoch 420 loss = 0.954630\n",
      "epoch 421 loss = 1.062024\n",
      "epoch 422 loss = 0.946729\n",
      "epoch 423 loss = 1.100567\n",
      "epoch 424 loss = 1.009809\n",
      "epoch 425 loss = 1.157385\n",
      "epoch 426 loss = 1.091196\n",
      "epoch 427 loss = 1.288898\n",
      "epoch 428 loss = 1.023539\n",
      "epoch 429 loss = 1.064711\n",
      "epoch 430 loss = 1.232995\n",
      "epoch 431 loss = 1.296518\n",
      "epoch 432 loss = 0.932699\n",
      "epoch 433 loss = 0.857809\n",
      "epoch 434 loss = 1.277084\n",
      "epoch 435 loss = 1.117699\n",
      "epoch 436 loss = 1.239039\n",
      "epoch 437 loss = 1.030947\n",
      "epoch 438 loss = 1.064994\n",
      "epoch 439 loss = 1.163384\n",
      "epoch 440 loss = 1.019329\n",
      "epoch 441 loss = 1.035409\n",
      "epoch 442 loss = 1.098909\n",
      "epoch 443 loss = 0.980750\n",
      "epoch 444 loss = 1.036784\n",
      "epoch 445 loss = 0.996126\n",
      "epoch 446 loss = 1.130550\n",
      "epoch 447 loss = 1.168431\n",
      "epoch 448 loss = 1.069222\n",
      "epoch 449 loss = 1.077901\n",
      "epoch 450 loss = 1.053547\n",
      "epoch 451 loss = 1.016364\n",
      "epoch 452 loss = 1.067910\n",
      "epoch 453 loss = 1.079689\n",
      "epoch 454 loss = 1.142989\n",
      "epoch 455 loss = 1.195561\n",
      "epoch 456 loss = 0.995365\n",
      "epoch 457 loss = 1.311119\n",
      "epoch 458 loss = 1.052935\n",
      "epoch 459 loss = 0.880606\n",
      "epoch 460 loss = 0.859451\n",
      "epoch 461 loss = 1.041177\n",
      "epoch 462 loss = 0.895516\n",
      "epoch 463 loss = 1.107912\n",
      "epoch 464 loss = 1.090833\n",
      "epoch 465 loss = 1.080791\n",
      "epoch 466 loss = 1.227249\n",
      "epoch 467 loss = 1.097630\n",
      "epoch 468 loss = 0.966973\n",
      "epoch 469 loss = 1.012820\n",
      "epoch 470 loss = 1.017168\n",
      "epoch 471 loss = 1.183868\n",
      "epoch 472 loss = 1.027689\n",
      "epoch 473 loss = 1.257630\n",
      "epoch 474 loss = 1.124119\n",
      "epoch 475 loss = 1.359352\n",
      "epoch 476 loss = 1.045714\n",
      "epoch 477 loss = 0.973537\n",
      "epoch 478 loss = 1.082412\n",
      "epoch 479 loss = 1.197278\n",
      "epoch 480 loss = 0.993123\n",
      "epoch 481 loss = 1.180672\n",
      "epoch 482 loss = 1.152337\n",
      "epoch 483 loss = 1.115764\n",
      "epoch 484 loss = 1.081345\n",
      "epoch 485 loss = 1.046266\n",
      "epoch 486 loss = 1.276030\n",
      "epoch 487 loss = 0.966823\n",
      "epoch 488 loss = 1.089965\n",
      "epoch 489 loss = 1.112989\n",
      "epoch 490 loss = 1.071119\n",
      "epoch 491 loss = 1.023579\n",
      "epoch 492 loss = 1.149696\n",
      "epoch 493 loss = 0.984405\n",
      "epoch 494 loss = 1.043955\n",
      "epoch 495 loss = 1.144942\n",
      "epoch 496 loss = 0.893925\n",
      "epoch 497 loss = 1.025665\n",
      "epoch 498 loss = 1.020591\n",
      "epoch 499 loss = 0.826331\n",
      "final loss = 0.826331\n",
      "accuracy_mc = tensor(0.4414, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4394, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6826, device='cuda:0')\n",
      "training time = 170.7882640361786 seconds\n",
      "testing time = 1.8876574039459229 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.283906\n",
      "epoch 1 loss = 2.246012\n",
      "epoch 2 loss = 2.223986\n",
      "epoch 3 loss = 2.218110\n",
      "epoch 4 loss = 2.235505\n",
      "epoch 5 loss = 2.271013\n",
      "epoch 6 loss = 2.322171\n",
      "epoch 7 loss = 2.403265\n",
      "epoch 8 loss = 2.241211\n",
      "epoch 9 loss = 2.290400\n",
      "epoch 10 loss = 2.210385\n",
      "epoch 11 loss = 2.417046\n",
      "epoch 12 loss = 2.290601\n",
      "epoch 13 loss = 2.352185\n",
      "epoch 14 loss = 2.074485\n",
      "epoch 15 loss = 2.064166\n",
      "epoch 16 loss = 2.325787\n",
      "epoch 17 loss = 2.106682\n",
      "epoch 18 loss = 2.239341\n",
      "epoch 19 loss = 2.112320\n",
      "epoch 20 loss = 2.106918\n",
      "epoch 21 loss = 2.209175\n",
      "epoch 22 loss = 1.963124\n",
      "epoch 23 loss = 2.237828\n",
      "epoch 24 loss = 2.231907\n",
      "epoch 25 loss = 2.116524\n",
      "epoch 26 loss = 2.167292\n",
      "epoch 27 loss = 1.924445\n",
      "epoch 28 loss = 1.996151\n",
      "epoch 29 loss = 2.176310\n",
      "epoch 30 loss = 2.089075\n",
      "epoch 31 loss = 1.992351\n",
      "epoch 32 loss = 2.168079\n",
      "epoch 33 loss = 2.028053\n",
      "epoch 34 loss = 2.032890\n",
      "epoch 35 loss = 2.074322\n",
      "epoch 36 loss = 2.028443\n",
      "epoch 37 loss = 2.107731\n",
      "epoch 38 loss = 2.044036\n",
      "epoch 39 loss = 2.008788\n",
      "epoch 40 loss = 2.031972\n",
      "epoch 41 loss = 1.868708\n",
      "epoch 42 loss = 2.038059\n",
      "epoch 43 loss = 1.911290\n",
      "epoch 44 loss = 1.969822\n",
      "epoch 45 loss = 2.035340\n",
      "epoch 46 loss = 1.794991\n",
      "epoch 47 loss = 2.039173\n",
      "epoch 48 loss = 1.990487\n",
      "epoch 49 loss = 2.043096\n",
      "epoch 50 loss = 1.832789\n",
      "epoch 51 loss = 1.874317\n",
      "epoch 52 loss = 1.849186\n",
      "epoch 53 loss = 2.267366\n",
      "epoch 54 loss = 1.856273\n",
      "epoch 55 loss = 1.786934\n",
      "epoch 56 loss = 1.818810\n",
      "epoch 57 loss = 1.987467\n",
      "epoch 58 loss = 1.829790\n",
      "epoch 59 loss = 1.853504\n",
      "epoch 60 loss = 1.978124\n",
      "epoch 61 loss = 1.865561\n",
      "epoch 62 loss = 1.986011\n",
      "epoch 63 loss = 1.839867\n",
      "epoch 64 loss = 1.698092\n",
      "epoch 65 loss = 1.974617\n",
      "epoch 66 loss = 1.749281\n",
      "epoch 67 loss = 1.879773\n",
      "epoch 68 loss = 1.647715\n",
      "epoch 69 loss = 1.605959\n",
      "epoch 70 loss = 1.732127\n",
      "epoch 71 loss = 1.864843\n",
      "epoch 72 loss = 1.849228\n",
      "epoch 73 loss = 1.766058\n",
      "epoch 74 loss = 1.898714\n",
      "epoch 75 loss = 1.643714\n",
      "epoch 76 loss = 1.693416\n",
      "epoch 77 loss = 1.711162\n",
      "epoch 78 loss = 1.930823\n",
      "epoch 79 loss = 1.736063\n",
      "epoch 80 loss = 1.551003\n",
      "epoch 81 loss = 1.913564\n",
      "epoch 82 loss = 1.754148\n",
      "epoch 83 loss = 1.559420\n",
      "epoch 84 loss = 1.678551\n",
      "epoch 85 loss = 1.930419\n",
      "epoch 86 loss = 1.776294\n",
      "epoch 87 loss = 1.639244\n",
      "epoch 88 loss = 1.810495\n",
      "epoch 89 loss = 1.797150\n",
      "epoch 90 loss = 1.780236\n",
      "epoch 91 loss = 1.684812\n",
      "epoch 92 loss = 1.697251\n",
      "epoch 93 loss = 1.642626\n",
      "epoch 94 loss = 1.765324\n",
      "epoch 95 loss = 1.842606\n",
      "epoch 96 loss = 1.600545\n",
      "epoch 97 loss = 1.666860\n",
      "epoch 98 loss = 1.832287\n",
      "epoch 99 loss = 1.770017\n",
      "epoch 100 loss = 1.723151\n",
      "epoch 101 loss = 1.597194\n",
      "epoch 102 loss = 1.702705\n",
      "epoch 103 loss = 1.681374\n",
      "epoch 104 loss = 1.571528\n",
      "epoch 105 loss = 1.768091\n",
      "epoch 106 loss = 1.800748\n",
      "epoch 107 loss = 1.628290\n",
      "epoch 108 loss = 1.512545\n",
      "epoch 109 loss = 1.380710\n",
      "epoch 110 loss = 1.622178\n",
      "epoch 111 loss = 1.788928\n",
      "epoch 112 loss = 1.548809\n",
      "epoch 113 loss = 1.632603\n",
      "epoch 114 loss = 1.699596\n",
      "epoch 115 loss = 1.554162\n",
      "epoch 116 loss = 1.603976\n",
      "epoch 117 loss = 1.600376\n",
      "epoch 118 loss = 1.573386\n",
      "epoch 119 loss = 1.546790\n",
      "epoch 120 loss = 1.547672\n",
      "epoch 121 loss = 1.415650\n",
      "epoch 122 loss = 1.561810\n",
      "epoch 123 loss = 1.590574\n",
      "epoch 124 loss = 1.886840\n",
      "epoch 125 loss = 1.554018\n",
      "epoch 126 loss = 1.632357\n",
      "epoch 127 loss = 1.521327\n",
      "epoch 128 loss = 1.430452\n",
      "epoch 129 loss = 1.517815\n",
      "epoch 130 loss = 1.723800\n",
      "epoch 131 loss = 1.841380\n",
      "epoch 132 loss = 1.660451\n",
      "epoch 133 loss = 1.413164\n",
      "epoch 134 loss = 1.606979\n",
      "epoch 135 loss = 1.601668\n",
      "epoch 136 loss = 1.641869\n",
      "epoch 137 loss = 1.551022\n",
      "epoch 138 loss = 1.509835\n",
      "epoch 139 loss = 1.511934\n",
      "epoch 140 loss = 1.535880\n",
      "epoch 141 loss = 1.273780\n",
      "epoch 142 loss = 1.464153\n",
      "epoch 143 loss = 1.548208\n",
      "epoch 144 loss = 1.329940\n",
      "epoch 145 loss = 1.607407\n",
      "epoch 146 loss = 1.440314\n",
      "epoch 147 loss = 1.467262\n",
      "epoch 148 loss = 1.618163\n",
      "epoch 149 loss = 1.594322\n",
      "epoch 150 loss = 1.510241\n",
      "epoch 151 loss = 1.590199\n",
      "epoch 152 loss = 1.463792\n",
      "epoch 153 loss = 1.417649\n",
      "epoch 154 loss = 1.527489\n",
      "epoch 155 loss = 1.588497\n",
      "epoch 156 loss = 1.382231\n",
      "epoch 157 loss = 1.396421\n",
      "epoch 158 loss = 1.557856\n",
      "epoch 159 loss = 1.628883\n",
      "epoch 160 loss = 1.469260\n",
      "epoch 161 loss = 1.572154\n",
      "epoch 162 loss = 1.557347\n",
      "epoch 163 loss = 1.499102\n",
      "epoch 164 loss = 1.537911\n",
      "epoch 165 loss = 1.404461\n",
      "epoch 166 loss = 1.461300\n",
      "epoch 167 loss = 1.287803\n",
      "epoch 168 loss = 1.436845\n",
      "epoch 169 loss = 1.391008\n",
      "epoch 170 loss = 1.662731\n",
      "epoch 171 loss = 1.581656\n",
      "epoch 172 loss = 1.582401\n",
      "epoch 173 loss = 1.370260\n",
      "epoch 174 loss = 1.291228\n",
      "epoch 175 loss = 1.391264\n",
      "epoch 176 loss = 1.410730\n",
      "epoch 177 loss = 1.367382\n",
      "epoch 178 loss = 1.393149\n",
      "epoch 179 loss = 1.478206\n",
      "epoch 180 loss = 1.379429\n",
      "epoch 181 loss = 1.626780\n",
      "epoch 182 loss = 1.346110\n",
      "epoch 183 loss = 1.499782\n",
      "epoch 184 loss = 1.413910\n",
      "epoch 185 loss = 1.573779\n",
      "epoch 186 loss = 1.376945\n",
      "epoch 187 loss = 1.405474\n",
      "epoch 188 loss = 1.435360\n",
      "epoch 189 loss = 1.436744\n",
      "epoch 190 loss = 1.470935\n",
      "epoch 191 loss = 1.337755\n",
      "epoch 192 loss = 1.465954\n",
      "epoch 193 loss = 1.490416\n",
      "epoch 194 loss = 1.506766\n",
      "epoch 195 loss = 1.357544\n",
      "epoch 196 loss = 1.456962\n",
      "epoch 197 loss = 1.327365\n",
      "epoch 198 loss = 1.658118\n",
      "epoch 199 loss = 1.483731\n",
      "epoch 200 loss = 1.393976\n",
      "epoch 201 loss = 1.571311\n",
      "epoch 202 loss = 1.373287\n",
      "epoch 203 loss = 1.125511\n",
      "epoch 204 loss = 1.603814\n",
      "epoch 205 loss = 1.400797\n",
      "epoch 206 loss = 1.521279\n",
      "epoch 207 loss = 1.515263\n",
      "epoch 208 loss = 1.616635\n",
      "epoch 209 loss = 1.472749\n",
      "epoch 210 loss = 1.488832\n",
      "epoch 211 loss = 1.450351\n",
      "epoch 212 loss = 1.441849\n",
      "epoch 213 loss = 1.614231\n",
      "epoch 214 loss = 1.340777\n",
      "epoch 215 loss = 1.759482\n",
      "epoch 216 loss = 1.384540\n",
      "epoch 217 loss = 1.604634\n",
      "epoch 218 loss = 1.437293\n",
      "epoch 219 loss = 1.580885\n",
      "epoch 220 loss = 1.460338\n",
      "epoch 221 loss = 1.628089\n",
      "epoch 222 loss = 1.379683\n",
      "epoch 223 loss = 1.631180\n",
      "epoch 224 loss = 1.358166\n",
      "epoch 225 loss = 1.323682\n",
      "epoch 226 loss = 1.463259\n",
      "epoch 227 loss = 1.359000\n",
      "epoch 228 loss = 1.631290\n",
      "epoch 229 loss = 1.364070\n",
      "epoch 230 loss = 1.478813\n",
      "epoch 231 loss = 1.605548\n",
      "epoch 232 loss = 1.337736\n",
      "epoch 233 loss = 1.326290\n",
      "epoch 234 loss = 1.397891\n",
      "epoch 235 loss = 1.302978\n",
      "epoch 236 loss = 1.753152\n",
      "epoch 237 loss = 1.496171\n",
      "epoch 238 loss = 1.373319\n",
      "epoch 239 loss = 1.370556\n",
      "epoch 240 loss = 1.352486\n",
      "epoch 241 loss = 1.416472\n",
      "epoch 242 loss = 1.369602\n",
      "epoch 243 loss = 1.377984\n",
      "epoch 244 loss = 1.381083\n",
      "epoch 245 loss = 1.534350\n",
      "epoch 246 loss = 1.620841\n",
      "epoch 247 loss = 1.257144\n",
      "epoch 248 loss = 1.298063\n",
      "epoch 249 loss = 1.565487\n",
      "epoch 250 loss = 1.301969\n",
      "epoch 251 loss = 1.397689\n",
      "epoch 252 loss = 1.296616\n",
      "epoch 253 loss = 1.500252\n",
      "epoch 254 loss = 1.670341\n",
      "epoch 255 loss = 1.504597\n",
      "epoch 256 loss = 1.472159\n",
      "epoch 257 loss = 1.564991\n",
      "epoch 258 loss = 1.438947\n",
      "epoch 259 loss = 1.573692\n",
      "epoch 260 loss = 1.675314\n",
      "epoch 261 loss = 1.445871\n",
      "epoch 262 loss = 1.409652\n",
      "epoch 263 loss = 1.369539\n",
      "epoch 264 loss = 1.350057\n",
      "epoch 265 loss = 1.361877\n",
      "epoch 266 loss = 1.280908\n",
      "epoch 267 loss = 1.526893\n",
      "epoch 268 loss = 1.451213\n",
      "epoch 269 loss = 1.316331\n",
      "epoch 270 loss = 1.417830\n",
      "epoch 271 loss = 1.276345\n",
      "epoch 272 loss = 1.338358\n",
      "epoch 273 loss = 1.401576\n",
      "epoch 274 loss = 1.661735\n",
      "epoch 275 loss = 1.303113\n",
      "epoch 276 loss = 1.459863\n",
      "epoch 277 loss = 1.339824\n",
      "epoch 278 loss = 1.328476\n",
      "epoch 279 loss = 1.269722\n",
      "epoch 280 loss = 1.429530\n",
      "epoch 281 loss = 1.428975\n",
      "epoch 282 loss = 1.599092\n",
      "epoch 283 loss = 1.407201\n",
      "epoch 284 loss = 1.330976\n",
      "epoch 285 loss = 1.423418\n",
      "epoch 286 loss = 1.519137\n",
      "epoch 287 loss = 1.386556\n",
      "epoch 288 loss = 1.331120\n",
      "epoch 289 loss = 1.300721\n",
      "epoch 290 loss = 1.364339\n",
      "epoch 291 loss = 1.340757\n",
      "epoch 292 loss = 1.366168\n",
      "epoch 293 loss = 1.466356\n",
      "epoch 294 loss = 1.339015\n",
      "epoch 295 loss = 1.401005\n",
      "epoch 296 loss = 1.380440\n",
      "epoch 297 loss = 1.638026\n",
      "epoch 298 loss = 1.478409\n",
      "epoch 299 loss = 1.268362\n",
      "epoch 300 loss = 1.512020\n",
      "epoch 301 loss = 1.298761\n",
      "epoch 302 loss = 1.359585\n",
      "epoch 303 loss = 1.345328\n",
      "epoch 304 loss = 1.298404\n",
      "epoch 305 loss = 1.274624\n",
      "epoch 306 loss = 1.210141\n",
      "epoch 307 loss = 1.361715\n",
      "epoch 308 loss = 1.689387\n",
      "epoch 309 loss = 1.488709\n",
      "epoch 310 loss = 1.477303\n",
      "epoch 311 loss = 1.456681\n",
      "epoch 312 loss = 1.422369\n",
      "epoch 313 loss = 1.274791\n",
      "epoch 314 loss = 1.316430\n",
      "epoch 315 loss = 1.243042\n",
      "epoch 316 loss = 1.506119\n",
      "epoch 317 loss = 1.426150\n",
      "epoch 318 loss = 1.424965\n",
      "epoch 319 loss = 1.276903\n",
      "epoch 320 loss = 1.595198\n",
      "epoch 321 loss = 1.231133\n",
      "epoch 322 loss = 1.179197\n",
      "epoch 323 loss = 1.392898\n",
      "epoch 324 loss = 1.392179\n",
      "epoch 325 loss = 1.613111\n",
      "epoch 326 loss = 1.433312\n",
      "epoch 327 loss = 1.397388\n",
      "epoch 328 loss = 1.341993\n",
      "epoch 329 loss = 1.417426\n",
      "epoch 330 loss = 1.364731\n",
      "epoch 331 loss = 1.517092\n",
      "epoch 332 loss = 1.439215\n",
      "epoch 333 loss = 1.357898\n",
      "epoch 334 loss = 1.280467\n",
      "epoch 335 loss = 1.476852\n",
      "epoch 336 loss = 1.517656\n",
      "epoch 337 loss = 1.377727\n",
      "epoch 338 loss = 1.473298\n",
      "epoch 339 loss = 1.431779\n",
      "epoch 340 loss = 1.384077\n",
      "epoch 341 loss = 1.226071\n",
      "epoch 342 loss = 1.334538\n",
      "epoch 343 loss = 1.364639\n",
      "epoch 344 loss = 1.242305\n",
      "epoch 345 loss = 1.282411\n",
      "epoch 346 loss = 1.269701\n",
      "epoch 347 loss = 1.432870\n",
      "epoch 348 loss = 1.319530\n",
      "epoch 349 loss = 1.360519\n",
      "epoch 350 loss = 1.182600\n",
      "epoch 351 loss = 1.294778\n",
      "epoch 352 loss = 1.275083\n",
      "epoch 353 loss = 1.258441\n",
      "epoch 354 loss = 1.355889\n",
      "epoch 355 loss = 1.288847\n",
      "epoch 356 loss = 1.293042\n",
      "epoch 357 loss = 1.170720\n",
      "epoch 358 loss = 1.276963\n",
      "epoch 359 loss = 1.361082\n",
      "epoch 360 loss = 1.452021\n",
      "epoch 361 loss = 1.561994\n",
      "epoch 362 loss = 1.375410\n",
      "epoch 363 loss = 1.329569\n",
      "epoch 364 loss = 1.230574\n",
      "epoch 365 loss = 1.388280\n",
      "epoch 366 loss = 1.503442\n",
      "epoch 367 loss = 1.226771\n",
      "epoch 368 loss = 1.349111\n",
      "epoch 369 loss = 1.455216\n",
      "epoch 370 loss = 1.309809\n",
      "epoch 371 loss = 1.577380\n",
      "epoch 372 loss = 1.310364\n",
      "epoch 373 loss = 1.321250\n",
      "epoch 374 loss = 1.354516\n",
      "epoch 375 loss = 1.187690\n",
      "epoch 376 loss = 1.436272\n",
      "epoch 377 loss = 1.337726\n",
      "epoch 378 loss = 1.156432\n",
      "epoch 379 loss = 1.286946\n",
      "epoch 380 loss = 1.432624\n",
      "epoch 381 loss = 1.407204\n",
      "epoch 382 loss = 1.616328\n",
      "epoch 383 loss = 1.197260\n",
      "epoch 384 loss = 1.195856\n",
      "epoch 385 loss = 1.490450\n",
      "epoch 386 loss = 1.511438\n",
      "epoch 387 loss = 1.387751\n",
      "epoch 388 loss = 1.453531\n",
      "epoch 389 loss = 1.354992\n",
      "epoch 390 loss = 1.255970\n",
      "epoch 391 loss = 1.115975\n",
      "epoch 392 loss = 1.614961\n",
      "epoch 393 loss = 1.223076\n",
      "epoch 394 loss = 1.336886\n",
      "epoch 395 loss = 1.247118\n",
      "epoch 396 loss = 1.215109\n",
      "epoch 397 loss = 1.239843\n",
      "epoch 398 loss = 1.274353\n",
      "epoch 399 loss = 1.305894\n",
      "epoch 400 loss = 1.123187\n",
      "epoch 401 loss = 1.365078\n",
      "epoch 402 loss = 1.200298\n",
      "epoch 403 loss = 1.428819\n",
      "epoch 404 loss = 1.375577\n",
      "epoch 405 loss = 1.311328\n",
      "epoch 406 loss = 1.404589\n",
      "epoch 407 loss = 1.174290\n",
      "epoch 408 loss = 1.245784\n",
      "epoch 409 loss = 1.359369\n",
      "epoch 410 loss = 1.228198\n",
      "epoch 411 loss = 1.574177\n",
      "epoch 412 loss = 1.298272\n",
      "epoch 413 loss = 1.327891\n",
      "epoch 414 loss = 1.188854\n",
      "epoch 415 loss = 1.268505\n",
      "epoch 416 loss = 1.410985\n",
      "epoch 417 loss = 1.220268\n",
      "epoch 418 loss = 1.181629\n",
      "epoch 419 loss = 1.214071\n",
      "epoch 420 loss = 1.367065\n",
      "epoch 421 loss = 1.268917\n",
      "epoch 422 loss = 1.393209\n",
      "epoch 423 loss = 1.362390\n",
      "epoch 424 loss = 1.216741\n",
      "epoch 425 loss = 1.534529\n",
      "epoch 426 loss = 1.485310\n",
      "epoch 427 loss = 1.229579\n",
      "epoch 428 loss = 1.323176\n",
      "epoch 429 loss = 1.360183\n",
      "epoch 430 loss = 1.346749\n",
      "epoch 431 loss = 1.261590\n",
      "epoch 432 loss = 1.354473\n",
      "epoch 433 loss = 1.422812\n",
      "epoch 434 loss = 1.182887\n",
      "epoch 435 loss = 1.535240\n",
      "epoch 436 loss = 1.334519\n",
      "epoch 437 loss = 1.249940\n",
      "epoch 438 loss = 1.294695\n",
      "epoch 439 loss = 1.450316\n",
      "epoch 440 loss = 1.269375\n",
      "epoch 441 loss = 1.422881\n",
      "epoch 442 loss = 1.314480\n",
      "epoch 443 loss = 1.304897\n",
      "epoch 444 loss = 1.290446\n",
      "epoch 445 loss = 1.358487\n",
      "epoch 446 loss = 1.507455\n",
      "epoch 447 loss = 1.230644\n",
      "epoch 448 loss = 1.392381\n",
      "epoch 449 loss = 1.319749\n",
      "epoch 450 loss = 1.285484\n",
      "epoch 451 loss = 1.318851\n",
      "epoch 452 loss = 1.260073\n",
      "epoch 453 loss = 1.268486\n",
      "epoch 454 loss = 1.277872\n",
      "epoch 455 loss = 1.364005\n",
      "epoch 456 loss = 1.264718\n",
      "epoch 457 loss = 1.431299\n",
      "epoch 458 loss = 1.339987\n",
      "epoch 459 loss = 1.239471\n",
      "epoch 460 loss = 1.264965\n",
      "epoch 461 loss = 1.104613\n",
      "epoch 462 loss = 1.164697\n",
      "epoch 463 loss = 1.171619\n",
      "epoch 464 loss = 1.319655\n",
      "epoch 465 loss = 1.342156\n",
      "epoch 466 loss = 1.372681\n",
      "epoch 467 loss = 1.159282\n",
      "epoch 468 loss = 1.341702\n",
      "epoch 469 loss = 1.497996\n",
      "epoch 470 loss = 1.246460\n",
      "epoch 471 loss = 1.279596\n",
      "epoch 472 loss = 1.253040\n",
      "epoch 473 loss = 1.267998\n",
      "epoch 474 loss = 1.500522\n",
      "epoch 475 loss = 1.186896\n",
      "epoch 476 loss = 1.474271\n",
      "epoch 477 loss = 1.232673\n",
      "epoch 478 loss = 1.132114\n",
      "epoch 479 loss = 1.264872\n",
      "epoch 480 loss = 1.297607\n",
      "epoch 481 loss = 1.407895\n",
      "epoch 482 loss = 1.431365\n",
      "epoch 483 loss = 1.343700\n",
      "epoch 484 loss = 1.427867\n",
      "epoch 485 loss = 1.294119\n",
      "epoch 486 loss = 1.064792\n",
      "epoch 487 loss = 1.332549\n",
      "epoch 488 loss = 1.338552\n",
      "epoch 489 loss = 1.268095\n",
      "epoch 490 loss = 1.326253\n",
      "epoch 491 loss = 1.339572\n",
      "epoch 492 loss = 1.338581\n",
      "epoch 493 loss = 1.223905\n",
      "epoch 494 loss = 1.266464\n",
      "epoch 495 loss = 1.368348\n",
      "epoch 496 loss = 1.178536\n",
      "epoch 497 loss = 1.196478\n",
      "epoch 498 loss = 1.282088\n",
      "epoch 499 loss = 1.483226\n",
      "final loss = 1.483226\n",
      "accuracy_mc = tensor(0.4222, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4075, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.5888, device='cuda:0')\n",
      "training time = 170.49582386016846 seconds\n",
      "testing time = 1.85343599319458 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.290829\n",
      "epoch 1 loss = 2.169519\n",
      "epoch 2 loss = 2.099821\n",
      "epoch 3 loss = 1.977093\n",
      "epoch 4 loss = 1.945604\n",
      "epoch 5 loss = 1.888905\n",
      "epoch 6 loss = 1.891817\n",
      "epoch 7 loss = 1.836095\n",
      "epoch 8 loss = 1.826880\n",
      "epoch 9 loss = 1.794796\n",
      "epoch 10 loss = 1.795969\n",
      "epoch 11 loss = 1.872687\n",
      "epoch 12 loss = 1.767425\n",
      "epoch 13 loss = 1.806406\n",
      "epoch 14 loss = 1.827450\n",
      "epoch 15 loss = 1.770216\n",
      "epoch 16 loss = 1.693895\n",
      "epoch 17 loss = 1.714808\n",
      "epoch 18 loss = 1.724535\n",
      "epoch 19 loss = 1.907707\n",
      "epoch 20 loss = 1.606616\n",
      "epoch 21 loss = 1.846717\n",
      "epoch 22 loss = 1.637338\n",
      "epoch 23 loss = 1.644770\n",
      "epoch 24 loss = 1.809891\n",
      "epoch 25 loss = 1.830680\n",
      "epoch 26 loss = 1.794031\n",
      "epoch 27 loss = 1.864673\n",
      "epoch 28 loss = 1.731104\n",
      "epoch 29 loss = 1.756425\n",
      "epoch 30 loss = 1.657516\n",
      "epoch 31 loss = 1.761900\n",
      "epoch 32 loss = 1.859517\n",
      "epoch 33 loss = 1.680395\n",
      "epoch 34 loss = 1.669503\n",
      "epoch 35 loss = 1.648949\n",
      "epoch 36 loss = 1.818764\n",
      "epoch 37 loss = 1.703801\n",
      "epoch 38 loss = 1.620175\n",
      "epoch 39 loss = 1.694144\n",
      "epoch 40 loss = 1.704045\n",
      "epoch 41 loss = 1.675452\n",
      "epoch 42 loss = 1.629438\n",
      "epoch 43 loss = 1.627230\n",
      "epoch 44 loss = 1.717778\n",
      "epoch 45 loss = 1.626659\n",
      "epoch 46 loss = 1.599486\n",
      "epoch 47 loss = 1.703187\n",
      "epoch 48 loss = 1.481691\n",
      "epoch 49 loss = 1.727635\n",
      "epoch 50 loss = 1.565912\n",
      "epoch 51 loss = 1.559226\n",
      "epoch 52 loss = 1.635047\n",
      "epoch 53 loss = 1.547094\n",
      "epoch 54 loss = 1.402800\n",
      "epoch 55 loss = 1.504923\n",
      "epoch 56 loss = 1.625203\n",
      "epoch 57 loss = 1.654653\n",
      "epoch 58 loss = 1.502448\n",
      "epoch 59 loss = 1.600481\n",
      "epoch 60 loss = 1.603311\n",
      "epoch 61 loss = 1.550004\n",
      "epoch 62 loss = 1.471041\n",
      "epoch 63 loss = 1.545877\n",
      "epoch 64 loss = 1.467426\n",
      "epoch 65 loss = 1.442401\n",
      "epoch 66 loss = 1.552579\n",
      "epoch 67 loss = 1.378626\n",
      "epoch 68 loss = 1.467121\n",
      "epoch 69 loss = 1.362131\n",
      "epoch 70 loss = 1.316185\n",
      "epoch 71 loss = 1.457025\n",
      "epoch 72 loss = 1.517740\n",
      "epoch 73 loss = 1.429601\n",
      "epoch 74 loss = 1.492763\n",
      "epoch 75 loss = 1.571590\n",
      "epoch 76 loss = 1.421002\n",
      "epoch 77 loss = 1.345790\n",
      "epoch 78 loss = 1.373146\n",
      "epoch 79 loss = 1.465057\n",
      "epoch 80 loss = 1.456778\n",
      "epoch 81 loss = 1.570004\n",
      "epoch 82 loss = 1.446725\n",
      "epoch 83 loss = 1.500421\n",
      "epoch 84 loss = 1.431243\n",
      "epoch 85 loss = 1.443886\n",
      "epoch 86 loss = 1.368774\n",
      "epoch 87 loss = 1.454028\n",
      "epoch 88 loss = 1.418966\n",
      "epoch 89 loss = 1.469703\n",
      "epoch 90 loss = 1.313857\n",
      "epoch 91 loss = 1.494344\n",
      "epoch 92 loss = 1.481416\n",
      "epoch 93 loss = 1.452647\n",
      "epoch 94 loss = 1.462497\n",
      "epoch 95 loss = 1.496762\n",
      "epoch 96 loss = 1.305138\n",
      "epoch 97 loss = 1.312720\n",
      "epoch 98 loss = 1.447932\n",
      "epoch 99 loss = 1.319126\n",
      "epoch 100 loss = 1.403396\n",
      "epoch 101 loss = 1.470549\n",
      "epoch 102 loss = 1.363168\n",
      "epoch 103 loss = 1.294883\n",
      "epoch 104 loss = 1.440976\n",
      "epoch 105 loss = 1.480507\n",
      "epoch 106 loss = 1.438630\n",
      "epoch 107 loss = 1.328145\n",
      "epoch 108 loss = 1.552048\n",
      "epoch 109 loss = 1.441989\n",
      "epoch 110 loss = 1.340990\n",
      "epoch 111 loss = 1.227746\n",
      "epoch 112 loss = 1.438499\n",
      "epoch 113 loss = 1.382604\n",
      "epoch 114 loss = 1.379424\n",
      "epoch 115 loss = 1.479405\n",
      "epoch 116 loss = 1.337114\n",
      "epoch 117 loss = 1.496046\n",
      "epoch 118 loss = 1.282979\n",
      "epoch 119 loss = 1.605948\n",
      "epoch 120 loss = 1.396541\n",
      "epoch 121 loss = 1.419592\n",
      "epoch 122 loss = 1.270940\n",
      "epoch 123 loss = 1.316934\n",
      "epoch 124 loss = 1.315014\n",
      "epoch 125 loss = 1.341665\n",
      "epoch 126 loss = 1.375107\n",
      "epoch 127 loss = 1.347147\n",
      "epoch 128 loss = 1.470344\n",
      "epoch 129 loss = 1.256456\n",
      "epoch 130 loss = 1.503567\n",
      "epoch 131 loss = 1.407432\n",
      "epoch 132 loss = 1.328086\n",
      "epoch 133 loss = 1.537522\n",
      "epoch 134 loss = 1.450392\n",
      "epoch 135 loss = 1.442213\n",
      "epoch 136 loss = 1.320780\n",
      "epoch 137 loss = 1.228797\n",
      "epoch 138 loss = 1.326162\n",
      "epoch 139 loss = 1.237587\n",
      "epoch 140 loss = 1.348144\n",
      "epoch 141 loss = 1.206801\n",
      "epoch 142 loss = 1.469388\n",
      "epoch 143 loss = 1.227043\n",
      "epoch 144 loss = 1.417096\n",
      "epoch 145 loss = 1.426403\n",
      "epoch 146 loss = 1.301301\n",
      "epoch 147 loss = 1.229655\n",
      "epoch 148 loss = 1.278331\n",
      "epoch 149 loss = 1.300142\n",
      "epoch 150 loss = 1.374586\n",
      "epoch 151 loss = 1.292966\n",
      "epoch 152 loss = 1.356398\n",
      "epoch 153 loss = 1.578507\n",
      "epoch 154 loss = 1.342512\n",
      "epoch 155 loss = 1.297059\n",
      "epoch 156 loss = 1.374310\n",
      "epoch 157 loss = 1.366870\n",
      "epoch 158 loss = 1.326390\n",
      "epoch 159 loss = 1.313863\n",
      "epoch 160 loss = 1.287371\n",
      "epoch 161 loss = 1.458958\n",
      "epoch 162 loss = 1.412515\n",
      "epoch 163 loss = 1.541800\n",
      "epoch 164 loss = 1.411016\n",
      "epoch 165 loss = 1.417695\n",
      "epoch 166 loss = 1.386676\n",
      "epoch 167 loss = 1.305899\n",
      "epoch 168 loss = 1.357720\n",
      "epoch 169 loss = 1.264482\n",
      "epoch 170 loss = 1.117535\n",
      "epoch 171 loss = 1.308713\n",
      "epoch 172 loss = 1.302615\n",
      "epoch 173 loss = 1.254925\n",
      "epoch 174 loss = 1.351025\n",
      "epoch 175 loss = 1.431459\n",
      "epoch 176 loss = 1.282789\n",
      "epoch 177 loss = 1.271227\n",
      "epoch 178 loss = 1.224721\n",
      "epoch 179 loss = 1.357017\n",
      "epoch 180 loss = 1.383767\n",
      "epoch 181 loss = 1.166906\n",
      "epoch 182 loss = 1.185967\n",
      "epoch 183 loss = 1.303560\n",
      "epoch 184 loss = 1.227334\n",
      "epoch 185 loss = 1.256583\n",
      "epoch 186 loss = 1.158042\n",
      "epoch 187 loss = 1.359194\n",
      "epoch 188 loss = 1.423251\n",
      "epoch 189 loss = 1.408893\n",
      "epoch 190 loss = 1.429594\n",
      "epoch 191 loss = 1.321107\n",
      "epoch 192 loss = 1.240200\n",
      "epoch 193 loss = 1.220957\n",
      "epoch 194 loss = 1.256066\n",
      "epoch 195 loss = 1.349303\n",
      "epoch 196 loss = 1.486580\n",
      "epoch 197 loss = 1.239552\n",
      "epoch 198 loss = 1.227821\n",
      "epoch 199 loss = 1.319084\n",
      "epoch 200 loss = 1.315601\n",
      "epoch 201 loss = 1.247567\n",
      "epoch 202 loss = 1.338858\n",
      "epoch 203 loss = 1.278003\n",
      "epoch 204 loss = 1.350715\n",
      "epoch 205 loss = 1.312895\n",
      "epoch 206 loss = 1.467089\n",
      "epoch 207 loss = 1.464678\n",
      "epoch 208 loss = 1.198169\n",
      "epoch 209 loss = 1.182751\n",
      "epoch 210 loss = 1.189671\n",
      "epoch 211 loss = 1.377775\n",
      "epoch 212 loss = 1.251593\n",
      "epoch 213 loss = 1.191949\n",
      "epoch 214 loss = 1.241578\n",
      "epoch 215 loss = 1.336167\n",
      "epoch 216 loss = 1.288085\n",
      "epoch 217 loss = 1.308792\n",
      "epoch 218 loss = 1.430804\n",
      "epoch 219 loss = 1.190948\n",
      "epoch 220 loss = 1.267242\n",
      "epoch 221 loss = 1.285267\n",
      "epoch 222 loss = 1.201400\n",
      "epoch 223 loss = 1.159936\n",
      "epoch 224 loss = 1.081811\n",
      "epoch 225 loss = 1.128501\n",
      "epoch 226 loss = 1.208333\n",
      "epoch 227 loss = 1.457950\n",
      "epoch 228 loss = 1.221697\n",
      "epoch 229 loss = 1.323570\n",
      "epoch 230 loss = 1.252634\n",
      "epoch 231 loss = 1.251448\n",
      "epoch 232 loss = 1.280223\n",
      "epoch 233 loss = 1.400195\n",
      "epoch 234 loss = 1.180544\n",
      "epoch 235 loss = 1.301658\n",
      "epoch 236 loss = 1.337375\n",
      "epoch 237 loss = 1.301151\n",
      "epoch 238 loss = 1.156735\n",
      "epoch 239 loss = 1.138447\n",
      "epoch 240 loss = 1.362714\n",
      "epoch 241 loss = 1.270231\n",
      "epoch 242 loss = 1.262264\n",
      "epoch 243 loss = 1.460147\n",
      "epoch 244 loss = 1.255532\n",
      "epoch 245 loss = 1.427348\n",
      "epoch 246 loss = 1.278377\n",
      "epoch 247 loss = 1.180694\n",
      "epoch 248 loss = 1.234074\n",
      "epoch 249 loss = 1.041413\n",
      "epoch 250 loss = 1.228468\n",
      "epoch 251 loss = 1.278421\n",
      "epoch 252 loss = 1.358826\n",
      "epoch 253 loss = 1.130936\n",
      "epoch 254 loss = 1.229981\n",
      "epoch 255 loss = 1.257704\n",
      "epoch 256 loss = 1.403167\n",
      "epoch 257 loss = 1.363647\n",
      "epoch 258 loss = 1.200631\n",
      "epoch 259 loss = 1.290559\n",
      "epoch 260 loss = 1.170085\n",
      "epoch 261 loss = 1.149697\n",
      "epoch 262 loss = 1.460713\n",
      "epoch 263 loss = 1.516003\n",
      "epoch 264 loss = 1.399952\n",
      "epoch 265 loss = 1.176833\n",
      "epoch 266 loss = 1.342168\n",
      "epoch 267 loss = 1.514444\n",
      "epoch 268 loss = 1.306805\n",
      "epoch 269 loss = 1.640645\n",
      "epoch 270 loss = 1.265630\n",
      "epoch 271 loss = 1.200740\n",
      "epoch 272 loss = 1.314919\n",
      "epoch 273 loss = 1.185040\n",
      "epoch 274 loss = 1.292289\n",
      "epoch 275 loss = 1.274501\n",
      "epoch 276 loss = 1.319566\n",
      "epoch 277 loss = 1.080885\n",
      "epoch 278 loss = 1.107467\n",
      "epoch 279 loss = 1.224208\n",
      "epoch 280 loss = 1.111790\n",
      "epoch 281 loss = 1.337336\n",
      "epoch 282 loss = 1.130208\n",
      "epoch 283 loss = 1.289320\n",
      "epoch 284 loss = 1.230011\n",
      "epoch 285 loss = 1.374593\n",
      "epoch 286 loss = 1.358119\n",
      "epoch 287 loss = 1.175519\n",
      "epoch 288 loss = 1.228872\n",
      "epoch 289 loss = 1.268786\n",
      "epoch 290 loss = 1.424401\n",
      "epoch 291 loss = 1.375975\n",
      "epoch 292 loss = 1.254759\n",
      "epoch 293 loss = 1.215709\n",
      "epoch 294 loss = 1.345096\n",
      "epoch 295 loss = 1.207422\n",
      "epoch 296 loss = 1.203138\n",
      "epoch 297 loss = 1.191804\n",
      "epoch 298 loss = 1.504423\n",
      "epoch 299 loss = 1.359746\n",
      "epoch 300 loss = 1.350146\n",
      "epoch 301 loss = 1.269954\n",
      "epoch 302 loss = 1.290415\n",
      "epoch 303 loss = 1.142070\n",
      "epoch 304 loss = 1.140476\n",
      "epoch 305 loss = 1.417670\n",
      "epoch 306 loss = 1.447675\n",
      "epoch 307 loss = 1.419718\n",
      "epoch 308 loss = 1.272335\n",
      "epoch 309 loss = 1.261827\n",
      "epoch 310 loss = 1.254276\n",
      "epoch 311 loss = 1.334128\n",
      "epoch 312 loss = 1.077322\n",
      "epoch 313 loss = 1.182696\n",
      "epoch 314 loss = 1.364697\n",
      "epoch 315 loss = 1.075274\n",
      "epoch 316 loss = 1.311118\n",
      "epoch 317 loss = 1.161203\n",
      "epoch 318 loss = 1.220770\n",
      "epoch 319 loss = 1.286234\n",
      "epoch 320 loss = 1.245518\n",
      "epoch 321 loss = 1.375293\n",
      "epoch 322 loss = 1.199729\n",
      "epoch 323 loss = 1.097857\n",
      "epoch 324 loss = 1.255985\n",
      "epoch 325 loss = 1.340947\n",
      "epoch 326 loss = 1.167266\n",
      "epoch 327 loss = 1.115155\n",
      "epoch 328 loss = 1.128944\n",
      "epoch 329 loss = 1.452029\n",
      "epoch 330 loss = 1.097461\n",
      "epoch 331 loss = 1.369218\n",
      "epoch 332 loss = 1.182847\n",
      "epoch 333 loss = 1.247410\n",
      "epoch 334 loss = 1.261947\n",
      "epoch 335 loss = 1.303983\n",
      "epoch 336 loss = 1.102673\n",
      "epoch 337 loss = 1.188730\n",
      "epoch 338 loss = 1.256877\n",
      "epoch 339 loss = 1.234063\n",
      "epoch 340 loss = 1.227226\n",
      "epoch 341 loss = 1.303260\n",
      "epoch 342 loss = 1.213055\n",
      "epoch 343 loss = 1.436954\n",
      "epoch 344 loss = 1.230050\n",
      "epoch 345 loss = 1.354370\n",
      "epoch 346 loss = 1.150866\n",
      "epoch 347 loss = 1.436270\n",
      "epoch 348 loss = 1.145756\n",
      "epoch 349 loss = 1.237208\n",
      "epoch 350 loss = 1.051355\n",
      "epoch 351 loss = 1.168046\n",
      "epoch 352 loss = 1.375272\n",
      "epoch 353 loss = 1.469942\n",
      "epoch 354 loss = 1.290537\n",
      "epoch 355 loss = 1.328546\n",
      "epoch 356 loss = 1.369195\n",
      "epoch 357 loss = 1.323058\n",
      "epoch 358 loss = 1.093317\n",
      "epoch 359 loss = 1.133203\n",
      "epoch 360 loss = 1.126797\n",
      "epoch 361 loss = 1.136872\n",
      "epoch 362 loss = 1.238620\n",
      "epoch 363 loss = 1.221259\n",
      "epoch 364 loss = 1.293018\n",
      "epoch 365 loss = 1.207414\n",
      "epoch 366 loss = 1.321724\n",
      "epoch 367 loss = 1.144404\n",
      "epoch 368 loss = 1.192582\n",
      "epoch 369 loss = 1.179455\n",
      "epoch 370 loss = 1.075155\n",
      "epoch 371 loss = 1.083087\n",
      "epoch 372 loss = 1.260452\n",
      "epoch 373 loss = 1.086092\n",
      "epoch 374 loss = 1.283105\n",
      "epoch 375 loss = 1.172749\n",
      "epoch 376 loss = 1.246307\n",
      "epoch 377 loss = 1.233218\n",
      "epoch 378 loss = 1.244519\n",
      "epoch 379 loss = 1.172775\n",
      "epoch 380 loss = 1.060152\n",
      "epoch 381 loss = 1.185882\n",
      "epoch 382 loss = 1.181992\n",
      "epoch 383 loss = 1.402575\n",
      "epoch 384 loss = 1.263786\n",
      "epoch 385 loss = 1.127574\n",
      "epoch 386 loss = 1.149342\n",
      "epoch 387 loss = 1.064767\n",
      "epoch 388 loss = 1.047761\n",
      "epoch 389 loss = 1.204747\n",
      "epoch 390 loss = 1.399407\n",
      "epoch 391 loss = 1.085574\n",
      "epoch 392 loss = 1.120696\n",
      "epoch 393 loss = 1.027524\n",
      "epoch 394 loss = 1.207167\n",
      "epoch 395 loss = 1.076036\n",
      "epoch 396 loss = 1.144496\n",
      "epoch 397 loss = 1.283679\n",
      "epoch 398 loss = 1.161999\n",
      "epoch 399 loss = 1.276513\n",
      "epoch 400 loss = 1.221025\n",
      "epoch 401 loss = 1.354502\n",
      "epoch 402 loss = 1.129624\n",
      "epoch 403 loss = 1.347901\n",
      "epoch 404 loss = 1.143229\n",
      "epoch 405 loss = 1.094574\n",
      "epoch 406 loss = 1.501276\n",
      "epoch 407 loss = 1.122047\n",
      "epoch 408 loss = 1.082300\n",
      "epoch 409 loss = 1.230953\n",
      "epoch 410 loss = 1.317697\n",
      "epoch 411 loss = 1.123601\n",
      "epoch 412 loss = 1.291379\n",
      "epoch 413 loss = 1.240876\n",
      "epoch 414 loss = 1.119260\n",
      "epoch 415 loss = 1.192039\n",
      "epoch 416 loss = 0.987367\n",
      "epoch 417 loss = 1.067350\n",
      "epoch 418 loss = 1.297435\n",
      "epoch 419 loss = 1.313663\n",
      "epoch 420 loss = 0.949579\n",
      "epoch 421 loss = 1.285430\n",
      "epoch 422 loss = 1.050616\n",
      "epoch 423 loss = 1.223002\n",
      "epoch 424 loss = 1.187543\n",
      "epoch 425 loss = 1.083286\n",
      "epoch 426 loss = 1.059218\n",
      "epoch 427 loss = 1.059783\n",
      "epoch 428 loss = 1.166235\n",
      "epoch 429 loss = 1.171197\n",
      "epoch 430 loss = 1.322187\n",
      "epoch 431 loss = 1.156309\n",
      "epoch 432 loss = 1.022303\n",
      "epoch 433 loss = 1.032447\n",
      "epoch 434 loss = 1.180754\n",
      "epoch 435 loss = 0.992108\n",
      "epoch 436 loss = 1.346373\n",
      "epoch 437 loss = 1.144861\n",
      "epoch 438 loss = 1.000512\n",
      "epoch 439 loss = 1.101709\n",
      "epoch 440 loss = 1.285666\n",
      "epoch 441 loss = 1.103520\n",
      "epoch 442 loss = 0.960660\n",
      "epoch 443 loss = 1.280092\n",
      "epoch 444 loss = 1.027693\n",
      "epoch 445 loss = 0.982878\n",
      "epoch 446 loss = 1.263974\n",
      "epoch 447 loss = 1.041719\n",
      "epoch 448 loss = 1.153849\n",
      "epoch 449 loss = 1.085846\n",
      "epoch 450 loss = 1.120875\n",
      "epoch 451 loss = 0.934702\n",
      "epoch 452 loss = 1.271210\n",
      "epoch 453 loss = 0.969204\n",
      "epoch 454 loss = 1.081650\n",
      "epoch 455 loss = 1.455837\n",
      "epoch 456 loss = 1.107293\n",
      "epoch 457 loss = 1.122290\n",
      "epoch 458 loss = 1.042441\n",
      "epoch 459 loss = 1.141422\n",
      "epoch 460 loss = 1.139891\n",
      "epoch 461 loss = 1.229759\n",
      "epoch 462 loss = 0.963760\n",
      "epoch 463 loss = 1.111576\n",
      "epoch 464 loss = 1.134606\n",
      "epoch 465 loss = 1.075852\n",
      "epoch 466 loss = 1.126297\n",
      "epoch 467 loss = 1.124255\n",
      "epoch 468 loss = 1.203006\n",
      "epoch 469 loss = 1.059292\n",
      "epoch 470 loss = 1.056039\n",
      "epoch 471 loss = 0.974358\n",
      "epoch 472 loss = 1.145104\n",
      "epoch 473 loss = 1.094673\n",
      "epoch 474 loss = 1.061220\n",
      "epoch 475 loss = 1.018940\n",
      "epoch 476 loss = 1.188105\n",
      "epoch 477 loss = 1.173627\n",
      "epoch 478 loss = 1.133813\n",
      "epoch 479 loss = 1.157377\n",
      "epoch 480 loss = 1.164201\n",
      "epoch 481 loss = 1.125088\n",
      "epoch 482 loss = 1.386986\n",
      "epoch 483 loss = 1.112729\n",
      "epoch 484 loss = 1.065890\n",
      "epoch 485 loss = 0.996951\n",
      "epoch 486 loss = 0.949477\n",
      "epoch 487 loss = 1.289423\n",
      "epoch 488 loss = 1.106452\n",
      "epoch 489 loss = 0.995821\n",
      "epoch 490 loss = 0.931515\n",
      "epoch 491 loss = 1.051960\n",
      "epoch 492 loss = 1.181357\n",
      "epoch 493 loss = 1.070879\n",
      "epoch 494 loss = 0.982345\n",
      "epoch 495 loss = 0.907280\n",
      "epoch 496 loss = 1.232271\n",
      "epoch 497 loss = 1.124337\n",
      "epoch 498 loss = 1.123629\n",
      "epoch 499 loss = 1.022556\n",
      "final loss = 1.022556\n",
      "accuracy_mc = tensor(0.4372, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4354, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6754, device='cuda:0')\n",
      "training time = 170.24059438705444 seconds\n",
      "testing time = 1.868166208267212 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.312642\n",
      "epoch 1 loss = 2.259855\n",
      "epoch 2 loss = 2.227204\n",
      "epoch 3 loss = 2.228926\n",
      "epoch 4 loss = 2.201014\n",
      "epoch 5 loss = 2.235920\n",
      "epoch 6 loss = 2.163741\n",
      "epoch 7 loss = 2.073805\n",
      "epoch 8 loss = 2.126267\n",
      "epoch 9 loss = 2.163301\n",
      "epoch 10 loss = 1.973898\n",
      "epoch 11 loss = 2.118507\n",
      "epoch 12 loss = 2.045679\n",
      "epoch 13 loss = 1.953497\n",
      "epoch 14 loss = 2.024954\n",
      "epoch 15 loss = 1.928946\n",
      "epoch 16 loss = 1.987620\n",
      "epoch 17 loss = 1.915626\n",
      "epoch 18 loss = 1.806216\n",
      "epoch 19 loss = 1.873703\n",
      "epoch 20 loss = 1.811633\n",
      "epoch 21 loss = 1.835589\n",
      "epoch 22 loss = 1.781286\n",
      "epoch 23 loss = 1.711997\n",
      "epoch 24 loss = 1.760025\n",
      "epoch 25 loss = 1.794589\n",
      "epoch 26 loss = 1.682626\n",
      "epoch 27 loss = 1.810782\n",
      "epoch 28 loss = 1.681186\n",
      "epoch 29 loss = 1.704679\n",
      "epoch 30 loss = 1.837491\n",
      "epoch 31 loss = 1.822398\n",
      "epoch 32 loss = 1.830510\n",
      "epoch 33 loss = 1.733994\n",
      "epoch 34 loss = 1.794642\n",
      "epoch 35 loss = 1.800710\n",
      "epoch 36 loss = 1.676939\n",
      "epoch 37 loss = 1.650956\n",
      "epoch 38 loss = 1.694255\n",
      "epoch 39 loss = 1.613312\n",
      "epoch 40 loss = 1.754902\n",
      "epoch 41 loss = 1.708627\n",
      "epoch 42 loss = 1.650649\n",
      "epoch 43 loss = 1.570542\n",
      "epoch 44 loss = 1.602401\n",
      "epoch 45 loss = 1.787593\n",
      "epoch 46 loss = 1.774340\n",
      "epoch 47 loss = 1.656389\n",
      "epoch 48 loss = 1.650007\n",
      "epoch 49 loss = 1.572506\n",
      "epoch 50 loss = 1.620806\n",
      "epoch 51 loss = 1.649820\n",
      "epoch 52 loss = 1.718790\n",
      "epoch 53 loss = 1.530493\n",
      "epoch 54 loss = 1.657254\n",
      "epoch 55 loss = 1.710330\n",
      "epoch 56 loss = 1.532440\n",
      "epoch 57 loss = 1.565224\n",
      "epoch 58 loss = 1.696763\n",
      "epoch 59 loss = 1.641543\n",
      "epoch 60 loss = 1.560518\n",
      "epoch 61 loss = 1.556558\n",
      "epoch 62 loss = 1.540948\n",
      "epoch 63 loss = 1.659786\n",
      "epoch 64 loss = 1.475001\n",
      "epoch 65 loss = 1.566318\n",
      "epoch 66 loss = 1.692287\n",
      "epoch 67 loss = 1.590881\n",
      "epoch 68 loss = 1.620821\n",
      "epoch 69 loss = 1.629348\n",
      "epoch 70 loss = 1.574067\n",
      "epoch 71 loss = 1.721336\n",
      "epoch 72 loss = 1.504691\n",
      "epoch 73 loss = 1.504372\n",
      "epoch 74 loss = 1.689907\n",
      "epoch 75 loss = 1.512479\n",
      "epoch 76 loss = 1.516444\n",
      "epoch 77 loss = 1.438094\n",
      "epoch 78 loss = 1.643050\n",
      "epoch 79 loss = 1.516756\n",
      "epoch 80 loss = 1.649285\n",
      "epoch 81 loss = 1.633752\n",
      "epoch 82 loss = 1.540697\n",
      "epoch 83 loss = 1.392429\n",
      "epoch 84 loss = 1.574112\n",
      "epoch 85 loss = 1.488503\n",
      "epoch 86 loss = 1.513069\n",
      "epoch 87 loss = 1.432750\n",
      "epoch 88 loss = 1.452861\n",
      "epoch 89 loss = 1.496300\n",
      "epoch 90 loss = 1.506341\n",
      "epoch 91 loss = 1.436319\n",
      "epoch 92 loss = 1.574183\n",
      "epoch 93 loss = 1.509193\n",
      "epoch 94 loss = 1.475427\n",
      "epoch 95 loss = 1.506452\n",
      "epoch 96 loss = 1.404579\n",
      "epoch 97 loss = 1.469592\n",
      "epoch 98 loss = 1.489741\n",
      "epoch 99 loss = 1.320581\n",
      "epoch 100 loss = 1.591863\n",
      "epoch 101 loss = 1.648250\n",
      "epoch 102 loss = 1.426139\n",
      "epoch 103 loss = 1.394760\n",
      "epoch 104 loss = 1.366627\n",
      "epoch 105 loss = 1.428758\n",
      "epoch 106 loss = 1.338120\n",
      "epoch 107 loss = 1.388354\n",
      "epoch 108 loss = 1.584429\n",
      "epoch 109 loss = 1.390297\n",
      "epoch 110 loss = 1.502941\n",
      "epoch 111 loss = 1.396563\n",
      "epoch 112 loss = 1.368890\n",
      "epoch 113 loss = 1.439497\n",
      "epoch 114 loss = 1.619282\n",
      "epoch 115 loss = 1.443170\n",
      "epoch 116 loss = 1.561283\n",
      "epoch 117 loss = 1.625762\n",
      "epoch 118 loss = 1.476343\n",
      "epoch 119 loss = 1.600760\n",
      "epoch 120 loss = 1.585346\n",
      "epoch 121 loss = 1.563641\n",
      "epoch 122 loss = 1.665757\n",
      "epoch 123 loss = 1.421085\n",
      "epoch 124 loss = 1.598250\n",
      "epoch 125 loss = 1.483667\n",
      "epoch 126 loss = 1.475435\n",
      "epoch 127 loss = 1.495170\n",
      "epoch 128 loss = 1.390111\n",
      "epoch 129 loss = 1.475641\n",
      "epoch 130 loss = 1.449477\n",
      "epoch 131 loss = 1.444660\n",
      "epoch 132 loss = 1.365053\n",
      "epoch 133 loss = 1.483395\n",
      "epoch 134 loss = 1.351942\n",
      "epoch 135 loss = 1.543348\n",
      "epoch 136 loss = 1.441777\n",
      "epoch 137 loss = 1.620467\n",
      "epoch 138 loss = 1.405074\n",
      "epoch 139 loss = 1.438953\n",
      "epoch 140 loss = 1.504526\n",
      "epoch 141 loss = 1.592851\n",
      "epoch 142 loss = 1.286742\n",
      "epoch 143 loss = 1.376544\n",
      "epoch 144 loss = 1.390473\n",
      "epoch 145 loss = 1.379678\n",
      "epoch 146 loss = 1.377467\n",
      "epoch 147 loss = 1.443100\n",
      "epoch 148 loss = 1.308059\n",
      "epoch 149 loss = 1.244006\n",
      "epoch 150 loss = 1.316999\n",
      "epoch 151 loss = 1.518724\n",
      "epoch 152 loss = 1.407164\n",
      "epoch 153 loss = 1.390844\n",
      "epoch 154 loss = 1.403313\n",
      "epoch 155 loss = 1.467138\n",
      "epoch 156 loss = 1.289297\n",
      "epoch 157 loss = 1.397123\n",
      "epoch 158 loss = 1.317111\n",
      "epoch 159 loss = 1.346648\n",
      "epoch 160 loss = 1.489085\n",
      "epoch 161 loss = 1.414045\n",
      "epoch 162 loss = 1.428609\n",
      "epoch 163 loss = 1.297615\n",
      "epoch 164 loss = 1.406925\n",
      "epoch 165 loss = 1.394745\n",
      "epoch 166 loss = 1.400646\n",
      "epoch 167 loss = 1.319170\n",
      "epoch 168 loss = 1.225926\n",
      "epoch 169 loss = 1.286103\n",
      "epoch 170 loss = 1.511486\n",
      "epoch 171 loss = 1.447730\n",
      "epoch 172 loss = 1.478963\n",
      "epoch 173 loss = 1.383458\n",
      "epoch 174 loss = 1.304381\n",
      "epoch 175 loss = 1.650151\n",
      "epoch 176 loss = 1.224940\n",
      "epoch 177 loss = 1.315922\n",
      "epoch 178 loss = 1.385405\n",
      "epoch 179 loss = 1.575895\n",
      "epoch 180 loss = 1.424961\n",
      "epoch 181 loss = 1.361040\n",
      "epoch 182 loss = 1.324115\n",
      "epoch 183 loss = 1.273011\n",
      "epoch 184 loss = 1.386558\n",
      "epoch 185 loss = 1.348169\n",
      "epoch 186 loss = 1.439301\n",
      "epoch 187 loss = 1.430299\n",
      "epoch 188 loss = 1.430909\n",
      "epoch 189 loss = 1.331923\n",
      "epoch 190 loss = 1.281322\n",
      "epoch 191 loss = 1.301213\n",
      "epoch 192 loss = 1.446258\n",
      "epoch 193 loss = 1.401836\n",
      "epoch 194 loss = 1.297566\n",
      "epoch 195 loss = 1.476022\n",
      "epoch 196 loss = 1.375604\n",
      "epoch 197 loss = 1.312442\n",
      "epoch 198 loss = 1.569112\n",
      "epoch 199 loss = 1.409062\n",
      "epoch 200 loss = 1.273857\n",
      "epoch 201 loss = 1.395077\n",
      "epoch 202 loss = 1.412635\n",
      "epoch 203 loss = 1.229172\n",
      "epoch 204 loss = 1.203061\n",
      "epoch 205 loss = 1.345594\n",
      "epoch 206 loss = 1.387625\n",
      "epoch 207 loss = 1.400496\n",
      "epoch 208 loss = 1.319227\n",
      "epoch 209 loss = 1.204295\n",
      "epoch 210 loss = 1.308377\n",
      "epoch 211 loss = 1.264915\n",
      "epoch 212 loss = 1.216257\n",
      "epoch 213 loss = 1.229446\n",
      "epoch 214 loss = 1.160386\n",
      "epoch 215 loss = 1.257855\n",
      "epoch 216 loss = 1.366435\n",
      "epoch 217 loss = 1.291752\n",
      "epoch 218 loss = 1.254619\n",
      "epoch 219 loss = 1.293258\n",
      "epoch 220 loss = 1.129583\n",
      "epoch 221 loss = 1.234250\n",
      "epoch 222 loss = 1.346639\n",
      "epoch 223 loss = 1.432938\n",
      "epoch 224 loss = 1.267749\n",
      "epoch 225 loss = 1.434698\n",
      "epoch 226 loss = 1.222749\n",
      "epoch 227 loss = 1.260806\n",
      "epoch 228 loss = 1.412397\n",
      "epoch 229 loss = 1.292753\n",
      "epoch 230 loss = 1.242122\n",
      "epoch 231 loss = 1.470301\n",
      "epoch 232 loss = 1.296136\n",
      "epoch 233 loss = 1.269814\n",
      "epoch 234 loss = 1.143722\n",
      "epoch 235 loss = 1.277997\n",
      "epoch 236 loss = 1.438070\n",
      "epoch 237 loss = 1.229298\n",
      "epoch 238 loss = 1.393773\n",
      "epoch 239 loss = 1.306098\n",
      "epoch 240 loss = 1.280942\n",
      "epoch 241 loss = 1.149937\n",
      "epoch 242 loss = 1.308205\n",
      "epoch 243 loss = 1.308621\n",
      "epoch 244 loss = 1.274879\n",
      "epoch 245 loss = 1.219828\n",
      "epoch 246 loss = 1.203223\n",
      "epoch 247 loss = 1.164645\n",
      "epoch 248 loss = 1.274736\n",
      "epoch 249 loss = 1.249616\n",
      "epoch 250 loss = 1.317016\n",
      "epoch 251 loss = 1.177693\n",
      "epoch 252 loss = 1.231114\n",
      "epoch 253 loss = 1.311050\n",
      "epoch 254 loss = 1.153189\n",
      "epoch 255 loss = 1.186375\n",
      "epoch 256 loss = 1.336427\n",
      "epoch 257 loss = 1.291911\n",
      "epoch 258 loss = 1.186177\n",
      "epoch 259 loss = 1.392328\n",
      "epoch 260 loss = 1.147429\n",
      "epoch 261 loss = 1.218865\n",
      "epoch 262 loss = 1.179983\n",
      "epoch 263 loss = 1.128990\n",
      "epoch 264 loss = 1.475004\n",
      "epoch 265 loss = 1.140158\n",
      "epoch 266 loss = 1.196493\n",
      "epoch 267 loss = 1.310441\n",
      "epoch 268 loss = 1.158213\n",
      "epoch 269 loss = 1.190560\n",
      "epoch 270 loss = 1.210577\n",
      "epoch 271 loss = 1.293097\n",
      "epoch 272 loss = 1.316675\n",
      "epoch 273 loss = 1.055972\n",
      "epoch 274 loss = 1.127705\n",
      "epoch 275 loss = 1.269807\n",
      "epoch 276 loss = 1.133507\n",
      "epoch 277 loss = 1.182261\n",
      "epoch 278 loss = 1.135092\n",
      "epoch 279 loss = 1.186730\n",
      "epoch 280 loss = 1.013558\n",
      "epoch 281 loss = 1.377027\n",
      "epoch 282 loss = 1.179991\n",
      "epoch 283 loss = 1.320631\n",
      "epoch 284 loss = 1.242844\n",
      "epoch 285 loss = 1.297811\n",
      "epoch 286 loss = 1.326238\n",
      "epoch 287 loss = 1.308539\n",
      "epoch 288 loss = 1.206740\n",
      "epoch 289 loss = 1.179321\n",
      "epoch 290 loss = 1.367235\n",
      "epoch 291 loss = 1.265499\n",
      "epoch 292 loss = 1.148573\n",
      "epoch 293 loss = 1.236064\n",
      "epoch 294 loss = 1.262135\n",
      "epoch 295 loss = 1.309036\n",
      "epoch 296 loss = 1.094789\n",
      "epoch 297 loss = 1.181737\n",
      "epoch 298 loss = 1.142223\n",
      "epoch 299 loss = 1.140654\n",
      "epoch 300 loss = 1.177769\n",
      "epoch 301 loss = 1.117177\n",
      "epoch 302 loss = 1.170426\n",
      "epoch 303 loss = 1.385524\n",
      "epoch 304 loss = 1.089285\n",
      "epoch 305 loss = 1.298189\n",
      "epoch 306 loss = 1.256204\n",
      "epoch 307 loss = 1.175384\n",
      "epoch 308 loss = 1.229712\n",
      "epoch 309 loss = 1.109468\n",
      "epoch 310 loss = 1.173844\n",
      "epoch 311 loss = 1.237237\n",
      "epoch 312 loss = 1.263330\n",
      "epoch 313 loss = 1.189213\n",
      "epoch 314 loss = 1.350137\n",
      "epoch 315 loss = 1.110344\n",
      "epoch 316 loss = 1.313420\n",
      "epoch 317 loss = 1.171708\n",
      "epoch 318 loss = 1.111709\n",
      "epoch 319 loss = 1.317704\n",
      "epoch 320 loss = 1.146320\n",
      "epoch 321 loss = 1.155590\n",
      "epoch 322 loss = 1.287239\n",
      "epoch 323 loss = 1.124351\n",
      "epoch 324 loss = 1.150755\n",
      "epoch 325 loss = 1.104090\n",
      "epoch 326 loss = 1.159068\n",
      "epoch 327 loss = 1.268880\n",
      "epoch 328 loss = 1.129791\n",
      "epoch 329 loss = 1.023931\n",
      "epoch 330 loss = 1.260347\n",
      "epoch 331 loss = 1.374298\n",
      "epoch 332 loss = 1.075020\n",
      "epoch 333 loss = 1.079904\n",
      "epoch 334 loss = 1.353379\n",
      "epoch 335 loss = 1.147145\n",
      "epoch 336 loss = 0.996324\n",
      "epoch 337 loss = 1.000871\n",
      "epoch 338 loss = 1.119654\n",
      "epoch 339 loss = 1.298526\n",
      "epoch 340 loss = 1.113780\n",
      "epoch 341 loss = 1.105883\n",
      "epoch 342 loss = 1.047858\n",
      "epoch 343 loss = 1.027311\n",
      "epoch 344 loss = 1.255614\n",
      "epoch 345 loss = 1.030129\n",
      "epoch 346 loss = 1.232817\n",
      "epoch 347 loss = 1.165646\n",
      "epoch 348 loss = 1.018249\n",
      "epoch 349 loss = 1.290870\n",
      "epoch 350 loss = 1.055873\n",
      "epoch 351 loss = 1.188030\n",
      "epoch 352 loss = 1.205435\n",
      "epoch 353 loss = 1.188888\n",
      "epoch 354 loss = 1.240618\n",
      "epoch 355 loss = 1.114389\n",
      "epoch 356 loss = 1.218233\n",
      "epoch 357 loss = 1.322295\n",
      "epoch 358 loss = 1.112254\n",
      "epoch 359 loss = 1.063995\n",
      "epoch 360 loss = 1.136401\n",
      "epoch 361 loss = 1.117418\n",
      "epoch 362 loss = 1.244699\n",
      "epoch 363 loss = 0.996876\n",
      "epoch 364 loss = 1.191676\n",
      "epoch 365 loss = 1.112313\n",
      "epoch 366 loss = 1.170837\n",
      "epoch 367 loss = 1.101538\n",
      "epoch 368 loss = 1.206618\n",
      "epoch 369 loss = 1.228154\n",
      "epoch 370 loss = 1.082658\n",
      "epoch 371 loss = 1.191770\n",
      "epoch 372 loss = 1.303659\n",
      "epoch 373 loss = 0.972763\n",
      "epoch 374 loss = 1.143585\n",
      "epoch 375 loss = 1.080290\n",
      "epoch 376 loss = 1.245651\n",
      "epoch 377 loss = 1.310003\n",
      "epoch 378 loss = 1.021090\n",
      "epoch 379 loss = 1.181682\n",
      "epoch 380 loss = 1.077467\n",
      "epoch 381 loss = 1.432593\n",
      "epoch 382 loss = 0.973280\n",
      "epoch 383 loss = 1.229471\n",
      "epoch 384 loss = 1.103816\n",
      "epoch 385 loss = 1.151045\n",
      "epoch 386 loss = 1.189069\n",
      "epoch 387 loss = 1.119040\n",
      "epoch 388 loss = 1.370669\n",
      "epoch 389 loss = 1.027506\n",
      "epoch 390 loss = 1.249990\n",
      "epoch 391 loss = 1.075691\n",
      "epoch 392 loss = 1.210091\n",
      "epoch 393 loss = 1.141041\n",
      "epoch 394 loss = 1.035765\n",
      "epoch 395 loss = 1.099956\n",
      "epoch 396 loss = 1.329259\n",
      "epoch 397 loss = 1.283111\n",
      "epoch 398 loss = 1.040699\n",
      "epoch 399 loss = 1.035346\n",
      "epoch 400 loss = 1.116017\n",
      "epoch 401 loss = 1.264031\n",
      "epoch 402 loss = 1.022475\n",
      "epoch 403 loss = 1.153080\n",
      "epoch 404 loss = 1.182313\n",
      "epoch 405 loss = 1.110678\n",
      "epoch 406 loss = 1.273238\n",
      "epoch 407 loss = 1.139608\n",
      "epoch 408 loss = 1.208758\n",
      "epoch 409 loss = 1.144329\n",
      "epoch 410 loss = 1.201314\n",
      "epoch 411 loss = 0.940873\n",
      "epoch 412 loss = 1.529294\n",
      "epoch 413 loss = 1.144386\n",
      "epoch 414 loss = 1.215111\n",
      "epoch 415 loss = 1.021669\n",
      "epoch 416 loss = 1.261951\n",
      "epoch 417 loss = 1.058735\n",
      "epoch 418 loss = 1.090343\n",
      "epoch 419 loss = 1.060023\n",
      "epoch 420 loss = 1.052658\n",
      "epoch 421 loss = 1.235644\n",
      "epoch 422 loss = 1.448885\n",
      "epoch 423 loss = 1.137126\n",
      "epoch 424 loss = 1.061247\n",
      "epoch 425 loss = 1.336824\n",
      "epoch 426 loss = 1.244951\n",
      "epoch 427 loss = 1.183488\n",
      "epoch 428 loss = 1.167547\n",
      "epoch 429 loss = 1.122141\n",
      "epoch 430 loss = 1.012552\n",
      "epoch 431 loss = 1.150559\n",
      "epoch 432 loss = 1.226769\n",
      "epoch 433 loss = 1.019030\n",
      "epoch 434 loss = 1.169995\n",
      "epoch 435 loss = 1.347505\n",
      "epoch 436 loss = 1.037729\n",
      "epoch 437 loss = 1.007287\n",
      "epoch 438 loss = 1.178783\n",
      "epoch 439 loss = 1.145818\n",
      "epoch 440 loss = 0.965635\n",
      "epoch 441 loss = 1.052927\n",
      "epoch 442 loss = 1.036708\n",
      "epoch 443 loss = 1.150807\n",
      "epoch 444 loss = 0.992131\n",
      "epoch 445 loss = 1.334564\n",
      "epoch 446 loss = 1.128335\n",
      "epoch 447 loss = 1.076944\n",
      "epoch 448 loss = 1.010065\n",
      "epoch 449 loss = 1.071232\n",
      "epoch 450 loss = 1.242721\n",
      "epoch 451 loss = 1.144716\n",
      "epoch 452 loss = 1.134930\n",
      "epoch 453 loss = 1.143501\n",
      "epoch 454 loss = 1.003224\n",
      "epoch 455 loss = 1.264356\n",
      "epoch 456 loss = 1.145849\n",
      "epoch 457 loss = 0.875029\n",
      "epoch 458 loss = 1.206210\n",
      "epoch 459 loss = 0.994736\n",
      "epoch 460 loss = 1.193823\n",
      "epoch 461 loss = 1.041390\n",
      "epoch 462 loss = 1.155206\n",
      "epoch 463 loss = 1.107575\n",
      "epoch 464 loss = 1.167594\n",
      "epoch 465 loss = 0.999957\n",
      "epoch 466 loss = 1.090184\n",
      "epoch 467 loss = 1.032762\n",
      "epoch 468 loss = 1.086587\n",
      "epoch 469 loss = 1.174992\n",
      "epoch 470 loss = 1.073016\n",
      "epoch 471 loss = 1.195397\n",
      "epoch 472 loss = 0.947284\n",
      "epoch 473 loss = 1.096250\n",
      "epoch 474 loss = 1.125216\n",
      "epoch 475 loss = 1.061463\n",
      "epoch 476 loss = 1.043583\n",
      "epoch 477 loss = 1.030633\n",
      "epoch 478 loss = 1.119128\n",
      "epoch 479 loss = 1.086457\n",
      "epoch 480 loss = 0.960839\n",
      "epoch 481 loss = 1.127629\n",
      "epoch 482 loss = 1.292786\n",
      "epoch 483 loss = 1.052155\n",
      "epoch 484 loss = 0.910381\n",
      "epoch 485 loss = 1.017335\n",
      "epoch 486 loss = 1.112723\n",
      "epoch 487 loss = 1.058999\n",
      "epoch 488 loss = 1.256108\n",
      "epoch 489 loss = 0.933179\n",
      "epoch 490 loss = 1.055140\n",
      "epoch 491 loss = 0.959309\n",
      "epoch 492 loss = 0.930233\n",
      "epoch 493 loss = 0.921150\n",
      "epoch 494 loss = 1.021525\n",
      "epoch 495 loss = 1.439905\n",
      "epoch 496 loss = 1.314872\n",
      "epoch 497 loss = 1.073576\n",
      "epoch 498 loss = 1.154774\n",
      "epoch 499 loss = 1.038446\n",
      "final loss = 1.038446\n",
      "accuracy_mc = tensor(0.4434, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4429, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.6681, device='cuda:0')\n",
      "training time = 170.21179270744324 seconds\n",
      "testing time = 1.8389489650726318 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 2.288876\n",
      "epoch 1 loss = 2.229799\n",
      "epoch 2 loss = 2.133081\n",
      "epoch 3 loss = 2.107551\n",
      "epoch 4 loss = 2.085116\n",
      "epoch 5 loss = 1.994239\n",
      "epoch 6 loss = 1.948531\n",
      "epoch 7 loss = 1.991303\n",
      "epoch 8 loss = 1.914928\n",
      "epoch 9 loss = 2.028776\n",
      "epoch 10 loss = 1.963137\n",
      "epoch 11 loss = 2.020159\n",
      "epoch 12 loss = 1.908981\n",
      "epoch 13 loss = 1.887437\n",
      "epoch 14 loss = 1.912096\n",
      "epoch 15 loss = 1.815129\n",
      "epoch 16 loss = 1.863476\n",
      "epoch 17 loss = 1.888679\n",
      "epoch 18 loss = 1.778028\n",
      "epoch 19 loss = 1.771505\n",
      "epoch 20 loss = 1.640925\n",
      "epoch 21 loss = 1.727717\n",
      "epoch 22 loss = 1.721843\n",
      "epoch 23 loss = 1.693334\n",
      "epoch 24 loss = 2.020356\n",
      "epoch 25 loss = 1.670979\n",
      "epoch 26 loss = 1.673663\n",
      "epoch 27 loss = 1.690619\n",
      "epoch 28 loss = 1.685267\n",
      "epoch 29 loss = 1.729063\n",
      "epoch 30 loss = 1.853631\n",
      "epoch 31 loss = 1.590505\n",
      "epoch 32 loss = 1.599324\n",
      "epoch 33 loss = 1.685813\n",
      "epoch 34 loss = 1.615077\n",
      "epoch 35 loss = 1.626592\n",
      "epoch 36 loss = 1.637184\n",
      "epoch 37 loss = 1.524151\n",
      "epoch 38 loss = 1.619667\n",
      "epoch 39 loss = 1.503531\n",
      "epoch 40 loss = 1.654420\n",
      "epoch 41 loss = 1.491287\n",
      "epoch 42 loss = 1.632495\n",
      "epoch 43 loss = 1.501065\n",
      "epoch 44 loss = 1.579380\n",
      "epoch 45 loss = 1.520767\n",
      "epoch 46 loss = 1.563660\n",
      "epoch 47 loss = 1.623495\n",
      "epoch 48 loss = 1.526695\n",
      "epoch 49 loss = 1.630421\n",
      "epoch 50 loss = 1.637157\n",
      "epoch 51 loss = 1.446045\n",
      "epoch 52 loss = 1.482513\n",
      "epoch 53 loss = 1.568468\n",
      "epoch 54 loss = 1.478891\n",
      "epoch 55 loss = 1.517726\n",
      "epoch 56 loss = 1.576572\n",
      "epoch 57 loss = 1.608406\n",
      "epoch 58 loss = 1.556556\n",
      "epoch 59 loss = 1.418684\n",
      "epoch 60 loss = 1.658444\n",
      "epoch 61 loss = 1.398508\n",
      "epoch 62 loss = 1.378436\n",
      "epoch 63 loss = 1.448450\n",
      "epoch 64 loss = 1.424169\n",
      "epoch 65 loss = 1.444969\n",
      "epoch 66 loss = 1.539892\n",
      "epoch 67 loss = 1.538964\n",
      "epoch 68 loss = 1.299197\n",
      "epoch 69 loss = 1.487565\n",
      "epoch 70 loss = 1.359515\n",
      "epoch 71 loss = 1.504989\n",
      "epoch 72 loss = 1.451059\n",
      "epoch 73 loss = 1.668927\n",
      "epoch 74 loss = 1.313428\n",
      "epoch 75 loss = 1.750068\n",
      "epoch 76 loss = 1.598218\n",
      "epoch 77 loss = 1.429677\n",
      "epoch 78 loss = 1.480143\n",
      "epoch 79 loss = 1.500549\n",
      "epoch 80 loss = 1.500888\n",
      "epoch 81 loss = 1.240785\n",
      "epoch 82 loss = 1.287200\n",
      "epoch 83 loss = 1.236401\n",
      "epoch 84 loss = 1.274425\n",
      "epoch 85 loss = 1.408039\n",
      "epoch 86 loss = 1.381908\n",
      "epoch 87 loss = 1.331432\n",
      "epoch 88 loss = 1.323457\n",
      "epoch 89 loss = 1.311321\n",
      "epoch 90 loss = 1.331895\n",
      "epoch 91 loss = 1.286752\n",
      "epoch 92 loss = 1.357380\n",
      "epoch 93 loss = 1.304894\n",
      "epoch 94 loss = 1.371230\n",
      "epoch 95 loss = 1.495210\n",
      "epoch 96 loss = 1.397789\n",
      "epoch 97 loss = 1.413158\n",
      "epoch 98 loss = 1.227826\n",
      "epoch 99 loss = 1.437667\n",
      "epoch 100 loss = 1.611346\n",
      "epoch 101 loss = 1.330058\n",
      "epoch 102 loss = 1.352980\n",
      "epoch 103 loss = 1.236317\n",
      "epoch 104 loss = 1.375605\n",
      "epoch 105 loss = 1.266578\n",
      "epoch 106 loss = 1.269157\n",
      "epoch 107 loss = 1.320988\n",
      "epoch 108 loss = 1.341086\n",
      "epoch 109 loss = 1.493681\n",
      "epoch 110 loss = 1.248426\n",
      "epoch 111 loss = 1.344845\n",
      "epoch 112 loss = 1.538544\n",
      "epoch 113 loss = 1.459660\n",
      "epoch 114 loss = 1.253327\n",
      "epoch 115 loss = 1.393633\n",
      "epoch 116 loss = 1.279688\n",
      "epoch 117 loss = 1.517149\n",
      "epoch 118 loss = 1.230348\n",
      "epoch 119 loss = 1.305782\n",
      "epoch 120 loss = 1.392434\n",
      "epoch 121 loss = 1.385777\n",
      "epoch 122 loss = 1.379555\n",
      "epoch 123 loss = 1.242005\n",
      "epoch 124 loss = 1.344033\n",
      "epoch 125 loss = 1.401842\n",
      "epoch 126 loss = 1.310026\n",
      "epoch 127 loss = 1.265922\n",
      "epoch 128 loss = 1.185126\n",
      "epoch 129 loss = 1.332546\n",
      "epoch 130 loss = 1.137866\n",
      "epoch 131 loss = 1.435921\n",
      "epoch 132 loss = 1.304669\n",
      "epoch 133 loss = 1.318938\n",
      "epoch 134 loss = 1.401998\n",
      "epoch 135 loss = 1.308453\n",
      "epoch 136 loss = 1.239553\n",
      "epoch 137 loss = 1.339067\n",
      "epoch 138 loss = 1.265736\n",
      "epoch 139 loss = 1.267095\n",
      "epoch 140 loss = 1.319995\n",
      "epoch 141 loss = 1.330689\n",
      "epoch 142 loss = 1.222631\n",
      "epoch 143 loss = 1.323697\n",
      "epoch 144 loss = 1.402536\n",
      "epoch 145 loss = 1.384877\n",
      "epoch 146 loss = 1.254313\n",
      "epoch 147 loss = 1.360488\n",
      "epoch 148 loss = 1.300625\n",
      "epoch 149 loss = 1.360257\n",
      "epoch 150 loss = 1.136554\n",
      "epoch 151 loss = 1.238085\n",
      "epoch 152 loss = 1.262683\n",
      "epoch 153 loss = 1.210703\n",
      "epoch 154 loss = 1.381438\n",
      "epoch 155 loss = 1.210418\n",
      "epoch 156 loss = 1.275846\n",
      "epoch 157 loss = 1.439745\n",
      "epoch 158 loss = 1.397825\n",
      "epoch 159 loss = 1.272191\n",
      "epoch 160 loss = 1.410858\n",
      "epoch 161 loss = 1.448891\n",
      "epoch 162 loss = 1.284943\n",
      "epoch 163 loss = 1.378072\n",
      "epoch 164 loss = 1.348899\n",
      "epoch 165 loss = 1.162811\n",
      "epoch 166 loss = 1.308927\n",
      "epoch 167 loss = 1.280500\n",
      "epoch 168 loss = 1.552078\n",
      "epoch 169 loss = 1.373468\n",
      "epoch 170 loss = 1.194605\n",
      "epoch 171 loss = 1.370872\n",
      "epoch 172 loss = 1.301684\n",
      "epoch 173 loss = 1.332274\n",
      "epoch 174 loss = 1.324385\n",
      "epoch 175 loss = 1.343984\n",
      "epoch 176 loss = 1.291468\n",
      "epoch 177 loss = 1.197995\n",
      "epoch 178 loss = 1.326669\n",
      "epoch 179 loss = 1.271122\n",
      "epoch 180 loss = 1.173330\n",
      "epoch 181 loss = 1.191563\n",
      "epoch 182 loss = 1.239857\n",
      "epoch 183 loss = 1.191191\n",
      "epoch 184 loss = 1.315256\n",
      "epoch 185 loss = 1.161208\n",
      "epoch 186 loss = 1.253995\n",
      "epoch 187 loss = 1.164468\n",
      "epoch 188 loss = 1.262450\n",
      "epoch 189 loss = 1.129344\n",
      "epoch 190 loss = 1.133271\n",
      "epoch 191 loss = 1.166927\n",
      "epoch 192 loss = 1.329644\n",
      "epoch 193 loss = 1.403333\n",
      "epoch 194 loss = 1.295478\n",
      "epoch 195 loss = 1.241112\n",
      "epoch 196 loss = 1.231519\n",
      "epoch 197 loss = 1.195784\n",
      "epoch 198 loss = 1.312250\n",
      "epoch 199 loss = 1.278242\n",
      "epoch 200 loss = 1.249591\n",
      "epoch 201 loss = 1.334294\n",
      "epoch 202 loss = 1.400640\n",
      "epoch 203 loss = 1.180900\n",
      "epoch 204 loss = 1.105648\n",
      "epoch 205 loss = 1.416273\n",
      "epoch 206 loss = 1.229932\n",
      "epoch 207 loss = 1.427730\n",
      "epoch 208 loss = 1.261240\n",
      "epoch 209 loss = 1.190287\n",
      "epoch 210 loss = 1.296853\n",
      "epoch 211 loss = 1.085574\n",
      "epoch 212 loss = 1.256614\n",
      "epoch 213 loss = 1.482229\n",
      "epoch 214 loss = 1.417744\n",
      "epoch 215 loss = 1.319150\n",
      "epoch 216 loss = 1.249497\n",
      "epoch 217 loss = 1.146598\n",
      "epoch 218 loss = 1.342460\n",
      "epoch 219 loss = 1.343718\n",
      "epoch 220 loss = 1.210815\n",
      "epoch 221 loss = 1.460446\n",
      "epoch 222 loss = 1.390449\n",
      "epoch 223 loss = 1.384058\n",
      "epoch 224 loss = 1.306244\n",
      "epoch 225 loss = 1.130326\n",
      "epoch 226 loss = 1.247864\n",
      "epoch 227 loss = 1.163188\n",
      "epoch 228 loss = 1.539414\n",
      "epoch 229 loss = 1.193891\n",
      "epoch 230 loss = 1.485552\n",
      "epoch 231 loss = 1.341625\n",
      "epoch 232 loss = 1.239850\n",
      "epoch 233 loss = 1.244963\n",
      "epoch 234 loss = 1.116578\n",
      "epoch 235 loss = 1.259793\n",
      "epoch 236 loss = 1.377641\n",
      "epoch 237 loss = 1.113630\n",
      "epoch 238 loss = 1.261732\n",
      "epoch 239 loss = 1.287233\n",
      "epoch 240 loss = 1.105430\n",
      "epoch 241 loss = 1.208648\n",
      "epoch 242 loss = 1.149647\n",
      "epoch 243 loss = 1.081202\n",
      "epoch 244 loss = 1.321107\n",
      "epoch 245 loss = 1.142975\n",
      "epoch 246 loss = 1.197577\n",
      "epoch 247 loss = 1.338926\n",
      "epoch 248 loss = 1.339043\n",
      "epoch 249 loss = 1.420069\n",
      "epoch 250 loss = 1.128658\n",
      "epoch 251 loss = 1.354909\n",
      "epoch 252 loss = 1.163898\n",
      "epoch 253 loss = 1.113388\n",
      "epoch 254 loss = 1.209015\n",
      "epoch 255 loss = 1.375828\n",
      "epoch 256 loss = 1.279799\n",
      "epoch 257 loss = 1.120958\n",
      "epoch 258 loss = 1.141453\n",
      "epoch 259 loss = 1.175155\n",
      "epoch 260 loss = 1.239699\n",
      "epoch 261 loss = 1.271918\n",
      "epoch 262 loss = 1.366446\n",
      "epoch 263 loss = 1.589077\n",
      "epoch 264 loss = 1.020670\n",
      "epoch 265 loss = 1.376551\n",
      "epoch 266 loss = 1.240044\n",
      "epoch 267 loss = 1.330009\n",
      "epoch 268 loss = 1.305809\n",
      "epoch 269 loss = 1.176888\n",
      "epoch 270 loss = 1.446744\n",
      "epoch 271 loss = 1.227029\n",
      "epoch 272 loss = 1.167760\n",
      "epoch 273 loss = 1.152573\n",
      "epoch 274 loss = 1.184822\n",
      "epoch 275 loss = 1.352895\n",
      "epoch 276 loss = 1.081354\n",
      "epoch 277 loss = 1.518021\n",
      "epoch 278 loss = 1.180802\n",
      "epoch 279 loss = 1.202358\n",
      "epoch 280 loss = 1.093876\n",
      "epoch 281 loss = 1.050195\n",
      "epoch 282 loss = 1.244174\n",
      "epoch 283 loss = 1.300873\n",
      "epoch 284 loss = 1.279191\n",
      "epoch 285 loss = 1.126072\n",
      "epoch 286 loss = 1.167351\n",
      "epoch 287 loss = 1.144203\n",
      "epoch 288 loss = 1.063823\n",
      "epoch 289 loss = 1.222149\n",
      "epoch 290 loss = 1.142962\n",
      "epoch 291 loss = 1.290699\n",
      "epoch 292 loss = 1.096564\n",
      "epoch 293 loss = 1.350156\n",
      "epoch 294 loss = 1.266193\n",
      "epoch 295 loss = 1.238970\n",
      "epoch 296 loss = 1.146199\n",
      "epoch 297 loss = 1.180685\n",
      "epoch 298 loss = 1.312843\n",
      "epoch 299 loss = 1.274976\n",
      "epoch 300 loss = 1.153362\n",
      "epoch 301 loss = 0.984246\n",
      "epoch 302 loss = 1.175019\n",
      "epoch 303 loss = 1.507811\n",
      "epoch 304 loss = 1.413597\n",
      "epoch 305 loss = 1.425681\n",
      "epoch 306 loss = 1.017996\n",
      "epoch 307 loss = 1.186741\n",
      "epoch 308 loss = 1.242950\n",
      "epoch 309 loss = 1.014247\n",
      "epoch 310 loss = 1.304729\n",
      "epoch 311 loss = 1.139606\n",
      "epoch 312 loss = 1.208123\n",
      "epoch 313 loss = 1.287116\n",
      "epoch 314 loss = 1.314367\n",
      "epoch 315 loss = 1.331335\n",
      "epoch 316 loss = 1.367712\n",
      "epoch 317 loss = 1.153257\n",
      "epoch 318 loss = 1.215455\n",
      "epoch 319 loss = 1.205015\n",
      "epoch 320 loss = 1.171811\n",
      "epoch 321 loss = 1.097273\n",
      "epoch 322 loss = 1.276338\n",
      "epoch 323 loss = 1.102584\n",
      "epoch 324 loss = 1.177165\n",
      "epoch 325 loss = 1.308841\n",
      "epoch 326 loss = 1.097382\n",
      "epoch 327 loss = 1.035946\n",
      "epoch 328 loss = 1.252596\n",
      "epoch 329 loss = 1.044526\n",
      "epoch 330 loss = 0.967269\n",
      "epoch 331 loss = 1.225836\n",
      "epoch 332 loss = 1.296749\n",
      "epoch 333 loss = 1.262228\n",
      "epoch 334 loss = 1.129212\n",
      "epoch 335 loss = 1.196799\n",
      "epoch 336 loss = 1.084284\n",
      "epoch 337 loss = 1.126886\n",
      "epoch 338 loss = 1.108102\n",
      "epoch 339 loss = 1.176009\n",
      "epoch 340 loss = 1.074979\n",
      "epoch 341 loss = 0.982784\n",
      "epoch 342 loss = 0.995409\n",
      "epoch 343 loss = 1.141821\n",
      "epoch 344 loss = 1.153821\n",
      "epoch 345 loss = 1.116801\n",
      "epoch 346 loss = 1.234552\n",
      "epoch 347 loss = 1.069755\n",
      "epoch 348 loss = 1.303828\n",
      "epoch 349 loss = 1.066276\n",
      "epoch 350 loss = 1.254745\n",
      "epoch 351 loss = 1.067378\n",
      "epoch 352 loss = 1.503376\n",
      "epoch 353 loss = 1.192229\n",
      "epoch 354 loss = 1.207501\n",
      "epoch 355 loss = 1.055089\n",
      "epoch 356 loss = 1.041925\n",
      "epoch 357 loss = 1.151960\n",
      "epoch 358 loss = 1.000120\n",
      "epoch 359 loss = 1.315343\n",
      "epoch 360 loss = 1.092080\n",
      "epoch 361 loss = 1.165956\n",
      "epoch 362 loss = 1.192626\n",
      "epoch 363 loss = 1.167676\n",
      "epoch 364 loss = 1.159625\n",
      "epoch 365 loss = 1.084880\n",
      "epoch 366 loss = 1.027286\n",
      "epoch 367 loss = 1.021883\n",
      "epoch 368 loss = 1.132647\n",
      "epoch 369 loss = 1.082919\n",
      "epoch 370 loss = 1.044479\n",
      "epoch 371 loss = 1.036180\n",
      "epoch 372 loss = 1.130727\n",
      "epoch 373 loss = 1.006195\n",
      "epoch 374 loss = 1.150816\n",
      "epoch 375 loss = 1.035518\n",
      "epoch 376 loss = 0.936857\n",
      "epoch 377 loss = 1.067574\n",
      "epoch 378 loss = 0.923027\n",
      "epoch 379 loss = 1.004016\n",
      "epoch 380 loss = 1.276396\n",
      "epoch 381 loss = 1.095022\n",
      "epoch 382 loss = 1.159241\n",
      "epoch 383 loss = 1.154837\n",
      "epoch 384 loss = 0.963183\n",
      "epoch 385 loss = 0.970282\n",
      "epoch 386 loss = 1.008554\n",
      "epoch 387 loss = 1.191486\n",
      "epoch 388 loss = 1.222019\n",
      "epoch 389 loss = 0.932023\n",
      "epoch 390 loss = 1.336244\n",
      "epoch 391 loss = 0.994537\n",
      "epoch 392 loss = 1.031166\n",
      "epoch 393 loss = 1.175590\n",
      "epoch 394 loss = 0.969606\n",
      "epoch 395 loss = 1.087682\n",
      "epoch 396 loss = 1.005477\n",
      "epoch 397 loss = 1.025248\n",
      "epoch 398 loss = 1.082365\n",
      "epoch 399 loss = 0.979873\n",
      "epoch 400 loss = 1.007293\n",
      "epoch 401 loss = 1.069086\n",
      "epoch 402 loss = 1.187857\n",
      "epoch 403 loss = 0.951949\n",
      "epoch 404 loss = 0.923842\n",
      "epoch 405 loss = 0.815085\n",
      "epoch 406 loss = 0.974533\n",
      "epoch 407 loss = 1.089907\n",
      "epoch 408 loss = 1.116599\n",
      "epoch 409 loss = 0.871403\n",
      "epoch 410 loss = 0.962644\n",
      "epoch 411 loss = 0.857411\n",
      "epoch 412 loss = 1.070675\n",
      "epoch 413 loss = 1.053620\n",
      "epoch 414 loss = 1.004294\n",
      "epoch 415 loss = 1.056156\n",
      "epoch 416 loss = 1.170064\n",
      "epoch 417 loss = 1.113353\n",
      "epoch 418 loss = 1.003057\n",
      "epoch 419 loss = 0.981954\n",
      "epoch 420 loss = 1.043357\n",
      "epoch 421 loss = 1.029276\n",
      "epoch 422 loss = 0.968529\n",
      "epoch 423 loss = 0.932348\n",
      "epoch 424 loss = 0.931918\n",
      "epoch 425 loss = 0.973873\n",
      "epoch 426 loss = 0.922419\n",
      "epoch 427 loss = 1.126819\n",
      "epoch 428 loss = 0.995614\n",
      "epoch 429 loss = 0.891595\n",
      "epoch 430 loss = 1.012841\n",
      "epoch 431 loss = 0.871164\n",
      "epoch 432 loss = 0.886408\n",
      "epoch 433 loss = 0.917778\n",
      "epoch 434 loss = 1.164481\n",
      "epoch 435 loss = 1.045395\n",
      "epoch 436 loss = 0.857164\n",
      "epoch 437 loss = 1.220377\n",
      "epoch 438 loss = 1.199362\n",
      "epoch 439 loss = 1.008898\n",
      "epoch 440 loss = 1.054135\n",
      "epoch 441 loss = 1.063236\n",
      "epoch 442 loss = 0.824227\n",
      "epoch 443 loss = 0.828174\n",
      "epoch 444 loss = 0.888259\n",
      "epoch 445 loss = 1.200474\n",
      "epoch 446 loss = 0.909535\n",
      "epoch 447 loss = 0.996863\n",
      "epoch 448 loss = 0.953960\n",
      "epoch 449 loss = 0.877136\n",
      "epoch 450 loss = 0.843778\n",
      "epoch 451 loss = 1.113452\n",
      "epoch 452 loss = 1.072201\n",
      "epoch 453 loss = 1.037141\n",
      "epoch 454 loss = 1.008289\n",
      "epoch 455 loss = 0.893970\n",
      "epoch 456 loss = 1.033448\n",
      "epoch 457 loss = 1.284365\n",
      "epoch 458 loss = 0.930267\n",
      "epoch 459 loss = 0.908001\n",
      "epoch 460 loss = 0.803903\n",
      "epoch 461 loss = 0.844391\n",
      "epoch 462 loss = 0.918380\n",
      "epoch 463 loss = 0.935696\n",
      "epoch 464 loss = 1.139064\n",
      "epoch 465 loss = 1.027455\n",
      "epoch 466 loss = 0.879516\n",
      "epoch 467 loss = 1.002574\n",
      "epoch 468 loss = 0.824027\n",
      "epoch 469 loss = 1.065585\n",
      "epoch 470 loss = 1.073033\n",
      "epoch 471 loss = 0.877457\n",
      "epoch 472 loss = 0.884545\n",
      "epoch 473 loss = 0.945369\n",
      "epoch 474 loss = 0.664053\n",
      "epoch 475 loss = 0.878606\n",
      "epoch 476 loss = 0.811258\n",
      "epoch 477 loss = 0.904480\n",
      "epoch 478 loss = 1.054882\n",
      "epoch 479 loss = 0.889330\n",
      "epoch 480 loss = 0.814069\n",
      "epoch 481 loss = 1.022568\n",
      "epoch 482 loss = 0.942164\n",
      "epoch 483 loss = 0.972295\n",
      "epoch 484 loss = 0.857408\n",
      "epoch 485 loss = 0.880247\n",
      "epoch 486 loss = 0.891010\n",
      "epoch 487 loss = 0.695657\n",
      "epoch 488 loss = 0.867071\n",
      "epoch 489 loss = 0.941111\n",
      "epoch 490 loss = 0.841012\n",
      "epoch 491 loss = 1.023637\n",
      "epoch 492 loss = 0.904129\n",
      "epoch 493 loss = 1.024688\n",
      "epoch 494 loss = 1.046504\n",
      "epoch 495 loss = 0.860148\n",
      "epoch 496 loss = 0.903929\n",
      "epoch 497 loss = 1.016170\n",
      "epoch 498 loss = 0.773998\n",
      "epoch 499 loss = 0.857285\n",
      "final loss = 0.857285\n",
      "accuracy_mc = tensor(0.4781, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.4652, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.4827, device='cuda:0')\n",
      "training time = 170.22980856895447 seconds\n",
      "testing time = 1.8543331623077393 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.500000, reg_strength 0.050000\n",
      "n_epoch 10\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.325371\n",
      "epoch 1 loss = 2.275664\n",
      "epoch 2 loss = 2.261942\n",
      "epoch 3 loss = 2.287587\n",
      "epoch 4 loss = 2.193944\n",
      "epoch 5 loss = 2.309253\n",
      "epoch 6 loss = 2.200701\n",
      "epoch 7 loss = 2.343623\n",
      "epoch 8 loss = 2.217948\n",
      "epoch 9 loss = 2.221842\n",
      "final loss = 2.221842\n",
      "accuracy_mc = tensor(0.2392, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2350, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1320, device='cuda:0')\n",
      "training time = 3.3762574195861816 seconds\n",
      "testing time = 1.9145233631134033 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.352610\n",
      "epoch 1 loss = 2.296514\n",
      "epoch 2 loss = 2.233983\n",
      "epoch 3 loss = 2.196598\n",
      "epoch 4 loss = 2.319817\n",
      "epoch 5 loss = 2.211834\n",
      "epoch 6 loss = 2.265304\n",
      "epoch 7 loss = 2.179981\n",
      "epoch 8 loss = 2.332457\n",
      "epoch 9 loss = 2.024220\n",
      "final loss = 2.024220\n",
      "accuracy_mc = tensor(0.2361, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2366, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0983, device='cuda:0')\n",
      "training time = 3.4485223293304443 seconds\n",
      "testing time = 1.8482801914215088 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.285503\n",
      "epoch 1 loss = 2.251692\n",
      "epoch 2 loss = 2.319532\n",
      "epoch 3 loss = 2.297616\n",
      "epoch 4 loss = 2.272619\n",
      "epoch 5 loss = 2.267446\n",
      "epoch 6 loss = 2.273120\n",
      "epoch 7 loss = 2.343847\n",
      "epoch 8 loss = 2.246828\n",
      "epoch 9 loss = 2.287887\n",
      "final loss = 2.287887\n",
      "accuracy_mc = tensor(0.1565, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1207, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.2441, device='cuda:0')\n",
      "training time = 3.379430055618286 seconds\n",
      "testing time = 1.8615198135375977 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.255814\n",
      "epoch 1 loss = 2.317859\n",
      "epoch 2 loss = 2.298298\n",
      "epoch 3 loss = 2.267058\n",
      "epoch 4 loss = 2.312243\n",
      "epoch 5 loss = 2.122897\n",
      "epoch 6 loss = 2.111910\n",
      "epoch 7 loss = 2.067555\n",
      "epoch 8 loss = 2.146199\n",
      "epoch 9 loss = 2.156569\n",
      "final loss = 2.156569\n",
      "accuracy_mc = tensor(0.1599, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2083, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1670, device='cuda:0')\n",
      "training time = 3.4202449321746826 seconds\n",
      "testing time = 1.8681316375732422 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 2.323317\n",
      "epoch 1 loss = 2.277745\n",
      "epoch 2 loss = 2.291071\n",
      "epoch 3 loss = 2.251792\n",
      "epoch 4 loss = 2.248883\n",
      "epoch 5 loss = 2.320579\n",
      "epoch 6 loss = 2.203563\n",
      "epoch 7 loss = 2.174332\n",
      "epoch 8 loss = 2.039400\n",
      "epoch 9 loss = 2.209030\n",
      "final loss = 2.209030\n",
      "accuracy_mc = tensor(0.2931, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2096, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1672, device='cuda:0')\n",
      "training time = 3.382770538330078 seconds\n",
      "testing time = 1.8660287857055664 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.305207\n",
      "epoch 1 loss = 2.315256\n",
      "epoch 2 loss = 2.317774\n",
      "epoch 3 loss = 2.344672\n",
      "epoch 4 loss = 2.257914\n",
      "epoch 5 loss = 2.287606\n",
      "epoch 6 loss = 2.251451\n",
      "epoch 7 loss = 2.334167\n",
      "epoch 8 loss = 2.213361\n",
      "epoch 9 loss = 2.237363\n",
      "final loss = 2.237363\n",
      "accuracy_mc = tensor(0.2194, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1713, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1711, device='cuda:0')\n",
      "training time = 3.3801090717315674 seconds\n",
      "testing time = 1.8442192077636719 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.338800\n",
      "epoch 1 loss = 2.305536\n",
      "epoch 2 loss = 2.170655\n",
      "epoch 3 loss = 2.250287\n",
      "epoch 4 loss = 2.267608\n",
      "epoch 5 loss = 2.211610\n",
      "epoch 6 loss = 2.266095\n",
      "epoch 7 loss = 2.361132\n",
      "epoch 8 loss = 2.122583\n",
      "epoch 9 loss = 2.144185\n",
      "final loss = 2.144185\n",
      "accuracy_mc = tensor(0.2266, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1696, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1690, device='cuda:0')\n",
      "training time = 3.441697597503662 seconds\n",
      "testing time = 1.8601713180541992 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.342962\n",
      "epoch 1 loss = 2.297409\n",
      "epoch 2 loss = 2.253801\n",
      "epoch 3 loss = 2.218985\n",
      "epoch 4 loss = 2.133530\n",
      "epoch 5 loss = 2.161247\n",
      "epoch 6 loss = 2.138988\n",
      "epoch 7 loss = 2.239245\n",
      "epoch 8 loss = 2.116755\n",
      "epoch 9 loss = 2.187771\n",
      "final loss = 2.187771\n",
      "accuracy_mc = tensor(0.2508, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1930, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1363, device='cuda:0')\n",
      "training time = 3.434798002243042 seconds\n",
      "testing time = 1.8320996761322021 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.290194\n",
      "epoch 1 loss = 2.283997\n",
      "epoch 2 loss = 2.276047\n",
      "epoch 3 loss = 2.253184\n",
      "epoch 4 loss = 2.282996\n",
      "epoch 5 loss = 2.301058\n",
      "epoch 6 loss = 2.266822\n",
      "epoch 7 loss = 2.148653\n",
      "epoch 8 loss = 2.234403\n",
      "epoch 9 loss = 2.164479\n",
      "final loss = 2.164479\n",
      "accuracy_mc = tensor(0.2513, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1530, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1754, device='cuda:0')\n",
      "training time = 3.384329319000244 seconds\n",
      "testing time = 1.892324447631836 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 2.322485\n",
      "epoch 1 loss = 2.332437\n",
      "epoch 2 loss = 2.205849\n",
      "epoch 3 loss = 2.250399\n",
      "epoch 4 loss = 2.228384\n",
      "epoch 5 loss = 2.138718\n",
      "epoch 6 loss = 2.304221\n",
      "epoch 7 loss = 2.140682\n",
      "epoch 8 loss = 2.281803\n",
      "epoch 9 loss = 2.107690\n",
      "final loss = 2.107690\n",
      "accuracy_mc = tensor(0.2124, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2122, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.1333, device='cuda:0')\n",
      "training time = 3.452639579772949 seconds\n",
      "testing time = 1.8800182342529297 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.500000, reg_strength 0.050000\n",
      "n_epoch 100\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.325371\n",
      "epoch 1 loss = 2.275664\n",
      "epoch 2 loss = 2.261942\n",
      "epoch 3 loss = 2.287587\n",
      "epoch 4 loss = 2.193944\n",
      "epoch 5 loss = 2.309253\n",
      "epoch 6 loss = 2.200701\n",
      "epoch 7 loss = 2.343623\n",
      "epoch 8 loss = 2.217948\n",
      "epoch 9 loss = 2.221842\n",
      "epoch 10 loss = 2.201942\n",
      "epoch 11 loss = 2.294660\n",
      "epoch 12 loss = 2.276495\n",
      "epoch 13 loss = 2.293447\n",
      "epoch 14 loss = 2.250941\n",
      "epoch 15 loss = 2.211240\n",
      "epoch 16 loss = 2.247408\n",
      "epoch 17 loss = 2.032119\n",
      "epoch 18 loss = 2.236619\n",
      "epoch 19 loss = 2.160795\n",
      "epoch 20 loss = 2.172107\n",
      "epoch 21 loss = 2.272074\n",
      "epoch 22 loss = 2.223838\n",
      "epoch 23 loss = 2.226907\n",
      "epoch 24 loss = 2.140364\n",
      "epoch 25 loss = 2.296629\n",
      "epoch 26 loss = 2.035148\n",
      "epoch 27 loss = 2.329648\n",
      "epoch 28 loss = 2.201749\n",
      "epoch 29 loss = 2.219563\n",
      "epoch 30 loss = 2.287009\n",
      "epoch 31 loss = 2.258601\n",
      "epoch 32 loss = 2.180708\n",
      "epoch 33 loss = 2.242010\n",
      "epoch 34 loss = 2.160405\n",
      "epoch 35 loss = 2.232176\n",
      "epoch 36 loss = 2.148942\n",
      "epoch 37 loss = 2.218006\n",
      "epoch 38 loss = 2.288918\n",
      "epoch 39 loss = 2.202667\n",
      "epoch 40 loss = 2.202408\n",
      "epoch 41 loss = 2.126400\n",
      "epoch 42 loss = 2.121985\n",
      "epoch 43 loss = 2.239499\n",
      "epoch 44 loss = 2.078190\n",
      "epoch 45 loss = 2.058232\n",
      "epoch 46 loss = 2.025114\n",
      "epoch 47 loss = 2.264864\n",
      "epoch 48 loss = 2.336934\n",
      "epoch 49 loss = 2.187485\n",
      "epoch 50 loss = 2.047389\n",
      "epoch 51 loss = 2.024281\n",
      "epoch 52 loss = 2.178514\n",
      "epoch 53 loss = 2.323046\n",
      "epoch 54 loss = 2.305793\n",
      "epoch 55 loss = 2.180195\n",
      "epoch 56 loss = 2.159059\n",
      "epoch 57 loss = 2.188448\n",
      "epoch 58 loss = 2.311116\n",
      "epoch 59 loss = 2.174184\n",
      "epoch 60 loss = 2.154535\n",
      "epoch 61 loss = 2.191578\n",
      "epoch 62 loss = 2.245034\n",
      "epoch 63 loss = 2.203607\n",
      "epoch 64 loss = 2.161890\n",
      "epoch 65 loss = 2.188503\n",
      "epoch 66 loss = 2.114487\n",
      "epoch 67 loss = 2.102676\n",
      "epoch 68 loss = 2.307245\n",
      "epoch 69 loss = 2.168669\n",
      "epoch 70 loss = 2.134252\n",
      "epoch 71 loss = 2.172695\n",
      "epoch 72 loss = 2.065642\n",
      "epoch 73 loss = 2.264936\n",
      "epoch 74 loss = 2.182493\n",
      "epoch 75 loss = 2.193332\n",
      "epoch 76 loss = 2.112833\n",
      "epoch 77 loss = 2.091846\n",
      "epoch 78 loss = 2.316846\n",
      "epoch 79 loss = 2.174633\n",
      "epoch 80 loss = 2.225368\n",
      "epoch 81 loss = 2.195183\n",
      "epoch 82 loss = 2.201903\n",
      "epoch 83 loss = 2.214846\n",
      "epoch 84 loss = 2.153491\n",
      "epoch 85 loss = 2.034606\n",
      "epoch 86 loss = 2.029563\n",
      "epoch 87 loss = 2.156196\n",
      "epoch 88 loss = 2.219522\n",
      "epoch 89 loss = 2.126483\n",
      "epoch 90 loss = 2.180094\n",
      "epoch 91 loss = 1.932569\n",
      "epoch 92 loss = 2.286063\n",
      "epoch 93 loss = 2.080154\n",
      "epoch 94 loss = 2.102141\n",
      "epoch 95 loss = 2.154418\n",
      "epoch 96 loss = 2.127085\n",
      "epoch 97 loss = 2.281439\n",
      "epoch 98 loss = 2.334756\n",
      "epoch 99 loss = 1.946298\n",
      "final loss = 1.946298\n",
      "accuracy_mc = tensor(0.2076, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2214, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0448, device='cuda:0')\n",
      "training time = 33.74147033691406 seconds\n",
      "testing time = 1.8504033088684082 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.285632\n",
      "epoch 1 loss = 2.281922\n",
      "epoch 2 loss = 2.299926\n",
      "epoch 3 loss = 2.198039\n",
      "epoch 4 loss = 2.271649\n",
      "epoch 5 loss = 2.270951\n",
      "epoch 6 loss = 2.217457\n",
      "epoch 7 loss = 2.209407\n",
      "epoch 8 loss = 2.053155\n",
      "epoch 9 loss = 2.186987\n",
      "epoch 10 loss = 2.235028\n",
      "epoch 11 loss = 2.261520\n",
      "epoch 12 loss = 2.163648\n",
      "epoch 13 loss = 2.020579\n",
      "epoch 14 loss = 2.200145\n",
      "epoch 15 loss = 2.250863\n",
      "epoch 16 loss = 2.192030\n",
      "epoch 17 loss = 2.206328\n",
      "epoch 18 loss = 2.233612\n",
      "epoch 19 loss = 2.060323\n",
      "epoch 20 loss = 2.124842\n",
      "epoch 21 loss = 2.140511\n",
      "epoch 22 loss = 1.964161\n",
      "epoch 23 loss = 1.985595\n",
      "epoch 24 loss = 2.153860\n",
      "epoch 25 loss = 2.272504\n",
      "epoch 26 loss = 2.101988\n",
      "epoch 27 loss = 2.125078\n",
      "epoch 28 loss = 2.353096\n",
      "epoch 29 loss = 2.261855\n",
      "epoch 30 loss = 1.903553\n",
      "epoch 31 loss = 1.995309\n",
      "epoch 32 loss = 1.962143\n",
      "epoch 33 loss = 2.017803\n",
      "epoch 34 loss = 2.204720\n",
      "epoch 35 loss = 2.325053\n",
      "epoch 36 loss = 2.162708\n",
      "epoch 37 loss = 2.156600\n",
      "epoch 38 loss = 2.041512\n",
      "epoch 39 loss = 1.840126\n",
      "epoch 40 loss = 2.001477\n",
      "epoch 41 loss = 2.190718\n",
      "epoch 42 loss = 2.108561\n",
      "epoch 43 loss = 2.069386\n",
      "epoch 44 loss = 2.023057\n",
      "epoch 45 loss = 2.024730\n",
      "epoch 46 loss = 1.987382\n",
      "epoch 47 loss = 1.909183\n",
      "epoch 48 loss = 2.174579\n",
      "epoch 49 loss = 2.144864\n",
      "epoch 50 loss = 2.264694\n",
      "epoch 51 loss = 1.970626\n",
      "epoch 52 loss = 1.847157\n",
      "epoch 53 loss = 1.899113\n",
      "epoch 54 loss = 2.073250\n",
      "epoch 55 loss = 2.117010\n",
      "epoch 56 loss = 2.210066\n",
      "epoch 57 loss = 2.140099\n",
      "epoch 58 loss = 1.956185\n",
      "epoch 59 loss = 2.148985\n",
      "epoch 60 loss = 1.909857\n",
      "epoch 61 loss = 2.077723\n",
      "epoch 62 loss = 1.934729\n",
      "epoch 63 loss = 2.194183\n",
      "epoch 64 loss = 1.869603\n",
      "epoch 65 loss = 2.051923\n",
      "epoch 66 loss = 1.943447\n",
      "epoch 67 loss = 1.988512\n",
      "epoch 68 loss = 2.034176\n",
      "epoch 69 loss = 1.949197\n",
      "epoch 70 loss = 2.154940\n",
      "epoch 71 loss = 2.055398\n",
      "epoch 72 loss = 1.722584\n",
      "epoch 73 loss = 1.784563\n",
      "epoch 74 loss = 1.836296\n",
      "epoch 75 loss = 1.937554\n",
      "epoch 76 loss = 1.994997\n",
      "epoch 77 loss = 2.060256\n",
      "epoch 78 loss = 2.132907\n",
      "epoch 79 loss = 2.010340\n",
      "epoch 80 loss = 2.075242\n",
      "epoch 81 loss = 2.093562\n",
      "epoch 82 loss = 2.005404\n",
      "epoch 83 loss = 1.831311\n",
      "epoch 84 loss = 1.958752\n",
      "epoch 85 loss = 2.098830\n",
      "epoch 86 loss = 2.113685\n",
      "epoch 87 loss = 1.994578\n",
      "epoch 88 loss = 1.929670\n",
      "epoch 89 loss = 1.952588\n",
      "epoch 90 loss = 1.949236\n",
      "epoch 91 loss = 2.001231\n",
      "epoch 92 loss = 1.876248\n",
      "epoch 93 loss = 1.946023\n",
      "epoch 94 loss = 2.020032\n",
      "epoch 95 loss = 1.890593\n",
      "epoch 96 loss = 1.870872\n",
      "epoch 97 loss = 1.835921\n",
      "epoch 98 loss = 1.837214\n",
      "epoch 99 loss = 2.026781\n",
      "final loss = 2.026781\n",
      "accuracy_mc = tensor(0.2871, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2632, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9749, device='cuda:0')\n",
      "training time = 34.13119411468506 seconds\n",
      "testing time = 1.9029343128204346 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.279895\n",
      "epoch 1 loss = 2.329687\n",
      "epoch 2 loss = 2.293303\n",
      "epoch 3 loss = 2.251691\n",
      "epoch 4 loss = 2.273385\n",
      "epoch 5 loss = 2.229998\n",
      "epoch 6 loss = 2.326510\n",
      "epoch 7 loss = 2.265797\n",
      "epoch 8 loss = 2.201902\n",
      "epoch 9 loss = 2.256742\n",
      "epoch 10 loss = 2.133485\n",
      "epoch 11 loss = 2.151142\n",
      "epoch 12 loss = 2.227168\n",
      "epoch 13 loss = 2.111438\n",
      "epoch 14 loss = 2.132820\n",
      "epoch 15 loss = 2.223687\n",
      "epoch 16 loss = 2.157333\n",
      "epoch 17 loss = 2.218924\n",
      "epoch 18 loss = 2.103987\n",
      "epoch 19 loss = 2.033670\n",
      "epoch 20 loss = 2.128780\n",
      "epoch 21 loss = 2.173657\n",
      "epoch 22 loss = 2.157446\n",
      "epoch 23 loss = 2.097491\n",
      "epoch 24 loss = 2.209135\n",
      "epoch 25 loss = 1.989919\n",
      "epoch 26 loss = 2.102923\n",
      "epoch 27 loss = 2.049835\n",
      "epoch 28 loss = 1.870320\n",
      "epoch 29 loss = 2.121360\n",
      "epoch 30 loss = 2.274914\n",
      "epoch 31 loss = 2.017192\n",
      "epoch 32 loss = 2.154924\n",
      "epoch 33 loss = 2.133601\n",
      "epoch 34 loss = 2.021894\n",
      "epoch 35 loss = 2.102885\n",
      "epoch 36 loss = 2.022766\n",
      "epoch 37 loss = 2.061717\n",
      "epoch 38 loss = 2.088986\n",
      "epoch 39 loss = 2.032619\n",
      "epoch 40 loss = 2.030617\n",
      "epoch 41 loss = 2.058522\n",
      "epoch 42 loss = 2.073908\n",
      "epoch 43 loss = 2.021943\n",
      "epoch 44 loss = 2.170803\n",
      "epoch 45 loss = 2.153503\n",
      "epoch 46 loss = 2.027723\n",
      "epoch 47 loss = 2.169846\n",
      "epoch 48 loss = 2.107742\n",
      "epoch 49 loss = 1.910563\n",
      "epoch 50 loss = 2.148923\n",
      "epoch 51 loss = 2.055211\n",
      "epoch 52 loss = 2.202850\n",
      "epoch 53 loss = 2.107059\n",
      "epoch 54 loss = 2.149388\n",
      "epoch 55 loss = 2.040239\n",
      "epoch 56 loss = 2.220219\n",
      "epoch 57 loss = 2.211550\n",
      "epoch 58 loss = 2.025331\n",
      "epoch 59 loss = 2.225414\n",
      "epoch 60 loss = 2.167975\n",
      "epoch 61 loss = 2.095562\n",
      "epoch 62 loss = 2.067338\n",
      "epoch 63 loss = 2.201368\n",
      "epoch 64 loss = 1.971812\n",
      "epoch 65 loss = 2.065458\n",
      "epoch 66 loss = 2.128961\n",
      "epoch 67 loss = 1.996719\n",
      "epoch 68 loss = 1.984993\n",
      "epoch 69 loss = 1.998881\n",
      "epoch 70 loss = 2.223591\n",
      "epoch 71 loss = 2.201031\n",
      "epoch 72 loss = 2.151462\n",
      "epoch 73 loss = 2.081873\n",
      "epoch 74 loss = 1.943082\n",
      "epoch 75 loss = 2.141836\n",
      "epoch 76 loss = 2.066736\n",
      "epoch 77 loss = 2.110143\n",
      "epoch 78 loss = 2.134413\n",
      "epoch 79 loss = 2.140894\n",
      "epoch 80 loss = 2.074781\n",
      "epoch 81 loss = 2.068240\n",
      "epoch 82 loss = 2.291869\n",
      "epoch 83 loss = 2.035125\n",
      "epoch 84 loss = 2.043362\n",
      "epoch 85 loss = 1.980839\n",
      "epoch 86 loss = 2.044416\n",
      "epoch 87 loss = 2.113094\n",
      "epoch 88 loss = 2.086234\n",
      "epoch 89 loss = 2.184423\n",
      "epoch 90 loss = 1.985665\n",
      "epoch 91 loss = 1.886226\n",
      "epoch 92 loss = 1.934227\n",
      "epoch 93 loss = 1.818286\n",
      "epoch 94 loss = 2.110911\n",
      "epoch 95 loss = 1.942930\n",
      "epoch 96 loss = 2.045899\n",
      "epoch 97 loss = 1.962531\n",
      "epoch 98 loss = 2.138308\n",
      "epoch 99 loss = 2.093950\n",
      "final loss = 2.093950\n",
      "accuracy_mc = tensor(0.2517, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1786, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9954, device='cuda:0')\n",
      "training time = 34.377309799194336 seconds\n",
      "testing time = 1.8464343547821045 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.287458\n",
      "epoch 1 loss = 2.249255\n",
      "epoch 2 loss = 2.186338\n",
      "epoch 3 loss = 2.159133\n",
      "epoch 4 loss = 2.245225\n",
      "epoch 5 loss = 2.249495\n",
      "epoch 6 loss = 2.016197\n",
      "epoch 7 loss = 2.027463\n",
      "epoch 8 loss = 2.209915\n",
      "epoch 9 loss = 2.111543\n",
      "epoch 10 loss = 2.156739\n",
      "epoch 11 loss = 2.001503\n",
      "epoch 12 loss = 1.923235\n",
      "epoch 13 loss = 2.065910\n",
      "epoch 14 loss = 2.278031\n",
      "epoch 15 loss = 1.900157\n",
      "epoch 16 loss = 1.984676\n",
      "epoch 17 loss = 2.172906\n",
      "epoch 18 loss = 1.888015\n",
      "epoch 19 loss = 2.044503\n",
      "epoch 20 loss = 2.087705\n",
      "epoch 21 loss = 1.933482\n",
      "epoch 22 loss = 2.065685\n",
      "epoch 23 loss = 1.985871\n",
      "epoch 24 loss = 1.946921\n",
      "epoch 25 loss = 2.069720\n",
      "epoch 26 loss = 1.962700\n",
      "epoch 27 loss = 2.042518\n",
      "epoch 28 loss = 2.099417\n",
      "epoch 29 loss = 2.010606\n",
      "epoch 30 loss = 1.992310\n",
      "epoch 31 loss = 1.985132\n",
      "epoch 32 loss = 2.064444\n",
      "epoch 33 loss = 2.117126\n",
      "epoch 34 loss = 1.943823\n",
      "epoch 35 loss = 2.019368\n",
      "epoch 36 loss = 1.975566\n",
      "epoch 37 loss = 1.966458\n",
      "epoch 38 loss = 2.034422\n",
      "epoch 39 loss = 1.951187\n",
      "epoch 40 loss = 1.933654\n",
      "epoch 41 loss = 2.089683\n",
      "epoch 42 loss = 2.088282\n",
      "epoch 43 loss = 1.971923\n",
      "epoch 44 loss = 2.085000\n",
      "epoch 45 loss = 1.959314\n",
      "epoch 46 loss = 1.910652\n",
      "epoch 47 loss = 2.095483\n",
      "epoch 48 loss = 2.118359\n",
      "epoch 49 loss = 2.132740\n",
      "epoch 50 loss = 1.850158\n",
      "epoch 51 loss = 2.049172\n",
      "epoch 52 loss = 1.847481\n",
      "epoch 53 loss = 1.891469\n",
      "epoch 54 loss = 1.898053\n",
      "epoch 55 loss = 1.856603\n",
      "epoch 56 loss = 2.026407\n",
      "epoch 57 loss = 2.238051\n",
      "epoch 58 loss = 2.110084\n",
      "epoch 59 loss = 2.035083\n",
      "epoch 60 loss = 2.112700\n",
      "epoch 61 loss = 2.001807\n",
      "epoch 62 loss = 2.061759\n",
      "epoch 63 loss = 2.025682\n",
      "epoch 64 loss = 2.041102\n",
      "epoch 65 loss = 2.028983\n",
      "epoch 66 loss = 1.956403\n",
      "epoch 67 loss = 1.958604\n",
      "epoch 68 loss = 1.998093\n",
      "epoch 69 loss = 1.984543\n",
      "epoch 70 loss = 1.878091\n",
      "epoch 71 loss = 2.111593\n",
      "epoch 72 loss = 2.091407\n",
      "epoch 73 loss = 1.880828\n",
      "epoch 74 loss = 2.093738\n",
      "epoch 75 loss = 2.024760\n",
      "epoch 76 loss = 1.947104\n",
      "epoch 77 loss = 2.050853\n",
      "epoch 78 loss = 2.062861\n",
      "epoch 79 loss = 2.017010\n",
      "epoch 80 loss = 2.141654\n",
      "epoch 81 loss = 2.061623\n",
      "epoch 82 loss = 1.987018\n",
      "epoch 83 loss = 2.187478\n",
      "epoch 84 loss = 1.822981\n",
      "epoch 85 loss = 2.044125\n",
      "epoch 86 loss = 1.868304\n",
      "epoch 87 loss = 1.896468\n",
      "epoch 88 loss = 1.920994\n",
      "epoch 89 loss = 1.815452\n",
      "epoch 90 loss = 2.072366\n",
      "epoch 91 loss = 1.906377\n",
      "epoch 92 loss = 2.080390\n",
      "epoch 93 loss = 2.175610\n",
      "epoch 94 loss = 2.047729\n",
      "epoch 95 loss = 2.048405\n",
      "epoch 96 loss = 1.966975\n",
      "epoch 97 loss = 1.928885\n",
      "epoch 98 loss = 1.890424\n",
      "epoch 99 loss = 1.965721\n",
      "final loss = 1.965721\n",
      "accuracy_mc = tensor(0.2206, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2281, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0047, device='cuda:0')\n",
      "training time = 34.11509418487549 seconds\n",
      "testing time = 1.8761134147644043 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 2.315260\n",
      "epoch 1 loss = 2.302720\n",
      "epoch 2 loss = 2.247466\n",
      "epoch 3 loss = 2.276862\n",
      "epoch 4 loss = 2.210751\n",
      "epoch 5 loss = 2.083333\n",
      "epoch 6 loss = 2.034261\n",
      "epoch 7 loss = 2.232732\n",
      "epoch 8 loss = 1.981572\n",
      "epoch 9 loss = 2.207860\n",
      "epoch 10 loss = 2.267285\n",
      "epoch 11 loss = 2.114683\n",
      "epoch 12 loss = 2.079784\n",
      "epoch 13 loss = 2.050993\n",
      "epoch 14 loss = 2.105724\n",
      "epoch 15 loss = 2.091237\n",
      "epoch 16 loss = 2.106071\n",
      "epoch 17 loss = 2.278621\n",
      "epoch 18 loss = 2.156892\n",
      "epoch 19 loss = 2.207918\n",
      "epoch 20 loss = 2.101654\n",
      "epoch 21 loss = 1.990692\n",
      "epoch 22 loss = 1.969347\n",
      "epoch 23 loss = 2.080017\n",
      "epoch 24 loss = 2.144672\n",
      "epoch 25 loss = 2.088459\n",
      "epoch 26 loss = 2.090961\n",
      "epoch 27 loss = 2.157872\n",
      "epoch 28 loss = 2.193559\n",
      "epoch 29 loss = 2.031911\n",
      "epoch 30 loss = 1.959661\n",
      "epoch 31 loss = 2.141729\n",
      "epoch 32 loss = 2.176836\n",
      "epoch 33 loss = 1.934837\n",
      "epoch 34 loss = 1.940177\n",
      "epoch 35 loss = 2.156554\n",
      "epoch 36 loss = 2.156539\n",
      "epoch 37 loss = 2.083481\n",
      "epoch 38 loss = 2.085401\n",
      "epoch 39 loss = 2.136477\n",
      "epoch 40 loss = 2.130907\n",
      "epoch 41 loss = 2.047481\n",
      "epoch 42 loss = 1.987434\n",
      "epoch 43 loss = 2.008679\n",
      "epoch 44 loss = 2.023935\n",
      "epoch 45 loss = 2.089038\n",
      "epoch 46 loss = 2.126713\n",
      "epoch 47 loss = 2.023151\n",
      "epoch 48 loss = 2.202639\n",
      "epoch 49 loss = 2.045953\n",
      "epoch 50 loss = 2.069950\n",
      "epoch 51 loss = 2.092447\n",
      "epoch 52 loss = 2.057817\n",
      "epoch 53 loss = 2.120146\n",
      "epoch 54 loss = 2.188484\n",
      "epoch 55 loss = 2.093102\n",
      "epoch 56 loss = 2.034540\n",
      "epoch 57 loss = 2.064711\n",
      "epoch 58 loss = 2.088668\n",
      "epoch 59 loss = 2.202257\n",
      "epoch 60 loss = 2.273434\n",
      "epoch 61 loss = 2.038170\n",
      "epoch 62 loss = 2.017439\n",
      "epoch 63 loss = 2.083170\n",
      "epoch 64 loss = 2.120475\n",
      "epoch 65 loss = 2.062907\n",
      "epoch 66 loss = 2.113889\n",
      "epoch 67 loss = 1.995213\n",
      "epoch 68 loss = 2.116171\n",
      "epoch 69 loss = 2.145782\n",
      "epoch 70 loss = 1.988186\n",
      "epoch 71 loss = 2.031105\n",
      "epoch 72 loss = 2.099602\n",
      "epoch 73 loss = 2.181046\n",
      "epoch 74 loss = 2.030713\n",
      "epoch 75 loss = 2.065291\n",
      "epoch 76 loss = 2.172197\n",
      "epoch 77 loss = 2.130229\n",
      "epoch 78 loss = 2.143746\n",
      "epoch 79 loss = 2.153077\n",
      "epoch 80 loss = 2.074543\n",
      "epoch 81 loss = 2.132191\n",
      "epoch 82 loss = 2.082925\n",
      "epoch 83 loss = 2.284167\n",
      "epoch 84 loss = 2.154264\n",
      "epoch 85 loss = 1.912217\n",
      "epoch 86 loss = 2.130001\n",
      "epoch 87 loss = 1.879134\n",
      "epoch 88 loss = 1.992629\n",
      "epoch 89 loss = 2.098525\n",
      "epoch 90 loss = 1.945900\n",
      "epoch 91 loss = 2.134111\n",
      "epoch 92 loss = 2.134912\n",
      "epoch 93 loss = 2.174606\n",
      "epoch 94 loss = 2.081350\n",
      "epoch 95 loss = 2.015594\n",
      "epoch 96 loss = 2.015557\n",
      "epoch 97 loss = 1.999443\n",
      "epoch 98 loss = 2.134173\n",
      "epoch 99 loss = 2.200829\n",
      "final loss = 2.200829\n",
      "accuracy_mc = tensor(0.3238, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2880, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9621, device='cuda:0')\n",
      "training time = 33.88462281227112 seconds\n",
      "testing time = 1.8373489379882812 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.328033\n",
      "epoch 1 loss = 2.303102\n",
      "epoch 2 loss = 2.276450\n",
      "epoch 3 loss = 2.304465\n",
      "epoch 4 loss = 2.270899\n",
      "epoch 5 loss = 2.304053\n",
      "epoch 6 loss = 2.295422\n",
      "epoch 7 loss = 2.187022\n",
      "epoch 8 loss = 2.237589\n",
      "epoch 9 loss = 2.287538\n",
      "epoch 10 loss = 2.364143\n",
      "epoch 11 loss = 2.313705\n",
      "epoch 12 loss = 2.270908\n",
      "epoch 13 loss = 2.103597\n",
      "epoch 14 loss = 2.262041\n",
      "epoch 15 loss = 2.206006\n",
      "epoch 16 loss = 2.237949\n",
      "epoch 17 loss = 2.262044\n",
      "epoch 18 loss = 2.358147\n",
      "epoch 19 loss = 2.300728\n",
      "epoch 20 loss = 2.267034\n",
      "epoch 21 loss = 2.272363\n",
      "epoch 22 loss = 2.163531\n",
      "epoch 23 loss = 2.176283\n",
      "epoch 24 loss = 2.221776\n",
      "epoch 25 loss = 2.230172\n",
      "epoch 26 loss = 2.288973\n",
      "epoch 27 loss = 2.346423\n",
      "epoch 28 loss = 2.111811\n",
      "epoch 29 loss = 2.141488\n",
      "epoch 30 loss = 2.099166\n",
      "epoch 31 loss = 2.177660\n",
      "epoch 32 loss = 2.210664\n",
      "epoch 33 loss = 2.117094\n",
      "epoch 34 loss = 2.246166\n",
      "epoch 35 loss = 2.138932\n",
      "epoch 36 loss = 2.240189\n",
      "epoch 37 loss = 2.113905\n",
      "epoch 38 loss = 2.206189\n",
      "epoch 39 loss = 2.115563\n",
      "epoch 40 loss = 2.090474\n",
      "epoch 41 loss = 2.294899\n",
      "epoch 42 loss = 2.221024\n",
      "epoch 43 loss = 2.330968\n",
      "epoch 44 loss = 2.201835\n",
      "epoch 45 loss = 2.080150\n",
      "epoch 46 loss = 2.119227\n",
      "epoch 47 loss = 2.100396\n",
      "epoch 48 loss = 2.204528\n",
      "epoch 49 loss = 2.176557\n",
      "epoch 50 loss = 2.194221\n",
      "epoch 51 loss = 1.952542\n",
      "epoch 52 loss = 2.311596\n",
      "epoch 53 loss = 2.200279\n",
      "epoch 54 loss = 2.342003\n",
      "epoch 55 loss = 2.336053\n",
      "epoch 56 loss = 2.154623\n",
      "epoch 57 loss = 2.156369\n",
      "epoch 58 loss = 2.313691\n",
      "epoch 59 loss = 2.025347\n",
      "epoch 60 loss = 2.209757\n",
      "epoch 61 loss = 2.172440\n",
      "epoch 62 loss = 2.203597\n",
      "epoch 63 loss = 2.161594\n",
      "epoch 64 loss = 2.219841\n",
      "epoch 65 loss = 2.252899\n",
      "epoch 66 loss = 2.217025\n",
      "epoch 67 loss = 2.010733\n",
      "epoch 68 loss = 2.167205\n",
      "epoch 69 loss = 2.277392\n",
      "epoch 70 loss = 2.111353\n",
      "epoch 71 loss = 2.297370\n",
      "epoch 72 loss = 2.089551\n",
      "epoch 73 loss = 2.134456\n",
      "epoch 74 loss = 2.168946\n",
      "epoch 75 loss = 2.090098\n",
      "epoch 76 loss = 2.210976\n",
      "epoch 77 loss = 2.178221\n",
      "epoch 78 loss = 2.108461\n",
      "epoch 79 loss = 2.135746\n",
      "epoch 80 loss = 2.098585\n",
      "epoch 81 loss = 2.194200\n",
      "epoch 82 loss = 2.213237\n",
      "epoch 83 loss = 1.995756\n",
      "epoch 84 loss = 2.168590\n",
      "epoch 85 loss = 2.365037\n",
      "epoch 86 loss = 2.017616\n",
      "epoch 87 loss = 2.107263\n",
      "epoch 88 loss = 2.326720\n",
      "epoch 89 loss = 2.290899\n",
      "epoch 90 loss = 2.240683\n",
      "epoch 91 loss = 2.053021\n",
      "epoch 92 loss = 2.196867\n",
      "epoch 93 loss = 2.082211\n",
      "epoch 94 loss = 2.158851\n",
      "epoch 95 loss = 2.262429\n",
      "epoch 96 loss = 2.009613\n",
      "epoch 97 loss = 2.158621\n",
      "epoch 98 loss = 2.154553\n",
      "epoch 99 loss = 2.215252\n",
      "final loss = 2.215252\n",
      "accuracy_mc = tensor(0.1775, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2136, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0741, device='cuda:0')\n",
      "training time = 34.23236393928528 seconds\n",
      "testing time = 1.860877275466919 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.226126\n",
      "epoch 1 loss = 2.216762\n",
      "epoch 2 loss = 2.206703\n",
      "epoch 3 loss = 2.184577\n",
      "epoch 4 loss = 2.152928\n",
      "epoch 5 loss = 2.261941\n",
      "epoch 6 loss = 2.164095\n",
      "epoch 7 loss = 2.303133\n",
      "epoch 8 loss = 2.345689\n",
      "epoch 9 loss = 2.240709\n",
      "epoch 10 loss = 2.095225\n",
      "epoch 11 loss = 2.292553\n",
      "epoch 12 loss = 2.432907\n",
      "epoch 13 loss = 2.340950\n",
      "epoch 14 loss = 2.533675\n",
      "epoch 15 loss = 2.303289\n",
      "epoch 16 loss = 2.226551\n",
      "epoch 17 loss = 2.211152\n",
      "epoch 18 loss = 2.321032\n",
      "epoch 19 loss = 2.199250\n",
      "epoch 20 loss = 2.318782\n",
      "epoch 21 loss = 2.277374\n",
      "epoch 22 loss = 2.433177\n",
      "epoch 23 loss = 2.298352\n",
      "epoch 24 loss = 2.126855\n",
      "epoch 25 loss = 2.404484\n",
      "epoch 26 loss = 2.319827\n",
      "epoch 27 loss = 2.079756\n",
      "epoch 28 loss = 2.254557\n",
      "epoch 29 loss = 2.520658\n",
      "epoch 30 loss = 2.277289\n",
      "epoch 31 loss = 2.282365\n",
      "epoch 32 loss = 2.185425\n",
      "epoch 33 loss = 2.579502\n",
      "epoch 34 loss = 2.310600\n",
      "epoch 35 loss = 2.242508\n",
      "epoch 36 loss = 2.334955\n",
      "epoch 37 loss = 2.084313\n",
      "epoch 38 loss = 2.115169\n",
      "epoch 39 loss = 2.483894\n",
      "epoch 40 loss = 2.348646\n",
      "epoch 41 loss = 2.257068\n",
      "epoch 42 loss = 2.223415\n",
      "epoch 43 loss = 2.322779\n",
      "epoch 44 loss = 2.447343\n",
      "epoch 45 loss = 2.202868\n",
      "epoch 46 loss = 2.154762\n",
      "epoch 47 loss = 2.112915\n",
      "epoch 48 loss = 2.249244\n",
      "epoch 49 loss = 2.268998\n",
      "epoch 50 loss = 2.063257\n",
      "epoch 51 loss = 2.011733\n",
      "epoch 52 loss = 2.540768\n",
      "epoch 53 loss = 2.284844\n",
      "epoch 54 loss = 2.322365\n",
      "epoch 55 loss = 2.184021\n",
      "epoch 56 loss = 2.274936\n",
      "epoch 57 loss = 2.263210\n",
      "epoch 58 loss = 2.305136\n",
      "epoch 59 loss = 2.129041\n",
      "epoch 60 loss = 2.511917\n",
      "epoch 61 loss = 2.346884\n",
      "epoch 62 loss = 2.290096\n",
      "epoch 63 loss = 2.276407\n",
      "epoch 64 loss = 2.282845\n",
      "epoch 65 loss = 2.333906\n",
      "epoch 66 loss = 2.125482\n",
      "epoch 67 loss = 2.154867\n",
      "epoch 68 loss = 2.343502\n",
      "epoch 69 loss = 2.010788\n",
      "epoch 70 loss = 2.270723\n",
      "epoch 71 loss = 2.226554\n",
      "epoch 72 loss = 2.251767\n",
      "epoch 73 loss = 2.294933\n",
      "epoch 74 loss = 2.140554\n",
      "epoch 75 loss = 2.348537\n",
      "epoch 76 loss = 2.043708\n",
      "epoch 77 loss = 2.086576\n",
      "epoch 78 loss = 2.253228\n",
      "epoch 79 loss = 2.362666\n",
      "epoch 80 loss = 2.072651\n",
      "epoch 81 loss = 2.053818\n",
      "epoch 82 loss = 2.292464\n",
      "epoch 83 loss = 2.329055\n",
      "epoch 84 loss = 1.956805\n",
      "epoch 85 loss = 2.197257\n",
      "epoch 86 loss = 2.183540\n",
      "epoch 87 loss = 2.323622\n",
      "epoch 88 loss = 2.236289\n",
      "epoch 89 loss = 2.158317\n",
      "epoch 90 loss = 2.091809\n",
      "epoch 91 loss = 2.152674\n",
      "epoch 92 loss = 2.263178\n",
      "epoch 93 loss = 2.142735\n",
      "epoch 94 loss = 2.319460\n",
      "epoch 95 loss = 2.248076\n",
      "epoch 96 loss = 2.186845\n",
      "epoch 97 loss = 2.149440\n",
      "epoch 98 loss = 2.285785\n",
      "epoch 99 loss = 2.076339\n",
      "final loss = 2.076339\n",
      "accuracy_mc = tensor(0.2004, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1758, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0382, device='cuda:0')\n",
      "training time = 33.80410051345825 seconds\n",
      "testing time = 1.8763577938079834 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.292987\n",
      "epoch 1 loss = 2.240824\n",
      "epoch 2 loss = 2.251395\n",
      "epoch 3 loss = 2.194759\n",
      "epoch 4 loss = 2.147288\n",
      "epoch 5 loss = 2.225586\n",
      "epoch 6 loss = 2.179480\n",
      "epoch 7 loss = 2.175661\n",
      "epoch 8 loss = 2.075585\n",
      "epoch 9 loss = 2.243436\n",
      "epoch 10 loss = 2.209975\n",
      "epoch 11 loss = 2.158878\n",
      "epoch 12 loss = 2.212372\n",
      "epoch 13 loss = 2.116028\n",
      "epoch 14 loss = 2.110844\n",
      "epoch 15 loss = 2.210368\n",
      "epoch 16 loss = 2.120226\n",
      "epoch 17 loss = 1.949035\n",
      "epoch 18 loss = 2.071631\n",
      "epoch 19 loss = 2.117918\n",
      "epoch 20 loss = 2.086503\n",
      "epoch 21 loss = 2.154341\n",
      "epoch 22 loss = 2.053110\n",
      "epoch 23 loss = 2.018590\n",
      "epoch 24 loss = 2.093248\n",
      "epoch 25 loss = 2.053916\n",
      "epoch 26 loss = 2.104518\n",
      "epoch 27 loss = 2.046325\n",
      "epoch 28 loss = 2.031330\n",
      "epoch 29 loss = 1.969088\n",
      "epoch 30 loss = 2.149769\n",
      "epoch 31 loss = 2.155894\n",
      "epoch 32 loss = 2.160017\n",
      "epoch 33 loss = 1.972160\n",
      "epoch 34 loss = 2.102339\n",
      "epoch 35 loss = 2.000637\n",
      "epoch 36 loss = 2.162124\n",
      "epoch 37 loss = 2.100179\n",
      "epoch 38 loss = 2.067263\n",
      "epoch 39 loss = 2.049989\n",
      "epoch 40 loss = 2.036192\n",
      "epoch 41 loss = 2.019761\n",
      "epoch 42 loss = 2.163817\n",
      "epoch 43 loss = 2.115990\n",
      "epoch 44 loss = 2.248136\n",
      "epoch 45 loss = 2.159953\n",
      "epoch 46 loss = 2.072108\n",
      "epoch 47 loss = 2.239387\n",
      "epoch 48 loss = 2.072779\n",
      "epoch 49 loss = 2.076458\n",
      "epoch 50 loss = 2.003351\n",
      "epoch 51 loss = 1.976624\n",
      "epoch 52 loss = 1.911085\n",
      "epoch 53 loss = 2.096130\n",
      "epoch 54 loss = 2.102317\n",
      "epoch 55 loss = 2.197302\n",
      "epoch 56 loss = 2.103195\n",
      "epoch 57 loss = 2.056151\n",
      "epoch 58 loss = 1.982523\n",
      "epoch 59 loss = 1.914138\n",
      "epoch 60 loss = 2.035282\n",
      "epoch 61 loss = 2.111898\n",
      "epoch 62 loss = 2.071043\n",
      "epoch 63 loss = 2.114545\n",
      "epoch 64 loss = 1.932686\n",
      "epoch 65 loss = 2.017048\n",
      "epoch 66 loss = 2.045408\n",
      "epoch 67 loss = 1.983565\n",
      "epoch 68 loss = 2.061150\n",
      "epoch 69 loss = 2.066003\n",
      "epoch 70 loss = 2.132710\n",
      "epoch 71 loss = 1.950905\n",
      "epoch 72 loss = 1.979303\n",
      "epoch 73 loss = 2.085467\n",
      "epoch 74 loss = 1.974265\n",
      "epoch 75 loss = 2.063018\n",
      "epoch 76 loss = 2.114755\n",
      "epoch 77 loss = 1.997592\n",
      "epoch 78 loss = 2.166080\n",
      "epoch 79 loss = 2.043944\n",
      "epoch 80 loss = 2.134614\n",
      "epoch 81 loss = 1.955732\n",
      "epoch 82 loss = 2.097252\n",
      "epoch 83 loss = 1.972429\n",
      "epoch 84 loss = 2.062904\n",
      "epoch 85 loss = 2.065641\n",
      "epoch 86 loss = 2.155963\n",
      "epoch 87 loss = 2.075936\n",
      "epoch 88 loss = 2.014650\n",
      "epoch 89 loss = 2.226209\n",
      "epoch 90 loss = 2.040293\n",
      "epoch 91 loss = 2.117867\n",
      "epoch 92 loss = 2.029497\n",
      "epoch 93 loss = 2.122738\n",
      "epoch 94 loss = 2.099178\n",
      "epoch 95 loss = 2.099122\n",
      "epoch 96 loss = 2.063198\n",
      "epoch 97 loss = 1.919363\n",
      "epoch 98 loss = 2.066021\n",
      "epoch 99 loss = 2.094407\n",
      "final loss = 2.094407\n",
      "accuracy_mc = tensor(0.3130, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3358, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9784, device='cuda:0')\n",
      "training time = 33.980159759521484 seconds\n",
      "testing time = 1.897589921951294 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.244917\n",
      "epoch 1 loss = 2.289084\n",
      "epoch 2 loss = 2.318929\n",
      "epoch 3 loss = 2.302222\n",
      "epoch 4 loss = 2.246297\n",
      "epoch 5 loss = 2.252143\n",
      "epoch 6 loss = 2.316981\n",
      "epoch 7 loss = 2.188565\n",
      "epoch 8 loss = 2.269232\n",
      "epoch 9 loss = 2.261106\n",
      "epoch 10 loss = 2.265173\n",
      "epoch 11 loss = 2.190786\n",
      "epoch 12 loss = 2.110053\n",
      "epoch 13 loss = 2.228846\n",
      "epoch 14 loss = 2.140816\n",
      "epoch 15 loss = 2.190432\n",
      "epoch 16 loss = 2.191455\n",
      "epoch 17 loss = 2.245715\n",
      "epoch 18 loss = 2.191678\n",
      "epoch 19 loss = 2.258130\n",
      "epoch 20 loss = 2.247397\n",
      "epoch 21 loss = 2.140311\n",
      "epoch 22 loss = 2.101821\n",
      "epoch 23 loss = 2.217447\n",
      "epoch 24 loss = 2.126995\n",
      "epoch 25 loss = 2.163341\n",
      "epoch 26 loss = 2.247156\n",
      "epoch 27 loss = 2.069877\n",
      "epoch 28 loss = 2.153062\n",
      "epoch 29 loss = 2.142888\n",
      "epoch 30 loss = 2.126168\n",
      "epoch 31 loss = 2.136854\n",
      "epoch 32 loss = 2.184072\n",
      "epoch 33 loss = 2.178167\n",
      "epoch 34 loss = 2.169839\n",
      "epoch 35 loss = 2.117579\n",
      "epoch 36 loss = 2.189532\n",
      "epoch 37 loss = 2.054608\n",
      "epoch 38 loss = 2.213793\n",
      "epoch 39 loss = 2.052495\n",
      "epoch 40 loss = 2.124094\n",
      "epoch 41 loss = 2.178406\n",
      "epoch 42 loss = 2.198600\n",
      "epoch 43 loss = 2.148824\n",
      "epoch 44 loss = 2.142814\n",
      "epoch 45 loss = 2.105368\n",
      "epoch 46 loss = 2.227926\n",
      "epoch 47 loss = 2.150672\n",
      "epoch 48 loss = 2.115729\n",
      "epoch 49 loss = 2.045348\n",
      "epoch 50 loss = 2.064755\n",
      "epoch 51 loss = 2.185278\n",
      "epoch 52 loss = 2.324873\n",
      "epoch 53 loss = 2.108456\n",
      "epoch 54 loss = 2.092630\n",
      "epoch 55 loss = 2.067538\n",
      "epoch 56 loss = 2.060652\n",
      "epoch 57 loss = 2.128686\n",
      "epoch 58 loss = 2.072225\n",
      "epoch 59 loss = 2.093749\n",
      "epoch 60 loss = 2.121903\n",
      "epoch 61 loss = 2.165628\n",
      "epoch 62 loss = 2.055959\n",
      "epoch 63 loss = 2.137745\n",
      "epoch 64 loss = 2.130982\n",
      "epoch 65 loss = 2.182172\n",
      "epoch 66 loss = 2.097822\n",
      "epoch 67 loss = 2.015409\n",
      "epoch 68 loss = 2.049280\n",
      "epoch 69 loss = 2.143295\n",
      "epoch 70 loss = 2.345160\n",
      "epoch 71 loss = 2.164405\n",
      "epoch 72 loss = 2.186780\n",
      "epoch 73 loss = 2.115521\n",
      "epoch 74 loss = 1.987723\n",
      "epoch 75 loss = 2.148376\n",
      "epoch 76 loss = 2.160387\n",
      "epoch 77 loss = 2.130215\n",
      "epoch 78 loss = 2.081515\n",
      "epoch 79 loss = 2.021994\n",
      "epoch 80 loss = 2.163276\n",
      "epoch 81 loss = 2.198291\n",
      "epoch 82 loss = 2.087857\n",
      "epoch 83 loss = 1.979088\n",
      "epoch 84 loss = 2.172738\n",
      "epoch 85 loss = 2.193140\n",
      "epoch 86 loss = 2.136268\n",
      "epoch 87 loss = 2.229559\n",
      "epoch 88 loss = 2.127702\n",
      "epoch 89 loss = 2.133747\n",
      "epoch 90 loss = 2.185692\n",
      "epoch 91 loss = 2.157976\n",
      "epoch 92 loss = 2.185102\n",
      "epoch 93 loss = 2.103383\n",
      "epoch 94 loss = 2.048288\n",
      "epoch 95 loss = 2.200433\n",
      "epoch 96 loss = 2.133873\n",
      "epoch 97 loss = 2.095462\n",
      "epoch 98 loss = 2.145997\n",
      "epoch 99 loss = 2.163330\n",
      "final loss = 2.163330\n",
      "accuracy_mc = tensor(0.0957, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0854, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0726, device='cuda:0')\n",
      "training time = 33.84540557861328 seconds\n",
      "testing time = 1.9447541236877441 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 2.290233\n",
      "epoch 1 loss = 2.340556\n",
      "epoch 2 loss = 2.313249\n",
      "epoch 3 loss = 2.312398\n",
      "epoch 4 loss = 2.288846\n",
      "epoch 5 loss = 2.269755\n",
      "epoch 6 loss = 2.252767\n",
      "epoch 7 loss = 2.288482\n",
      "epoch 8 loss = 2.286830\n",
      "epoch 9 loss = 2.223869\n",
      "epoch 10 loss = 2.299446\n",
      "epoch 11 loss = 2.207511\n",
      "epoch 12 loss = 2.227324\n",
      "epoch 13 loss = 2.195307\n",
      "epoch 14 loss = 2.246556\n",
      "epoch 15 loss = 2.158579\n",
      "epoch 16 loss = 2.128605\n",
      "epoch 17 loss = 2.115848\n",
      "epoch 18 loss = 2.201657\n",
      "epoch 19 loss = 2.241714\n",
      "epoch 20 loss = 2.291431\n",
      "epoch 21 loss = 2.184559\n",
      "epoch 22 loss = 2.005543\n",
      "epoch 23 loss = 2.135790\n",
      "epoch 24 loss = 2.103056\n",
      "epoch 25 loss = 2.103862\n",
      "epoch 26 loss = 2.168227\n",
      "epoch 27 loss = 2.140604\n",
      "epoch 28 loss = 2.243236\n",
      "epoch 29 loss = 2.188310\n",
      "epoch 30 loss = 2.124038\n",
      "epoch 31 loss = 2.067605\n",
      "epoch 32 loss = 2.310869\n",
      "epoch 33 loss = 2.024836\n",
      "epoch 34 loss = 2.116605\n",
      "epoch 35 loss = 2.103509\n",
      "epoch 36 loss = 2.014392\n",
      "epoch 37 loss = 2.116287\n",
      "epoch 38 loss = 2.107404\n",
      "epoch 39 loss = 2.084106\n",
      "epoch 40 loss = 2.156918\n",
      "epoch 41 loss = 2.285790\n",
      "epoch 42 loss = 2.146681\n",
      "epoch 43 loss = 2.137951\n",
      "epoch 44 loss = 2.200281\n",
      "epoch 45 loss = 2.025959\n",
      "epoch 46 loss = 2.313706\n",
      "epoch 47 loss = 2.105849\n",
      "epoch 48 loss = 2.103676\n",
      "epoch 49 loss = 2.175175\n",
      "epoch 50 loss = 1.995877\n",
      "epoch 51 loss = 2.176057\n",
      "epoch 52 loss = 2.057863\n",
      "epoch 53 loss = 2.180401\n",
      "epoch 54 loss = 2.129870\n",
      "epoch 55 loss = 2.293335\n",
      "epoch 56 loss = 2.068400\n",
      "epoch 57 loss = 2.195414\n",
      "epoch 58 loss = 2.114199\n",
      "epoch 59 loss = 2.079904\n",
      "epoch 60 loss = 2.274005\n",
      "epoch 61 loss = 1.962635\n",
      "epoch 62 loss = 2.106631\n",
      "epoch 63 loss = 2.180939\n",
      "epoch 64 loss = 2.102543\n",
      "epoch 65 loss = 2.105472\n",
      "epoch 66 loss = 2.127142\n",
      "epoch 67 loss = 2.100384\n",
      "epoch 68 loss = 2.116678\n",
      "epoch 69 loss = 2.090255\n",
      "epoch 70 loss = 2.065770\n",
      "epoch 71 loss = 2.150496\n",
      "epoch 72 loss = 2.112292\n",
      "epoch 73 loss = 1.983961\n",
      "epoch 74 loss = 2.071521\n",
      "epoch 75 loss = 2.208477\n",
      "epoch 76 loss = 2.339231\n",
      "epoch 77 loss = 2.091208\n",
      "epoch 78 loss = 1.969173\n",
      "epoch 79 loss = 1.898007\n",
      "epoch 80 loss = 1.964025\n",
      "epoch 81 loss = 1.993997\n",
      "epoch 82 loss = 1.955419\n",
      "epoch 83 loss = 1.996241\n",
      "epoch 84 loss = 2.108621\n",
      "epoch 85 loss = 2.132059\n",
      "epoch 86 loss = 2.123247\n",
      "epoch 87 loss = 2.048661\n",
      "epoch 88 loss = 2.095425\n",
      "epoch 89 loss = 2.006726\n",
      "epoch 90 loss = 2.032605\n",
      "epoch 91 loss = 2.006525\n",
      "epoch 92 loss = 1.996773\n",
      "epoch 93 loss = 2.018133\n",
      "epoch 94 loss = 2.098750\n",
      "epoch 95 loss = 2.069490\n",
      "epoch 96 loss = 1.960252\n",
      "epoch 97 loss = 2.054066\n",
      "epoch 98 loss = 2.037412\n",
      "epoch 99 loss = 1.970102\n",
      "final loss = 1.970102\n",
      "accuracy_mc = tensor(0.2768, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2297, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9619, device='cuda:0')\n",
      "training time = 33.69537138938904 seconds\n",
      "testing time = 1.854778528213501 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.500000, reg_strength 0.050000\n",
      "n_epoch 500\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = 2.325371\n",
      "epoch 1 loss = 2.275664\n",
      "epoch 2 loss = 2.261942\n",
      "epoch 3 loss = 2.287587\n",
      "epoch 4 loss = 2.193944\n",
      "epoch 5 loss = 2.309253\n",
      "epoch 6 loss = 2.200701\n",
      "epoch 7 loss = 2.343623\n",
      "epoch 8 loss = 2.217948\n",
      "epoch 9 loss = 2.221842\n",
      "epoch 10 loss = 2.201942\n",
      "epoch 11 loss = 2.294660\n",
      "epoch 12 loss = 2.276495\n",
      "epoch 13 loss = 2.293447\n",
      "epoch 14 loss = 2.250941\n",
      "epoch 15 loss = 2.211240\n",
      "epoch 16 loss = 2.247408\n",
      "epoch 17 loss = 2.032119\n",
      "epoch 18 loss = 2.236619\n",
      "epoch 19 loss = 2.160795\n",
      "epoch 20 loss = 2.172107\n",
      "epoch 21 loss = 2.272074\n",
      "epoch 22 loss = 2.223838\n",
      "epoch 23 loss = 2.226907\n",
      "epoch 24 loss = 2.140364\n",
      "epoch 25 loss = 2.296629\n",
      "epoch 26 loss = 2.035148\n",
      "epoch 27 loss = 2.329648\n",
      "epoch 28 loss = 2.201749\n",
      "epoch 29 loss = 2.219563\n",
      "epoch 30 loss = 2.287009\n",
      "epoch 31 loss = 2.258601\n",
      "epoch 32 loss = 2.180708\n",
      "epoch 33 loss = 2.242010\n",
      "epoch 34 loss = 2.160405\n",
      "epoch 35 loss = 2.232176\n",
      "epoch 36 loss = 2.148942\n",
      "epoch 37 loss = 2.218006\n",
      "epoch 38 loss = 2.288918\n",
      "epoch 39 loss = 2.202667\n",
      "epoch 40 loss = 2.202408\n",
      "epoch 41 loss = 2.126400\n",
      "epoch 42 loss = 2.121985\n",
      "epoch 43 loss = 2.239499\n",
      "epoch 44 loss = 2.078190\n",
      "epoch 45 loss = 2.058232\n",
      "epoch 46 loss = 2.025114\n",
      "epoch 47 loss = 2.264864\n",
      "epoch 48 loss = 2.336934\n",
      "epoch 49 loss = 2.187485\n",
      "epoch 50 loss = 2.047389\n",
      "epoch 51 loss = 2.024281\n",
      "epoch 52 loss = 2.178514\n",
      "epoch 53 loss = 2.323046\n",
      "epoch 54 loss = 2.305793\n",
      "epoch 55 loss = 2.180195\n",
      "epoch 56 loss = 2.159059\n",
      "epoch 57 loss = 2.188448\n",
      "epoch 58 loss = 2.311116\n",
      "epoch 59 loss = 2.174184\n",
      "epoch 60 loss = 2.154535\n",
      "epoch 61 loss = 2.191578\n",
      "epoch 62 loss = 2.245034\n",
      "epoch 63 loss = 2.203607\n",
      "epoch 64 loss = 2.161890\n",
      "epoch 65 loss = 2.188503\n",
      "epoch 66 loss = 2.114487\n",
      "epoch 67 loss = 2.102676\n",
      "epoch 68 loss = 2.307245\n",
      "epoch 69 loss = 2.168669\n",
      "epoch 70 loss = 2.134252\n",
      "epoch 71 loss = 2.172695\n",
      "epoch 72 loss = 2.065642\n",
      "epoch 73 loss = 2.264936\n",
      "epoch 74 loss = 2.182493\n",
      "epoch 75 loss = 2.193332\n",
      "epoch 76 loss = 2.112833\n",
      "epoch 77 loss = 2.091846\n",
      "epoch 78 loss = 2.316846\n",
      "epoch 79 loss = 2.174633\n",
      "epoch 80 loss = 2.225368\n",
      "epoch 81 loss = 2.195183\n",
      "epoch 82 loss = 2.201903\n",
      "epoch 83 loss = 2.214846\n",
      "epoch 84 loss = 2.153491\n",
      "epoch 85 loss = 2.034606\n",
      "epoch 86 loss = 2.029563\n",
      "epoch 87 loss = 2.156196\n",
      "epoch 88 loss = 2.219522\n",
      "epoch 89 loss = 2.126483\n",
      "epoch 90 loss = 2.180094\n",
      "epoch 91 loss = 1.932569\n",
      "epoch 92 loss = 2.286063\n",
      "epoch 93 loss = 2.080154\n",
      "epoch 94 loss = 2.102141\n",
      "epoch 95 loss = 2.154418\n",
      "epoch 96 loss = 2.127085\n",
      "epoch 97 loss = 2.281439\n",
      "epoch 98 loss = 2.334756\n",
      "epoch 99 loss = 1.946298\n",
      "epoch 100 loss = 2.320022\n",
      "epoch 101 loss = 2.044368\n",
      "epoch 102 loss = 2.151913\n",
      "epoch 103 loss = 1.946347\n",
      "epoch 104 loss = 2.164397\n",
      "epoch 105 loss = 2.123479\n",
      "epoch 106 loss = 2.463242\n",
      "epoch 107 loss = 2.190156\n",
      "epoch 108 loss = 2.213350\n",
      "epoch 109 loss = 2.098195\n",
      "epoch 110 loss = 2.083706\n",
      "epoch 111 loss = 2.051410\n",
      "epoch 112 loss = 1.887818\n",
      "epoch 113 loss = 1.870452\n",
      "epoch 114 loss = 2.193218\n",
      "epoch 115 loss = 2.149105\n",
      "epoch 116 loss = 2.105632\n",
      "epoch 117 loss = 2.125058\n",
      "epoch 118 loss = 1.912536\n",
      "epoch 119 loss = 2.246322\n",
      "epoch 120 loss = 2.017024\n",
      "epoch 121 loss = 2.047917\n",
      "epoch 122 loss = 2.063557\n",
      "epoch 123 loss = 1.993171\n",
      "epoch 124 loss = 2.171530\n",
      "epoch 125 loss = 2.115399\n",
      "epoch 126 loss = 2.320982\n",
      "epoch 127 loss = 2.028202\n",
      "epoch 128 loss = 2.149426\n",
      "epoch 129 loss = 2.114050\n",
      "epoch 130 loss = 2.138289\n",
      "epoch 131 loss = 2.088133\n",
      "epoch 132 loss = 2.349513\n",
      "epoch 133 loss = 1.920766\n",
      "epoch 134 loss = 1.980442\n",
      "epoch 135 loss = 2.142108\n",
      "epoch 136 loss = 1.941383\n",
      "epoch 137 loss = 1.992860\n",
      "epoch 138 loss = 2.005819\n",
      "epoch 139 loss = 2.035586\n",
      "epoch 140 loss = 2.189120\n",
      "epoch 141 loss = 2.123241\n",
      "epoch 142 loss = 2.068074\n",
      "epoch 143 loss = 1.954369\n",
      "epoch 144 loss = 2.337817\n",
      "epoch 145 loss = 2.233014\n",
      "epoch 146 loss = 1.990591\n",
      "epoch 147 loss = 1.971979\n",
      "epoch 148 loss = 1.993193\n",
      "epoch 149 loss = 2.046800\n",
      "epoch 150 loss = 2.008170\n",
      "epoch 151 loss = 2.226658\n",
      "epoch 152 loss = 2.163764\n",
      "epoch 153 loss = 2.266905\n",
      "epoch 154 loss = 2.046129\n",
      "epoch 155 loss = 2.159048\n",
      "epoch 156 loss = 2.057766\n",
      "epoch 157 loss = 1.931968\n",
      "epoch 158 loss = 2.007142\n",
      "epoch 159 loss = 2.131516\n",
      "epoch 160 loss = 2.257431\n",
      "epoch 161 loss = 1.947256\n",
      "epoch 162 loss = 1.938095\n",
      "epoch 163 loss = 2.224298\n",
      "epoch 164 loss = 2.008419\n",
      "epoch 165 loss = 1.925625\n",
      "epoch 166 loss = 2.009587\n",
      "epoch 167 loss = 2.005230\n",
      "epoch 168 loss = 2.095173\n",
      "epoch 169 loss = 2.070899\n",
      "epoch 170 loss = 2.153795\n",
      "epoch 171 loss = 2.181088\n",
      "epoch 172 loss = 2.045161\n",
      "epoch 173 loss = 2.343326\n",
      "epoch 174 loss = 2.100627\n",
      "epoch 175 loss = 2.032571\n",
      "epoch 176 loss = 1.927029\n",
      "epoch 177 loss = 1.960944\n",
      "epoch 178 loss = 2.066460\n",
      "epoch 179 loss = 2.159992\n",
      "epoch 180 loss = 2.015153\n",
      "epoch 181 loss = 2.025729\n",
      "epoch 182 loss = 2.008912\n",
      "epoch 183 loss = 2.056448\n",
      "epoch 184 loss = 2.013299\n",
      "epoch 185 loss = 2.097309\n",
      "epoch 186 loss = 1.982170\n",
      "epoch 187 loss = 1.982366\n",
      "epoch 188 loss = 1.999801\n",
      "epoch 189 loss = 2.142674\n",
      "epoch 190 loss = 2.126393\n",
      "epoch 191 loss = 1.877500\n",
      "epoch 192 loss = 1.991614\n",
      "epoch 193 loss = 2.150017\n",
      "epoch 194 loss = 1.992196\n",
      "epoch 195 loss = 2.119555\n",
      "epoch 196 loss = 2.133985\n",
      "epoch 197 loss = 1.932759\n",
      "epoch 198 loss = 2.026823\n",
      "epoch 199 loss = 1.959492\n",
      "epoch 200 loss = 1.994391\n",
      "epoch 201 loss = 1.906206\n",
      "epoch 202 loss = 2.144936\n",
      "epoch 203 loss = 2.019526\n",
      "epoch 204 loss = 1.878953\n",
      "epoch 205 loss = 2.073495\n",
      "epoch 206 loss = 1.935708\n",
      "epoch 207 loss = 2.163965\n",
      "epoch 208 loss = 1.894166\n",
      "epoch 209 loss = 2.247153\n",
      "epoch 210 loss = 2.167025\n",
      "epoch 211 loss = 1.960413\n",
      "epoch 212 loss = 2.022183\n",
      "epoch 213 loss = 2.039830\n",
      "epoch 214 loss = 1.878171\n",
      "epoch 215 loss = 1.955235\n",
      "epoch 216 loss = 2.073998\n",
      "epoch 217 loss = 2.175189\n",
      "epoch 218 loss = 2.055587\n",
      "epoch 219 loss = 2.082978\n",
      "epoch 220 loss = 1.876349\n",
      "epoch 221 loss = 2.118246\n",
      "epoch 222 loss = 2.046737\n",
      "epoch 223 loss = 2.040488\n",
      "epoch 224 loss = 1.963807\n",
      "epoch 225 loss = 2.262208\n",
      "epoch 226 loss = 1.975010\n",
      "epoch 227 loss = 1.954886\n",
      "epoch 228 loss = 1.856678\n",
      "epoch 229 loss = 2.042963\n",
      "epoch 230 loss = 2.185019\n",
      "epoch 231 loss = 1.994582\n",
      "epoch 232 loss = 1.982862\n",
      "epoch 233 loss = 2.063845\n",
      "epoch 234 loss = 1.889609\n",
      "epoch 235 loss = 2.080569\n",
      "epoch 236 loss = 2.098838\n",
      "epoch 237 loss = 1.868405\n",
      "epoch 238 loss = 1.886077\n",
      "epoch 239 loss = 1.927346\n",
      "epoch 240 loss = 2.068123\n",
      "epoch 241 loss = 2.021005\n",
      "epoch 242 loss = 2.166814\n",
      "epoch 243 loss = 1.934628\n",
      "epoch 244 loss = 2.013556\n",
      "epoch 245 loss = 2.247934\n",
      "epoch 246 loss = 1.945396\n",
      "epoch 247 loss = 2.081797\n",
      "epoch 248 loss = 2.022851\n",
      "epoch 249 loss = 2.129710\n",
      "epoch 250 loss = 1.883943\n",
      "epoch 251 loss = 2.190952\n",
      "epoch 252 loss = 2.075172\n",
      "epoch 253 loss = 1.949360\n",
      "epoch 254 loss = 1.706691\n",
      "epoch 255 loss = 1.808416\n",
      "epoch 256 loss = 2.097909\n",
      "epoch 257 loss = 2.021532\n",
      "epoch 258 loss = 2.132273\n",
      "epoch 259 loss = 2.074466\n",
      "epoch 260 loss = 1.925640\n",
      "epoch 261 loss = 2.057433\n",
      "epoch 262 loss = 2.279335\n",
      "epoch 263 loss = 1.783766\n",
      "epoch 264 loss = 1.927585\n",
      "epoch 265 loss = 2.118686\n",
      "epoch 266 loss = 2.003238\n",
      "epoch 267 loss = 2.059340\n",
      "epoch 268 loss = 2.020193\n",
      "epoch 269 loss = 1.924434\n",
      "epoch 270 loss = 2.104633\n",
      "epoch 271 loss = 2.014700\n",
      "epoch 272 loss = 2.012949\n",
      "epoch 273 loss = 2.092707\n",
      "epoch 274 loss = 1.741845\n",
      "epoch 275 loss = 1.977989\n",
      "epoch 276 loss = 1.900864\n",
      "epoch 277 loss = 2.465990\n",
      "epoch 278 loss = 2.114171\n",
      "epoch 279 loss = 1.988199\n",
      "epoch 280 loss = 1.977392\n",
      "epoch 281 loss = 2.057333\n",
      "epoch 282 loss = 2.020681\n",
      "epoch 283 loss = 2.007326\n",
      "epoch 284 loss = 2.098165\n",
      "epoch 285 loss = 2.130262\n",
      "epoch 286 loss = 2.044166\n",
      "epoch 287 loss = 2.061251\n",
      "epoch 288 loss = 2.186806\n",
      "epoch 289 loss = 2.140482\n",
      "epoch 290 loss = 1.828245\n",
      "epoch 291 loss = 1.911891\n",
      "epoch 292 loss = 1.922199\n",
      "epoch 293 loss = 1.928041\n",
      "epoch 294 loss = 1.883357\n",
      "epoch 295 loss = 1.993922\n",
      "epoch 296 loss = 2.193931\n",
      "epoch 297 loss = 2.298434\n",
      "epoch 298 loss = 2.118596\n",
      "epoch 299 loss = 2.000944\n",
      "epoch 300 loss = 2.034076\n",
      "epoch 301 loss = 2.072156\n",
      "epoch 302 loss = 1.988831\n",
      "epoch 303 loss = 2.086582\n",
      "epoch 304 loss = 1.911568\n",
      "epoch 305 loss = 2.032315\n",
      "epoch 306 loss = 2.115538\n",
      "epoch 307 loss = 1.832991\n",
      "epoch 308 loss = 1.986403\n",
      "epoch 309 loss = 1.982126\n",
      "epoch 310 loss = 1.956601\n",
      "epoch 311 loss = 2.115062\n",
      "epoch 312 loss = 2.091701\n",
      "epoch 313 loss = 2.050486\n",
      "epoch 314 loss = 1.905174\n",
      "epoch 315 loss = 2.029348\n",
      "epoch 316 loss = 2.112271\n",
      "epoch 317 loss = 2.206718\n",
      "epoch 318 loss = 1.972516\n",
      "epoch 319 loss = 2.011281\n",
      "epoch 320 loss = 2.188778\n",
      "epoch 321 loss = 1.954412\n",
      "epoch 322 loss = 1.983323\n",
      "epoch 323 loss = 2.074738\n",
      "epoch 324 loss = 2.211815\n",
      "epoch 325 loss = 2.028121\n",
      "epoch 326 loss = 1.969139\n",
      "epoch 327 loss = 2.157394\n",
      "epoch 328 loss = 1.956697\n",
      "epoch 329 loss = 2.026546\n",
      "epoch 330 loss = 1.725033\n",
      "epoch 331 loss = 1.817174\n",
      "epoch 332 loss = 1.948907\n",
      "epoch 333 loss = 2.041501\n",
      "epoch 334 loss = 1.986600\n",
      "epoch 335 loss = 2.034830\n",
      "epoch 336 loss = 1.896699\n",
      "epoch 337 loss = 1.963421\n",
      "epoch 338 loss = 2.163255\n",
      "epoch 339 loss = 1.938802\n",
      "epoch 340 loss = 1.833330\n",
      "epoch 341 loss = 2.133126\n",
      "epoch 342 loss = 1.992378\n",
      "epoch 343 loss = 2.028347\n",
      "epoch 344 loss = 2.026779\n",
      "epoch 345 loss = 1.961953\n",
      "epoch 346 loss = 2.028005\n",
      "epoch 347 loss = 1.868520\n",
      "epoch 348 loss = 2.161463\n",
      "epoch 349 loss = 2.050526\n",
      "epoch 350 loss = 2.079002\n",
      "epoch 351 loss = 2.106786\n",
      "epoch 352 loss = 1.867387\n",
      "epoch 353 loss = 2.081003\n",
      "epoch 354 loss = 1.922805\n",
      "epoch 355 loss = 1.946805\n",
      "epoch 356 loss = 2.024155\n",
      "epoch 357 loss = 2.096359\n",
      "epoch 358 loss = 2.117980\n",
      "epoch 359 loss = 1.969998\n",
      "epoch 360 loss = 1.909886\n",
      "epoch 361 loss = 1.918725\n",
      "epoch 362 loss = 2.026173\n",
      "epoch 363 loss = 2.076434\n",
      "epoch 364 loss = 1.736172\n",
      "epoch 365 loss = 1.825846\n",
      "epoch 366 loss = 2.322101\n",
      "epoch 367 loss = 1.881422\n",
      "epoch 368 loss = 1.875079\n",
      "epoch 369 loss = 1.999851\n",
      "epoch 370 loss = 1.974709\n",
      "epoch 371 loss = 1.892756\n",
      "epoch 372 loss = 1.941711\n",
      "epoch 373 loss = 2.137114\n",
      "epoch 374 loss = 2.059264\n",
      "epoch 375 loss = 1.906853\n",
      "epoch 376 loss = 2.180521\n",
      "epoch 377 loss = 1.927590\n",
      "epoch 378 loss = 2.062898\n",
      "epoch 379 loss = 1.885639\n",
      "epoch 380 loss = 2.196252\n",
      "epoch 381 loss = 1.999028\n",
      "epoch 382 loss = 1.950220\n",
      "epoch 383 loss = 2.024714\n",
      "epoch 384 loss = 2.127153\n",
      "epoch 385 loss = 2.278463\n",
      "epoch 386 loss = 1.979799\n",
      "epoch 387 loss = 2.056795\n",
      "epoch 388 loss = 1.733664\n",
      "epoch 389 loss = 2.025349\n",
      "epoch 390 loss = 1.946854\n",
      "epoch 391 loss = 2.151532\n",
      "epoch 392 loss = 1.771862\n",
      "epoch 393 loss = 2.314935\n",
      "epoch 394 loss = 1.993530\n",
      "epoch 395 loss = 2.018517\n",
      "epoch 396 loss = 2.060449\n",
      "epoch 397 loss = 2.199439\n",
      "epoch 398 loss = 2.025397\n",
      "epoch 399 loss = 2.095881\n",
      "epoch 400 loss = 2.003202\n",
      "epoch 401 loss = 1.777785\n",
      "epoch 402 loss = 2.003843\n",
      "epoch 403 loss = 2.078390\n",
      "epoch 404 loss = 1.976961\n",
      "epoch 405 loss = 2.000207\n",
      "epoch 406 loss = 2.015785\n",
      "epoch 407 loss = 1.978803\n",
      "epoch 408 loss = 2.155474\n",
      "epoch 409 loss = 2.106917\n",
      "epoch 410 loss = 1.969343\n",
      "epoch 411 loss = 1.836631\n",
      "epoch 412 loss = 1.973469\n",
      "epoch 413 loss = 1.704886\n",
      "epoch 414 loss = 1.592965\n",
      "epoch 415 loss = 1.878134\n",
      "epoch 416 loss = 2.237906\n",
      "epoch 417 loss = 1.703900\n",
      "epoch 418 loss = 2.031747\n",
      "epoch 419 loss = 2.085724\n",
      "epoch 420 loss = 2.239161\n",
      "epoch 421 loss = 2.056734\n",
      "epoch 422 loss = 2.146226\n",
      "epoch 423 loss = 1.943694\n",
      "epoch 424 loss = 1.825373\n",
      "epoch 425 loss = 1.931097\n",
      "epoch 426 loss = 1.832118\n",
      "epoch 427 loss = 2.317991\n",
      "epoch 428 loss = 1.978978\n",
      "epoch 429 loss = 2.157144\n",
      "epoch 430 loss = 2.057831\n",
      "epoch 431 loss = 1.813389\n",
      "epoch 432 loss = 1.943615\n",
      "epoch 433 loss = 2.065481\n",
      "epoch 434 loss = 1.756971\n",
      "epoch 435 loss = 2.033105\n",
      "epoch 436 loss = 2.027690\n",
      "epoch 437 loss = 1.973020\n",
      "epoch 438 loss = 2.079472\n",
      "epoch 439 loss = 2.103821\n",
      "epoch 440 loss = 1.807078\n",
      "epoch 441 loss = 1.990920\n",
      "epoch 442 loss = 1.892349\n",
      "epoch 443 loss = 2.149626\n",
      "epoch 444 loss = 2.204438\n",
      "epoch 445 loss = 1.938279\n",
      "epoch 446 loss = 2.042764\n",
      "epoch 447 loss = 2.244423\n",
      "epoch 448 loss = 2.131706\n",
      "epoch 449 loss = 1.977107\n",
      "epoch 450 loss = 2.065557\n",
      "epoch 451 loss = 1.975143\n",
      "epoch 452 loss = 1.863904\n",
      "epoch 453 loss = 1.984155\n",
      "epoch 454 loss = 1.891095\n",
      "epoch 455 loss = 1.940138\n",
      "epoch 456 loss = 1.892594\n",
      "epoch 457 loss = 1.872981\n",
      "epoch 458 loss = 2.077916\n",
      "epoch 459 loss = 1.855720\n",
      "epoch 460 loss = 2.073606\n",
      "epoch 461 loss = 1.752619\n",
      "epoch 462 loss = 1.937796\n",
      "epoch 463 loss = 2.044586\n",
      "epoch 464 loss = 1.937271\n",
      "epoch 465 loss = 1.830449\n",
      "epoch 466 loss = 2.084872\n",
      "epoch 467 loss = 1.928955\n",
      "epoch 468 loss = 1.902698\n",
      "epoch 469 loss = 1.789221\n",
      "epoch 470 loss = 2.108984\n",
      "epoch 471 loss = 2.090073\n",
      "epoch 472 loss = 1.746273\n",
      "epoch 473 loss = 1.876630\n",
      "epoch 474 loss = 1.839198\n",
      "epoch 475 loss = 1.952362\n",
      "epoch 476 loss = 1.960160\n",
      "epoch 477 loss = 1.843412\n",
      "epoch 478 loss = 2.023059\n",
      "epoch 479 loss = 1.617606\n",
      "epoch 480 loss = 1.960310\n",
      "epoch 481 loss = 1.929959\n",
      "epoch 482 loss = 1.830095\n",
      "epoch 483 loss = 1.970538\n",
      "epoch 484 loss = 1.948980\n",
      "epoch 485 loss = 1.803592\n",
      "epoch 486 loss = 2.008610\n",
      "epoch 487 loss = 1.855471\n",
      "epoch 488 loss = 1.794388\n",
      "epoch 489 loss = 1.985579\n",
      "epoch 490 loss = 1.928401\n",
      "epoch 491 loss = 1.802772\n",
      "epoch 492 loss = 1.906082\n",
      "epoch 493 loss = 2.026451\n",
      "epoch 494 loss = 2.191117\n",
      "epoch 495 loss = 1.914375\n",
      "epoch 496 loss = 1.979593\n",
      "epoch 497 loss = 1.929675\n",
      "epoch 498 loss = 2.005996\n",
      "epoch 499 loss = 1.946313\n",
      "final loss = 1.946313\n",
      "accuracy_mc = tensor(0.2690, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3326, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9749, device='cuda:0')\n",
      "training time = 169.739253282547 seconds\n",
      "testing time = 1.8809258937835693 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = 2.350933\n",
      "epoch 1 loss = 2.257027\n",
      "epoch 2 loss = 2.283915\n",
      "epoch 3 loss = 2.231488\n",
      "epoch 4 loss = 2.238459\n",
      "epoch 5 loss = 2.265729\n",
      "epoch 6 loss = 2.234431\n",
      "epoch 7 loss = 2.084378\n",
      "epoch 8 loss = 2.167014\n",
      "epoch 9 loss = 2.268759\n",
      "epoch 10 loss = 2.317276\n",
      "epoch 11 loss = 2.069332\n",
      "epoch 12 loss = 2.241082\n",
      "epoch 13 loss = 2.282926\n",
      "epoch 14 loss = 2.255664\n",
      "epoch 15 loss = 2.194872\n",
      "epoch 16 loss = 2.205498\n",
      "epoch 17 loss = 2.173829\n",
      "epoch 18 loss = 2.232062\n",
      "epoch 19 loss = 2.146257\n",
      "epoch 20 loss = 2.245359\n",
      "epoch 21 loss = 2.261927\n",
      "epoch 22 loss = 2.098246\n",
      "epoch 23 loss = 2.126107\n",
      "epoch 24 loss = 2.180239\n",
      "epoch 25 loss = 2.195218\n",
      "epoch 26 loss = 2.319493\n",
      "epoch 27 loss = 2.395463\n",
      "epoch 28 loss = 2.156215\n",
      "epoch 29 loss = 2.175873\n",
      "epoch 30 loss = 2.168457\n",
      "epoch 31 loss = 2.263447\n",
      "epoch 32 loss = 2.335264\n",
      "epoch 33 loss = 2.300968\n",
      "epoch 34 loss = 2.032635\n",
      "epoch 35 loss = 2.142806\n",
      "epoch 36 loss = 2.104656\n",
      "epoch 37 loss = 2.038441\n",
      "epoch 38 loss = 2.193625\n",
      "epoch 39 loss = 2.196496\n",
      "epoch 40 loss = 2.198167\n",
      "epoch 41 loss = 2.235971\n",
      "epoch 42 loss = 2.125274\n",
      "epoch 43 loss = 2.119910\n",
      "epoch 44 loss = 2.126243\n",
      "epoch 45 loss = 2.117253\n",
      "epoch 46 loss = 2.177544\n",
      "epoch 47 loss = 2.159590\n",
      "epoch 48 loss = 2.047137\n",
      "epoch 49 loss = 2.050022\n",
      "epoch 50 loss = 2.063219\n",
      "epoch 51 loss = 2.109848\n",
      "epoch 52 loss = 2.120602\n",
      "epoch 53 loss = 2.158300\n",
      "epoch 54 loss = 2.075902\n",
      "epoch 55 loss = 2.052805\n",
      "epoch 56 loss = 2.157680\n",
      "epoch 57 loss = 2.074435\n",
      "epoch 58 loss = 2.132588\n",
      "epoch 59 loss = 2.208847\n",
      "epoch 60 loss = 2.019880\n",
      "epoch 61 loss = 2.003664\n",
      "epoch 62 loss = 2.104102\n",
      "epoch 63 loss = 2.051745\n",
      "epoch 64 loss = 1.872605\n",
      "epoch 65 loss = 1.897650\n",
      "epoch 66 loss = 2.029601\n",
      "epoch 67 loss = 2.084126\n",
      "epoch 68 loss = 1.835648\n",
      "epoch 69 loss = 2.152891\n",
      "epoch 70 loss = 2.167428\n",
      "epoch 71 loss = 2.166146\n",
      "epoch 72 loss = 2.069812\n",
      "epoch 73 loss = 2.139868\n",
      "epoch 74 loss = 1.933854\n",
      "epoch 75 loss = 2.060457\n",
      "epoch 76 loss = 2.125629\n",
      "epoch 77 loss = 1.979139\n",
      "epoch 78 loss = 1.974259\n",
      "epoch 79 loss = 1.947617\n",
      "epoch 80 loss = 2.145648\n",
      "epoch 81 loss = 2.241318\n",
      "epoch 82 loss = 2.088458\n",
      "epoch 83 loss = 2.044288\n",
      "epoch 84 loss = 2.120852\n",
      "epoch 85 loss = 2.121631\n",
      "epoch 86 loss = 1.894669\n",
      "epoch 87 loss = 2.293311\n",
      "epoch 88 loss = 2.015117\n",
      "epoch 89 loss = 2.108846\n",
      "epoch 90 loss = 2.149789\n",
      "epoch 91 loss = 2.182932\n",
      "epoch 92 loss = 2.174585\n",
      "epoch 93 loss = 2.193494\n",
      "epoch 94 loss = 2.117991\n",
      "epoch 95 loss = 1.992260\n",
      "epoch 96 loss = 2.158166\n",
      "epoch 97 loss = 2.000991\n",
      "epoch 98 loss = 1.928161\n",
      "epoch 99 loss = 1.983567\n",
      "epoch 100 loss = 2.031523\n",
      "epoch 101 loss = 1.990792\n",
      "epoch 102 loss = 2.031268\n",
      "epoch 103 loss = 2.065820\n",
      "epoch 104 loss = 1.926337\n",
      "epoch 105 loss = 2.141060\n",
      "epoch 106 loss = 2.000304\n",
      "epoch 107 loss = 1.923847\n",
      "epoch 108 loss = 1.982028\n",
      "epoch 109 loss = 1.962860\n",
      "epoch 110 loss = 1.966245\n",
      "epoch 111 loss = 2.008529\n",
      "epoch 112 loss = 2.168562\n",
      "epoch 113 loss = 2.118268\n",
      "epoch 114 loss = 1.896828\n",
      "epoch 115 loss = 2.157431\n",
      "epoch 116 loss = 2.101550\n",
      "epoch 117 loss = 2.081315\n",
      "epoch 118 loss = 1.907389\n",
      "epoch 119 loss = 2.025980\n",
      "epoch 120 loss = 1.974623\n",
      "epoch 121 loss = 2.058337\n",
      "epoch 122 loss = 1.955407\n",
      "epoch 123 loss = 1.964311\n",
      "epoch 124 loss = 1.969758\n",
      "epoch 125 loss = 2.059702\n",
      "epoch 126 loss = 1.971514\n",
      "epoch 127 loss = 2.060345\n",
      "epoch 128 loss = 1.993773\n",
      "epoch 129 loss = 2.312162\n",
      "epoch 130 loss = 2.257440\n",
      "epoch 131 loss = 2.088274\n",
      "epoch 132 loss = 2.205635\n",
      "epoch 133 loss = 1.922969\n",
      "epoch 134 loss = 2.227196\n",
      "epoch 135 loss = 1.987805\n",
      "epoch 136 loss = 2.016698\n",
      "epoch 137 loss = 1.937576\n",
      "epoch 138 loss = 1.952129\n",
      "epoch 139 loss = 1.940188\n",
      "epoch 140 loss = 2.306090\n",
      "epoch 141 loss = 2.141634\n",
      "epoch 142 loss = 1.861522\n",
      "epoch 143 loss = 2.245149\n",
      "epoch 144 loss = 2.131839\n",
      "epoch 145 loss = 2.111751\n",
      "epoch 146 loss = 2.027443\n",
      "epoch 147 loss = 1.918702\n",
      "epoch 148 loss = 2.002215\n",
      "epoch 149 loss = 2.004427\n",
      "epoch 150 loss = 2.018411\n",
      "epoch 151 loss = 2.100912\n",
      "epoch 152 loss = 2.139778\n",
      "epoch 153 loss = 2.273356\n",
      "epoch 154 loss = 2.163272\n",
      "epoch 155 loss = 2.014416\n",
      "epoch 156 loss = 2.087588\n",
      "epoch 157 loss = 2.253267\n",
      "epoch 158 loss = 2.101743\n",
      "epoch 159 loss = 2.202052\n",
      "epoch 160 loss = 1.985509\n",
      "epoch 161 loss = 2.013096\n",
      "epoch 162 loss = 2.073388\n",
      "epoch 163 loss = 2.020173\n",
      "epoch 164 loss = 1.992704\n",
      "epoch 165 loss = 2.015682\n",
      "epoch 166 loss = 1.929458\n",
      "epoch 167 loss = 2.174537\n",
      "epoch 168 loss = 2.138694\n",
      "epoch 169 loss = 2.045906\n",
      "epoch 170 loss = 2.107781\n",
      "epoch 171 loss = 2.017324\n",
      "epoch 172 loss = 2.161916\n",
      "epoch 173 loss = 1.827435\n",
      "epoch 174 loss = 1.888699\n",
      "epoch 175 loss = 2.126372\n",
      "epoch 176 loss = 2.123594\n",
      "epoch 177 loss = 2.052774\n",
      "epoch 178 loss = 2.204546\n",
      "epoch 179 loss = 2.082373\n",
      "epoch 180 loss = 2.028324\n",
      "epoch 181 loss = 2.350038\n",
      "epoch 182 loss = 1.951536\n",
      "epoch 183 loss = 2.127232\n",
      "epoch 184 loss = 1.925866\n",
      "epoch 185 loss = 2.131870\n",
      "epoch 186 loss = 2.186968\n",
      "epoch 187 loss = 2.068257\n",
      "epoch 188 loss = 1.799292\n",
      "epoch 189 loss = 2.135339\n",
      "epoch 190 loss = 2.105666\n",
      "epoch 191 loss = 2.089438\n",
      "epoch 192 loss = 2.004624\n",
      "epoch 193 loss = 2.233171\n",
      "epoch 194 loss = 1.996871\n",
      "epoch 195 loss = 2.007745\n",
      "epoch 196 loss = 2.072819\n",
      "epoch 197 loss = 2.136846\n",
      "epoch 198 loss = 2.070062\n",
      "epoch 199 loss = 1.922551\n",
      "epoch 200 loss = 2.119676\n",
      "epoch 201 loss = 2.223877\n",
      "epoch 202 loss = 2.077497\n",
      "epoch 203 loss = 2.007346\n",
      "epoch 204 loss = 2.012617\n",
      "epoch 205 loss = 2.222919\n",
      "epoch 206 loss = 2.074120\n",
      "epoch 207 loss = 2.032353\n",
      "epoch 208 loss = 2.082689\n",
      "epoch 209 loss = 1.765680\n",
      "epoch 210 loss = 1.937708\n",
      "epoch 211 loss = 2.081812\n",
      "epoch 212 loss = 2.087821\n",
      "epoch 213 loss = 2.062979\n",
      "epoch 214 loss = 1.937918\n",
      "epoch 215 loss = 1.931213\n",
      "epoch 216 loss = 2.070198\n",
      "epoch 217 loss = 2.074519\n",
      "epoch 218 loss = 1.941506\n",
      "epoch 219 loss = 2.110037\n",
      "epoch 220 loss = 2.136889\n",
      "epoch 221 loss = 2.171964\n",
      "epoch 222 loss = 2.117648\n",
      "epoch 223 loss = 2.157726\n",
      "epoch 224 loss = 1.989687\n",
      "epoch 225 loss = 1.892768\n",
      "epoch 226 loss = 1.899342\n",
      "epoch 227 loss = 2.029934\n",
      "epoch 228 loss = 1.814249\n",
      "epoch 229 loss = 2.046499\n",
      "epoch 230 loss = 2.030995\n",
      "epoch 231 loss = 1.991509\n",
      "epoch 232 loss = 2.026691\n",
      "epoch 233 loss = 2.102625\n",
      "epoch 234 loss = 2.093596\n",
      "epoch 235 loss = 2.097648\n",
      "epoch 236 loss = 2.053194\n",
      "epoch 237 loss = 1.979173\n",
      "epoch 238 loss = 1.889250\n",
      "epoch 239 loss = 2.144847\n",
      "epoch 240 loss = 2.009821\n",
      "epoch 241 loss = 2.139979\n",
      "epoch 242 loss = 2.146080\n",
      "epoch 243 loss = 2.163515\n",
      "epoch 244 loss = 2.040618\n",
      "epoch 245 loss = 2.013931\n",
      "epoch 246 loss = 2.207576\n",
      "epoch 247 loss = 2.258887\n",
      "epoch 248 loss = 2.129148\n",
      "epoch 249 loss = 2.028916\n",
      "epoch 250 loss = 2.293508\n",
      "epoch 251 loss = 2.090567\n",
      "epoch 252 loss = 1.993779\n",
      "epoch 253 loss = 2.162253\n",
      "epoch 254 loss = 2.148977\n",
      "epoch 255 loss = 2.036987\n",
      "epoch 256 loss = 2.149409\n",
      "epoch 257 loss = 2.089228\n",
      "epoch 258 loss = 2.116456\n",
      "epoch 259 loss = 1.946018\n",
      "epoch 260 loss = 1.996889\n",
      "epoch 261 loss = 2.109782\n",
      "epoch 262 loss = 2.076274\n",
      "epoch 263 loss = 2.111992\n",
      "epoch 264 loss = 1.947649\n",
      "epoch 265 loss = 2.231702\n",
      "epoch 266 loss = 2.095799\n",
      "epoch 267 loss = 2.076814\n",
      "epoch 268 loss = 2.174861\n",
      "epoch 269 loss = 2.222413\n",
      "epoch 270 loss = 1.934828\n",
      "epoch 271 loss = 2.129780\n",
      "epoch 272 loss = 2.143098\n",
      "epoch 273 loss = 1.959440\n",
      "epoch 274 loss = 2.097759\n",
      "epoch 275 loss = 2.033450\n",
      "epoch 276 loss = 1.898408\n",
      "epoch 277 loss = 2.041365\n",
      "epoch 278 loss = 2.273961\n",
      "epoch 279 loss = 1.923006\n",
      "epoch 280 loss = 1.962890\n",
      "epoch 281 loss = 2.183023\n",
      "epoch 282 loss = 2.063504\n",
      "epoch 283 loss = 2.025735\n",
      "epoch 284 loss = 2.093117\n",
      "epoch 285 loss = 2.145583\n",
      "epoch 286 loss = 2.091731\n",
      "epoch 287 loss = 2.033767\n",
      "epoch 288 loss = 2.047562\n",
      "epoch 289 loss = 1.980222\n",
      "epoch 290 loss = 2.190895\n",
      "epoch 291 loss = 2.003697\n",
      "epoch 292 loss = 2.047364\n",
      "epoch 293 loss = 2.073843\n",
      "epoch 294 loss = 2.223942\n",
      "epoch 295 loss = 1.986430\n",
      "epoch 296 loss = 1.988673\n",
      "epoch 297 loss = 1.880652\n",
      "epoch 298 loss = 2.099005\n",
      "epoch 299 loss = 1.979579\n",
      "epoch 300 loss = 2.208299\n",
      "epoch 301 loss = 2.120582\n",
      "epoch 302 loss = 2.079865\n",
      "epoch 303 loss = 1.869434\n",
      "epoch 304 loss = 2.152281\n",
      "epoch 305 loss = 2.070626\n",
      "epoch 306 loss = 2.030422\n",
      "epoch 307 loss = 1.978783\n",
      "epoch 308 loss = 2.235891\n",
      "epoch 309 loss = 2.126758\n",
      "epoch 310 loss = 2.174692\n",
      "epoch 311 loss = 2.001328\n",
      "epoch 312 loss = 2.055332\n",
      "epoch 313 loss = 2.084634\n",
      "epoch 314 loss = 2.169373\n",
      "epoch 315 loss = 2.078411\n",
      "epoch 316 loss = 1.907814\n",
      "epoch 317 loss = 1.982330\n",
      "epoch 318 loss = 2.126237\n",
      "epoch 319 loss = 2.059430\n",
      "epoch 320 loss = 1.896398\n",
      "epoch 321 loss = 2.030239\n",
      "epoch 322 loss = 1.997304\n",
      "epoch 323 loss = 1.869228\n",
      "epoch 324 loss = 2.109710\n",
      "epoch 325 loss = 2.170599\n",
      "epoch 326 loss = 2.038646\n",
      "epoch 327 loss = 2.202842\n",
      "epoch 328 loss = 2.264660\n",
      "epoch 329 loss = 2.017971\n",
      "epoch 330 loss = 2.072852\n",
      "epoch 331 loss = 2.059535\n",
      "epoch 332 loss = 2.111361\n",
      "epoch 333 loss = 2.029610\n",
      "epoch 334 loss = 2.198640\n",
      "epoch 335 loss = 2.026911\n",
      "epoch 336 loss = 2.147939\n",
      "epoch 337 loss = 2.071059\n",
      "epoch 338 loss = 2.021800\n",
      "epoch 339 loss = 2.161546\n",
      "epoch 340 loss = 2.175400\n",
      "epoch 341 loss = 2.199559\n",
      "epoch 342 loss = 2.058196\n",
      "epoch 343 loss = 2.020201\n",
      "epoch 344 loss = 2.038046\n",
      "epoch 345 loss = 2.028339\n",
      "epoch 346 loss = 2.140488\n",
      "epoch 347 loss = 2.057966\n",
      "epoch 348 loss = 2.033496\n",
      "epoch 349 loss = 2.094208\n",
      "epoch 350 loss = 2.005230\n",
      "epoch 351 loss = 2.004491\n",
      "epoch 352 loss = 1.985314\n",
      "epoch 353 loss = 2.126050\n",
      "epoch 354 loss = 2.168541\n",
      "epoch 355 loss = 2.047774\n",
      "epoch 356 loss = 2.089806\n",
      "epoch 357 loss = 2.161197\n",
      "epoch 358 loss = 2.144850\n",
      "epoch 359 loss = 2.217313\n",
      "epoch 360 loss = 2.159494\n",
      "epoch 361 loss = 1.907600\n",
      "epoch 362 loss = 2.006249\n",
      "epoch 363 loss = 2.130862\n",
      "epoch 364 loss = 2.240499\n",
      "epoch 365 loss = 1.955765\n",
      "epoch 366 loss = 1.976751\n",
      "epoch 367 loss = 1.984224\n",
      "epoch 368 loss = 1.985466\n",
      "epoch 369 loss = 1.930385\n",
      "epoch 370 loss = 1.972401\n",
      "epoch 371 loss = 2.017296\n",
      "epoch 372 loss = 2.033164\n",
      "epoch 373 loss = 2.122753\n",
      "epoch 374 loss = 1.959678\n",
      "epoch 375 loss = 1.965713\n",
      "epoch 376 loss = 2.038594\n",
      "epoch 377 loss = 2.096622\n",
      "epoch 378 loss = 2.070720\n",
      "epoch 379 loss = 1.957562\n",
      "epoch 380 loss = 2.015276\n",
      "epoch 381 loss = 1.981734\n",
      "epoch 382 loss = 2.147045\n",
      "epoch 383 loss = 2.094481\n",
      "epoch 384 loss = 1.937118\n",
      "epoch 385 loss = 2.047354\n",
      "epoch 386 loss = 2.171001\n",
      "epoch 387 loss = 2.208528\n",
      "epoch 388 loss = 2.168514\n",
      "epoch 389 loss = 1.896106\n",
      "epoch 390 loss = 2.194296\n",
      "epoch 391 loss = 1.996493\n",
      "epoch 392 loss = 2.150753\n",
      "epoch 393 loss = 2.115349\n",
      "epoch 394 loss = 2.156368\n",
      "epoch 395 loss = 2.012034\n",
      "epoch 396 loss = 2.010005\n",
      "epoch 397 loss = 1.971326\n",
      "epoch 398 loss = 2.004359\n",
      "epoch 399 loss = 2.117161\n",
      "epoch 400 loss = 2.147792\n",
      "epoch 401 loss = 1.828395\n",
      "epoch 402 loss = 2.149717\n",
      "epoch 403 loss = 2.175404\n",
      "epoch 404 loss = 2.062235\n",
      "epoch 405 loss = 2.192318\n",
      "epoch 406 loss = 2.171439\n",
      "epoch 407 loss = 1.960733\n",
      "epoch 408 loss = 2.094718\n",
      "epoch 409 loss = 2.037920\n",
      "epoch 410 loss = 1.861862\n",
      "epoch 411 loss = 2.052767\n",
      "epoch 412 loss = 2.233235\n",
      "epoch 413 loss = 2.108370\n",
      "epoch 414 loss = 2.004307\n",
      "epoch 415 loss = 2.039724\n",
      "epoch 416 loss = 1.822530\n",
      "epoch 417 loss = 1.974931\n",
      "epoch 418 loss = 2.141318\n",
      "epoch 419 loss = 1.950479\n",
      "epoch 420 loss = 1.914565\n",
      "epoch 421 loss = 2.102496\n",
      "epoch 422 loss = 2.059534\n",
      "epoch 423 loss = 2.130139\n",
      "epoch 424 loss = 2.108022\n",
      "epoch 425 loss = 1.942120\n",
      "epoch 426 loss = 1.995270\n",
      "epoch 427 loss = 1.837654\n",
      "epoch 428 loss = 1.755316\n",
      "epoch 429 loss = 2.164625\n",
      "epoch 430 loss = 2.010891\n",
      "epoch 431 loss = 1.843316\n",
      "epoch 432 loss = 1.841476\n",
      "epoch 433 loss = 2.285387\n",
      "epoch 434 loss = 2.162552\n",
      "epoch 435 loss = 1.867814\n",
      "epoch 436 loss = 1.861061\n",
      "epoch 437 loss = 1.978263\n",
      "epoch 438 loss = 2.071024\n",
      "epoch 439 loss = 1.975197\n",
      "epoch 440 loss = 2.200151\n",
      "epoch 441 loss = 2.019258\n",
      "epoch 442 loss = 2.104644\n",
      "epoch 443 loss = 2.096512\n",
      "epoch 444 loss = 1.776945\n",
      "epoch 445 loss = 2.161546\n",
      "epoch 446 loss = 1.971556\n",
      "epoch 447 loss = 1.929247\n",
      "epoch 448 loss = 1.974719\n",
      "epoch 449 loss = 1.957934\n",
      "epoch 450 loss = 2.003637\n",
      "epoch 451 loss = 1.910907\n",
      "epoch 452 loss = 2.357261\n",
      "epoch 453 loss = 1.900952\n",
      "epoch 454 loss = 2.013709\n",
      "epoch 455 loss = 1.973832\n",
      "epoch 456 loss = 2.170655\n",
      "epoch 457 loss = 2.055362\n",
      "epoch 458 loss = 2.106451\n",
      "epoch 459 loss = 2.000539\n",
      "epoch 460 loss = 2.032119\n",
      "epoch 461 loss = 1.988275\n",
      "epoch 462 loss = 2.201393\n",
      "epoch 463 loss = 1.985541\n",
      "epoch 464 loss = 1.975564\n",
      "epoch 465 loss = 1.930008\n",
      "epoch 466 loss = 1.991703\n",
      "epoch 467 loss = 2.106162\n",
      "epoch 468 loss = 2.077991\n",
      "epoch 469 loss = 2.135293\n",
      "epoch 470 loss = 2.075876\n",
      "epoch 471 loss = 1.901140\n",
      "epoch 472 loss = 2.017531\n",
      "epoch 473 loss = 2.056086\n",
      "epoch 474 loss = 1.930916\n",
      "epoch 475 loss = 2.218256\n",
      "epoch 476 loss = 1.932456\n",
      "epoch 477 loss = 2.137500\n",
      "epoch 478 loss = 2.212969\n",
      "epoch 479 loss = 1.972970\n",
      "epoch 480 loss = 1.912330\n",
      "epoch 481 loss = 1.946444\n",
      "epoch 482 loss = 1.856729\n",
      "epoch 483 loss = 2.178116\n",
      "epoch 484 loss = 2.241000\n",
      "epoch 485 loss = 2.066308\n",
      "epoch 486 loss = 2.225182\n",
      "epoch 487 loss = 1.889289\n",
      "epoch 488 loss = 2.142923\n",
      "epoch 489 loss = 2.173049\n",
      "epoch 490 loss = 2.015381\n",
      "epoch 491 loss = 2.028662\n",
      "epoch 492 loss = 1.876461\n",
      "epoch 493 loss = 2.019982\n",
      "epoch 494 loss = 1.974481\n",
      "epoch 495 loss = 2.070074\n",
      "epoch 496 loss = 1.987424\n",
      "epoch 497 loss = 2.059371\n",
      "epoch 498 loss = 2.127763\n",
      "epoch 499 loss = 2.088708\n",
      "final loss = 2.088708\n",
      "accuracy_mc = tensor(0.2640, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1626, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9910, device='cuda:0')\n",
      "training time = 168.81394624710083 seconds\n",
      "testing time = 1.8510956764221191 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = 2.284559\n",
      "epoch 1 loss = 2.220279\n",
      "epoch 2 loss = 2.184965\n",
      "epoch 3 loss = 2.251066\n",
      "epoch 4 loss = 2.230193\n",
      "epoch 5 loss = 2.140033\n",
      "epoch 6 loss = 2.210257\n",
      "epoch 7 loss = 2.265631\n",
      "epoch 8 loss = 2.146979\n",
      "epoch 9 loss = 2.086957\n",
      "epoch 10 loss = 2.154228\n",
      "epoch 11 loss = 2.132482\n",
      "epoch 12 loss = 2.120801\n",
      "epoch 13 loss = 2.115419\n",
      "epoch 14 loss = 2.041545\n",
      "epoch 15 loss = 2.194612\n",
      "epoch 16 loss = 2.227542\n",
      "epoch 17 loss = 2.157832\n",
      "epoch 18 loss = 2.226079\n",
      "epoch 19 loss = 2.154170\n",
      "epoch 20 loss = 2.186659\n",
      "epoch 21 loss = 2.161823\n",
      "epoch 22 loss = 2.171934\n",
      "epoch 23 loss = 2.202759\n",
      "epoch 24 loss = 2.058669\n",
      "epoch 25 loss = 2.030235\n",
      "epoch 26 loss = 2.197773\n",
      "epoch 27 loss = 2.063802\n",
      "epoch 28 loss = 2.074544\n",
      "epoch 29 loss = 2.024013\n",
      "epoch 30 loss = 2.049129\n",
      "epoch 31 loss = 2.130829\n",
      "epoch 32 loss = 2.117035\n",
      "epoch 33 loss = 2.007690\n",
      "epoch 34 loss = 2.193291\n",
      "epoch 35 loss = 2.208679\n",
      "epoch 36 loss = 2.276863\n",
      "epoch 37 loss = 2.157911\n",
      "epoch 38 loss = 2.028623\n",
      "epoch 39 loss = 2.150925\n",
      "epoch 40 loss = 2.027886\n",
      "epoch 41 loss = 2.194395\n",
      "epoch 42 loss = 2.186446\n",
      "epoch 43 loss = 1.961694\n",
      "epoch 44 loss = 2.149574\n",
      "epoch 45 loss = 2.014024\n",
      "epoch 46 loss = 1.996704\n",
      "epoch 47 loss = 2.009185\n",
      "epoch 48 loss = 2.213886\n",
      "epoch 49 loss = 2.008990\n",
      "epoch 50 loss = 2.104121\n",
      "epoch 51 loss = 2.091593\n",
      "epoch 52 loss = 1.925375\n",
      "epoch 53 loss = 2.185786\n",
      "epoch 54 loss = 2.036297\n",
      "epoch 55 loss = 2.053568\n",
      "epoch 56 loss = 2.183486\n",
      "epoch 57 loss = 2.101001\n",
      "epoch 58 loss = 1.884699\n",
      "epoch 59 loss = 2.039658\n",
      "epoch 60 loss = 2.083501\n",
      "epoch 61 loss = 2.240898\n",
      "epoch 62 loss = 2.080314\n",
      "epoch 63 loss = 2.108412\n",
      "epoch 64 loss = 2.128492\n",
      "epoch 65 loss = 2.007821\n",
      "epoch 66 loss = 2.125358\n",
      "epoch 67 loss = 2.118847\n",
      "epoch 68 loss = 2.065944\n",
      "epoch 69 loss = 2.030940\n",
      "epoch 70 loss = 2.102931\n",
      "epoch 71 loss = 2.073036\n",
      "epoch 72 loss = 2.085104\n",
      "epoch 73 loss = 2.093293\n",
      "epoch 74 loss = 2.156204\n",
      "epoch 75 loss = 2.000941\n",
      "epoch 76 loss = 2.139978\n",
      "epoch 77 loss = 2.122381\n",
      "epoch 78 loss = 2.158241\n",
      "epoch 79 loss = 2.190071\n",
      "epoch 80 loss = 2.011926\n",
      "epoch 81 loss = 2.011543\n",
      "epoch 82 loss = 1.976748\n",
      "epoch 83 loss = 2.043621\n",
      "epoch 84 loss = 2.102577\n",
      "epoch 85 loss = 2.110162\n",
      "epoch 86 loss = 2.012537\n",
      "epoch 87 loss = 2.007159\n",
      "epoch 88 loss = 2.121507\n",
      "epoch 89 loss = 2.104338\n",
      "epoch 90 loss = 2.153794\n",
      "epoch 91 loss = 2.050532\n",
      "epoch 92 loss = 1.999798\n",
      "epoch 93 loss = 1.883991\n",
      "epoch 94 loss = 2.022263\n",
      "epoch 95 loss = 2.061337\n",
      "epoch 96 loss = 2.188503\n",
      "epoch 97 loss = 1.995724\n",
      "epoch 98 loss = 2.076539\n",
      "epoch 99 loss = 2.013325\n",
      "epoch 100 loss = 2.144839\n",
      "epoch 101 loss = 2.098319\n",
      "epoch 102 loss = 2.062172\n",
      "epoch 103 loss = 2.171755\n",
      "epoch 104 loss = 2.017714\n",
      "epoch 105 loss = 2.094669\n",
      "epoch 106 loss = 2.050110\n",
      "epoch 107 loss = 2.164167\n",
      "epoch 108 loss = 2.119935\n",
      "epoch 109 loss = 2.161727\n",
      "epoch 110 loss = 2.137572\n",
      "epoch 111 loss = 2.050086\n",
      "epoch 112 loss = 1.916283\n",
      "epoch 113 loss = 1.978838\n",
      "epoch 114 loss = 2.034873\n",
      "epoch 115 loss = 2.166063\n",
      "epoch 116 loss = 2.151787\n",
      "epoch 117 loss = 2.204741\n",
      "epoch 118 loss = 2.085135\n",
      "epoch 119 loss = 2.121418\n",
      "epoch 120 loss = 2.007651\n",
      "epoch 121 loss = 2.065704\n",
      "epoch 122 loss = 1.948392\n",
      "epoch 123 loss = 2.109288\n",
      "epoch 124 loss = 2.118953\n",
      "epoch 125 loss = 2.074879\n",
      "epoch 126 loss = 2.125333\n",
      "epoch 127 loss = 1.993242\n",
      "epoch 128 loss = 2.225402\n",
      "epoch 129 loss = 2.021410\n",
      "epoch 130 loss = 2.239214\n",
      "epoch 131 loss = 2.134465\n",
      "epoch 132 loss = 2.080590\n",
      "epoch 133 loss = 2.075435\n",
      "epoch 134 loss = 1.930794\n",
      "epoch 135 loss = 2.057769\n",
      "epoch 136 loss = 1.975791\n",
      "epoch 137 loss = 1.999172\n",
      "epoch 138 loss = 2.205862\n",
      "epoch 139 loss = 2.243080\n",
      "epoch 140 loss = 2.056389\n",
      "epoch 141 loss = 2.168725\n",
      "epoch 142 loss = 1.987772\n",
      "epoch 143 loss = 2.125662\n",
      "epoch 144 loss = 1.893959\n",
      "epoch 145 loss = 2.035023\n",
      "epoch 146 loss = 2.123771\n",
      "epoch 147 loss = 2.112350\n",
      "epoch 148 loss = 1.982537\n",
      "epoch 149 loss = 2.044692\n",
      "epoch 150 loss = 2.060618\n",
      "epoch 151 loss = 2.133950\n",
      "epoch 152 loss = 1.971701\n",
      "epoch 153 loss = 2.020282\n",
      "epoch 154 loss = 2.012473\n",
      "epoch 155 loss = 2.078998\n",
      "epoch 156 loss = 2.009209\n",
      "epoch 157 loss = 2.074423\n",
      "epoch 158 loss = 2.159351\n",
      "epoch 159 loss = 2.123601\n",
      "epoch 160 loss = 2.028744\n",
      "epoch 161 loss = 2.071775\n",
      "epoch 162 loss = 2.129943\n",
      "epoch 163 loss = 2.102731\n",
      "epoch 164 loss = 2.128867\n",
      "epoch 165 loss = 2.133464\n",
      "epoch 166 loss = 2.074107\n",
      "epoch 167 loss = 1.838910\n",
      "epoch 168 loss = 2.111303\n",
      "epoch 169 loss = 1.995583\n",
      "epoch 170 loss = 2.117034\n",
      "epoch 171 loss = 1.787517\n",
      "epoch 172 loss = 2.189701\n",
      "epoch 173 loss = 2.090770\n",
      "epoch 174 loss = 2.095459\n",
      "epoch 175 loss = 2.007614\n",
      "epoch 176 loss = 1.986887\n",
      "epoch 177 loss = 2.107142\n",
      "epoch 178 loss = 2.161328\n",
      "epoch 179 loss = 2.003907\n",
      "epoch 180 loss = 1.943965\n",
      "epoch 181 loss = 2.160633\n",
      "epoch 182 loss = 2.032737\n",
      "epoch 183 loss = 2.020942\n",
      "epoch 184 loss = 1.969780\n",
      "epoch 185 loss = 1.977254\n",
      "epoch 186 loss = 2.183633\n",
      "epoch 187 loss = 2.050223\n",
      "epoch 188 loss = 2.195992\n",
      "epoch 189 loss = 2.083127\n",
      "epoch 190 loss = 1.881538\n",
      "epoch 191 loss = 2.017375\n",
      "epoch 192 loss = 2.219999\n",
      "epoch 193 loss = 2.164102\n",
      "epoch 194 loss = 2.137092\n",
      "epoch 195 loss = 1.847686\n",
      "epoch 196 loss = 2.032229\n",
      "epoch 197 loss = 2.004438\n",
      "epoch 198 loss = 2.106717\n",
      "epoch 199 loss = 1.768454\n",
      "epoch 200 loss = 2.076995\n",
      "epoch 201 loss = 2.132308\n",
      "epoch 202 loss = 2.279654\n",
      "epoch 203 loss = 2.064333\n",
      "epoch 204 loss = 2.056170\n",
      "epoch 205 loss = 2.002498\n",
      "epoch 206 loss = 2.103941\n",
      "epoch 207 loss = 2.197832\n",
      "epoch 208 loss = 2.037942\n",
      "epoch 209 loss = 1.994701\n",
      "epoch 210 loss = 2.058750\n",
      "epoch 211 loss = 2.055718\n",
      "epoch 212 loss = 2.155338\n",
      "epoch 213 loss = 1.970047\n",
      "epoch 214 loss = 2.118251\n",
      "epoch 215 loss = 1.972485\n",
      "epoch 216 loss = 2.088717\n",
      "epoch 217 loss = 2.105658\n",
      "epoch 218 loss = 2.155164\n",
      "epoch 219 loss = 1.933912\n",
      "epoch 220 loss = 1.957831\n",
      "epoch 221 loss = 1.956044\n",
      "epoch 222 loss = 1.993149\n",
      "epoch 223 loss = 1.991966\n",
      "epoch 224 loss = 2.005317\n",
      "epoch 225 loss = 2.173315\n",
      "epoch 226 loss = 1.934990\n",
      "epoch 227 loss = 2.035824\n",
      "epoch 228 loss = 2.009358\n",
      "epoch 229 loss = 2.050379\n",
      "epoch 230 loss = 2.162130\n",
      "epoch 231 loss = 1.989585\n",
      "epoch 232 loss = 2.127029\n",
      "epoch 233 loss = 2.145898\n",
      "epoch 234 loss = 1.999482\n",
      "epoch 235 loss = 2.128717\n",
      "epoch 236 loss = 2.059601\n",
      "epoch 237 loss = 2.136842\n",
      "epoch 238 loss = 2.247825\n",
      "epoch 239 loss = 2.086777\n",
      "epoch 240 loss = 2.031709\n",
      "epoch 241 loss = 2.001527\n",
      "epoch 242 loss = 2.142391\n",
      "epoch 243 loss = 1.933014\n",
      "epoch 244 loss = 2.026006\n",
      "epoch 245 loss = 1.966969\n",
      "epoch 246 loss = 1.901224\n",
      "epoch 247 loss = 2.184245\n",
      "epoch 248 loss = 2.203575\n",
      "epoch 249 loss = 2.032665\n",
      "epoch 250 loss = 2.162901\n",
      "epoch 251 loss = 2.096066\n",
      "epoch 252 loss = 2.171586\n",
      "epoch 253 loss = 2.021478\n",
      "epoch 254 loss = 1.972861\n",
      "epoch 255 loss = 2.269920\n",
      "epoch 256 loss = 2.117132\n",
      "epoch 257 loss = 1.873094\n",
      "epoch 258 loss = 1.996175\n",
      "epoch 259 loss = 2.015314\n",
      "epoch 260 loss = 1.990023\n",
      "epoch 261 loss = 1.982287\n",
      "epoch 262 loss = 2.116687\n",
      "epoch 263 loss = 2.191364\n",
      "epoch 264 loss = 2.234107\n",
      "epoch 265 loss = 2.027246\n",
      "epoch 266 loss = 1.951908\n",
      "epoch 267 loss = 1.961443\n",
      "epoch 268 loss = 1.990293\n",
      "epoch 269 loss = 2.129498\n",
      "epoch 270 loss = 1.996426\n",
      "epoch 271 loss = 2.091172\n",
      "epoch 272 loss = 1.824869\n",
      "epoch 273 loss = 2.056509\n",
      "epoch 274 loss = 2.133180\n",
      "epoch 275 loss = 1.976669\n",
      "epoch 276 loss = 1.964158\n",
      "epoch 277 loss = 2.123069\n",
      "epoch 278 loss = 2.011678\n",
      "epoch 279 loss = 2.111660\n",
      "epoch 280 loss = 2.044281\n",
      "epoch 281 loss = 2.047165\n",
      "epoch 282 loss = 2.192672\n",
      "epoch 283 loss = 2.012770\n",
      "epoch 284 loss = 1.870989\n",
      "epoch 285 loss = 2.012423\n",
      "epoch 286 loss = 2.018193\n",
      "epoch 287 loss = 2.133824\n",
      "epoch 288 loss = 2.165143\n",
      "epoch 289 loss = 2.016521\n",
      "epoch 290 loss = 2.065389\n",
      "epoch 291 loss = 2.179915\n",
      "epoch 292 loss = 2.109126\n",
      "epoch 293 loss = 2.013025\n",
      "epoch 294 loss = 1.935339\n",
      "epoch 295 loss = 2.146925\n",
      "epoch 296 loss = 1.762926\n",
      "epoch 297 loss = 1.920681\n",
      "epoch 298 loss = 2.028303\n",
      "epoch 299 loss = 2.151016\n",
      "epoch 300 loss = 1.988191\n",
      "epoch 301 loss = 2.140370\n",
      "epoch 302 loss = 2.015427\n",
      "epoch 303 loss = 2.017047\n",
      "epoch 304 loss = 1.944760\n",
      "epoch 305 loss = 2.013566\n",
      "epoch 306 loss = 1.905803\n",
      "epoch 307 loss = 2.070929\n",
      "epoch 308 loss = 1.840117\n",
      "epoch 309 loss = 2.122516\n",
      "epoch 310 loss = 2.072002\n",
      "epoch 311 loss = 2.141348\n",
      "epoch 312 loss = 2.045910\n",
      "epoch 313 loss = 2.014179\n",
      "epoch 314 loss = 1.973246\n",
      "epoch 315 loss = 1.949267\n",
      "epoch 316 loss = 2.244500\n",
      "epoch 317 loss = 1.978670\n",
      "epoch 318 loss = 1.934259\n",
      "epoch 319 loss = 1.936382\n",
      "epoch 320 loss = 1.996628\n",
      "epoch 321 loss = 2.014441\n",
      "epoch 322 loss = 2.163256\n",
      "epoch 323 loss = 2.002722\n",
      "epoch 324 loss = 1.957795\n",
      "epoch 325 loss = 2.129835\n",
      "epoch 326 loss = 2.038344\n",
      "epoch 327 loss = 2.012242\n",
      "epoch 328 loss = 2.088901\n",
      "epoch 329 loss = 1.960576\n",
      "epoch 330 loss = 2.067750\n",
      "epoch 331 loss = 2.157476\n",
      "epoch 332 loss = 2.037477\n",
      "epoch 333 loss = 1.917096\n",
      "epoch 334 loss = 1.970333\n",
      "epoch 335 loss = 2.014633\n",
      "epoch 336 loss = 1.897058\n",
      "epoch 337 loss = 1.960752\n",
      "epoch 338 loss = 2.085999\n",
      "epoch 339 loss = 2.102391\n",
      "epoch 340 loss = 2.030767\n",
      "epoch 341 loss = 1.984200\n",
      "epoch 342 loss = 2.131394\n",
      "epoch 343 loss = 1.705247\n",
      "epoch 344 loss = 2.082242\n",
      "epoch 345 loss = 1.985990\n",
      "epoch 346 loss = 2.097698\n",
      "epoch 347 loss = 1.947289\n",
      "epoch 348 loss = 2.183230\n",
      "epoch 349 loss = 2.106997\n",
      "epoch 350 loss = 2.175281\n",
      "epoch 351 loss = 2.226852\n",
      "epoch 352 loss = 2.169334\n",
      "epoch 353 loss = 2.078538\n",
      "epoch 354 loss = 2.131421\n",
      "epoch 355 loss = 2.109001\n",
      "epoch 356 loss = 2.025402\n",
      "epoch 357 loss = 2.092604\n",
      "epoch 358 loss = 1.962843\n",
      "epoch 359 loss = 2.055386\n",
      "epoch 360 loss = 2.031784\n",
      "epoch 361 loss = 2.145878\n",
      "epoch 362 loss = 2.058079\n",
      "epoch 363 loss = 2.090618\n",
      "epoch 364 loss = 1.996989\n",
      "epoch 365 loss = 1.995699\n",
      "epoch 366 loss = 2.150638\n",
      "epoch 367 loss = 2.146180\n",
      "epoch 368 loss = 2.078723\n",
      "epoch 369 loss = 2.137413\n",
      "epoch 370 loss = 2.096040\n",
      "epoch 371 loss = 1.922931\n",
      "epoch 372 loss = 2.023513\n",
      "epoch 373 loss = 1.920188\n",
      "epoch 374 loss = 2.084050\n",
      "epoch 375 loss = 2.051761\n",
      "epoch 376 loss = 1.756696\n",
      "epoch 377 loss = 2.025967\n",
      "epoch 378 loss = 1.961675\n",
      "epoch 379 loss = 2.008390\n",
      "epoch 380 loss = 2.031101\n",
      "epoch 381 loss = 2.074706\n",
      "epoch 382 loss = 2.117408\n",
      "epoch 383 loss = 1.979100\n",
      "epoch 384 loss = 2.018967\n",
      "epoch 385 loss = 1.895473\n",
      "epoch 386 loss = 1.875452\n",
      "epoch 387 loss = 1.992663\n",
      "epoch 388 loss = 2.184330\n",
      "epoch 389 loss = 2.104827\n",
      "epoch 390 loss = 1.945514\n",
      "epoch 391 loss = 2.131444\n",
      "epoch 392 loss = 1.964747\n",
      "epoch 393 loss = 1.988473\n",
      "epoch 394 loss = 1.849207\n",
      "epoch 395 loss = 2.154971\n",
      "epoch 396 loss = 2.000290\n",
      "epoch 397 loss = 2.059327\n",
      "epoch 398 loss = 2.103257\n",
      "epoch 399 loss = 2.063724\n",
      "epoch 400 loss = 1.989506\n",
      "epoch 401 loss = 2.092052\n",
      "epoch 402 loss = 1.919272\n",
      "epoch 403 loss = 1.931641\n",
      "epoch 404 loss = 1.906301\n",
      "epoch 405 loss = 2.172579\n",
      "epoch 406 loss = 1.901273\n",
      "epoch 407 loss = 2.194825\n",
      "epoch 408 loss = 1.992227\n",
      "epoch 409 loss = 2.083592\n",
      "epoch 410 loss = 2.144607\n",
      "epoch 411 loss = 2.031682\n",
      "epoch 412 loss = 2.052480\n",
      "epoch 413 loss = 1.985793\n",
      "epoch 414 loss = 1.974127\n",
      "epoch 415 loss = 2.048262\n",
      "epoch 416 loss = 2.010854\n",
      "epoch 417 loss = 2.013471\n",
      "epoch 418 loss = 2.090053\n",
      "epoch 419 loss = 2.239686\n",
      "epoch 420 loss = 1.953560\n",
      "epoch 421 loss = 1.958950\n",
      "epoch 422 loss = 2.098333\n",
      "epoch 423 loss = 2.054459\n",
      "epoch 424 loss = 1.929624\n",
      "epoch 425 loss = 1.991910\n",
      "epoch 426 loss = 1.890671\n",
      "epoch 427 loss = 1.964601\n",
      "epoch 428 loss = 2.060071\n",
      "epoch 429 loss = 2.073294\n",
      "epoch 430 loss = 2.038594\n",
      "epoch 431 loss = 2.040222\n",
      "epoch 432 loss = 2.027871\n",
      "epoch 433 loss = 2.107694\n",
      "epoch 434 loss = 2.148072\n",
      "epoch 435 loss = 2.087841\n",
      "epoch 436 loss = 2.056585\n",
      "epoch 437 loss = 2.064830\n",
      "epoch 438 loss = 2.012095\n",
      "epoch 439 loss = 1.859659\n",
      "epoch 440 loss = 2.039622\n",
      "epoch 441 loss = 1.982400\n",
      "epoch 442 loss = 2.067066\n",
      "epoch 443 loss = 2.034961\n",
      "epoch 444 loss = 2.104391\n",
      "epoch 445 loss = 1.976243\n",
      "epoch 446 loss = 2.075032\n",
      "epoch 447 loss = 2.022286\n",
      "epoch 448 loss = 2.033681\n",
      "epoch 449 loss = 2.108385\n",
      "epoch 450 loss = 2.098836\n",
      "epoch 451 loss = 2.040291\n",
      "epoch 452 loss = 2.119050\n",
      "epoch 453 loss = 2.110122\n",
      "epoch 454 loss = 2.058804\n",
      "epoch 455 loss = 1.945661\n",
      "epoch 456 loss = 1.887989\n",
      "epoch 457 loss = 1.998017\n",
      "epoch 458 loss = 2.099413\n",
      "epoch 459 loss = 1.835020\n",
      "epoch 460 loss = 1.934089\n",
      "epoch 461 loss = 2.221155\n",
      "epoch 462 loss = 1.758894\n",
      "epoch 463 loss = 1.998962\n",
      "epoch 464 loss = 1.978142\n",
      "epoch 465 loss = 1.935616\n",
      "epoch 466 loss = 1.950125\n",
      "epoch 467 loss = 2.069935\n",
      "epoch 468 loss = 1.928806\n",
      "epoch 469 loss = 2.085237\n",
      "epoch 470 loss = 1.982576\n",
      "epoch 471 loss = 2.091993\n",
      "epoch 472 loss = 2.088745\n",
      "epoch 473 loss = 1.845303\n",
      "epoch 474 loss = 2.105377\n",
      "epoch 475 loss = 1.948365\n",
      "epoch 476 loss = 2.119088\n",
      "epoch 477 loss = 2.099794\n",
      "epoch 478 loss = 2.009758\n",
      "epoch 479 loss = 2.017344\n",
      "epoch 480 loss = 2.159132\n",
      "epoch 481 loss = 1.907787\n",
      "epoch 482 loss = 1.932962\n",
      "epoch 483 loss = 2.157974\n",
      "epoch 484 loss = 2.030478\n",
      "epoch 485 loss = 2.070524\n",
      "epoch 486 loss = 2.114219\n",
      "epoch 487 loss = 1.902288\n",
      "epoch 488 loss = 1.910620\n",
      "epoch 489 loss = 2.101865\n",
      "epoch 490 loss = 1.953310\n",
      "epoch 491 loss = 2.085150\n",
      "epoch 492 loss = 1.942460\n",
      "epoch 493 loss = 2.027057\n",
      "epoch 494 loss = 2.043112\n",
      "epoch 495 loss = 1.964546\n",
      "epoch 496 loss = 1.986359\n",
      "epoch 497 loss = 2.114583\n",
      "epoch 498 loss = 1.714391\n",
      "epoch 499 loss = 1.916999\n",
      "final loss = 1.916999\n",
      "accuracy_mc = tensor(0.2778, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2390, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9744, device='cuda:0')\n",
      "training time = 172.35707020759583 seconds\n",
      "testing time = 1.8771493434906006 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = 2.260431\n",
      "epoch 1 loss = 2.259613\n",
      "epoch 2 loss = 2.256073\n",
      "epoch 3 loss = 2.051630\n",
      "epoch 4 loss = 2.150387\n",
      "epoch 5 loss = 2.231033\n",
      "epoch 6 loss = 2.189501\n",
      "epoch 7 loss = 2.084666\n",
      "epoch 8 loss = 2.074926\n",
      "epoch 9 loss = 2.136445\n",
      "epoch 10 loss = 2.183554\n",
      "epoch 11 loss = 2.068074\n",
      "epoch 12 loss = 2.037501\n",
      "epoch 13 loss = 2.101547\n",
      "epoch 14 loss = 2.096068\n",
      "epoch 15 loss = 2.096725\n",
      "epoch 16 loss = 2.006570\n",
      "epoch 17 loss = 2.079507\n",
      "epoch 18 loss = 1.973841\n",
      "epoch 19 loss = 1.991743\n",
      "epoch 20 loss = 1.978678\n",
      "epoch 21 loss = 2.077252\n",
      "epoch 22 loss = 2.150175\n",
      "epoch 23 loss = 2.088481\n",
      "epoch 24 loss = 2.040161\n",
      "epoch 25 loss = 1.982824\n",
      "epoch 26 loss = 2.183680\n",
      "epoch 27 loss = 1.948599\n",
      "epoch 28 loss = 1.950494\n",
      "epoch 29 loss = 1.897763\n",
      "epoch 30 loss = 1.997423\n",
      "epoch 31 loss = 1.900710\n",
      "epoch 32 loss = 2.054425\n",
      "epoch 33 loss = 2.107407\n",
      "epoch 34 loss = 2.026174\n",
      "epoch 35 loss = 1.899083\n",
      "epoch 36 loss = 1.971334\n",
      "epoch 37 loss = 1.935936\n",
      "epoch 38 loss = 2.052850\n",
      "epoch 39 loss = 1.831018\n",
      "epoch 40 loss = 2.205132\n",
      "epoch 41 loss = 2.062218\n",
      "epoch 42 loss = 2.289529\n",
      "epoch 43 loss = 2.039361\n",
      "epoch 44 loss = 2.029113\n",
      "epoch 45 loss = 1.830800\n",
      "epoch 46 loss = 1.951862\n",
      "epoch 47 loss = 2.039202\n",
      "epoch 48 loss = 1.830783\n",
      "epoch 49 loss = 1.936237\n",
      "epoch 50 loss = 1.936586\n",
      "epoch 51 loss = 1.976905\n",
      "epoch 52 loss = 2.022224\n",
      "epoch 53 loss = 1.963207\n",
      "epoch 54 loss = 1.976728\n",
      "epoch 55 loss = 1.977937\n",
      "epoch 56 loss = 2.098075\n",
      "epoch 57 loss = 1.930805\n",
      "epoch 58 loss = 1.900973\n",
      "epoch 59 loss = 2.276707\n",
      "epoch 60 loss = 1.899293\n",
      "epoch 61 loss = 2.091800\n",
      "epoch 62 loss = 1.885283\n",
      "epoch 63 loss = 2.052843\n",
      "epoch 64 loss = 1.978523\n",
      "epoch 65 loss = 2.160058\n",
      "epoch 66 loss = 1.924169\n",
      "epoch 67 loss = 2.228399\n",
      "epoch 68 loss = 1.823167\n",
      "epoch 69 loss = 1.935799\n",
      "epoch 70 loss = 2.169807\n",
      "epoch 71 loss = 1.977466\n",
      "epoch 72 loss = 1.965574\n",
      "epoch 73 loss = 1.892077\n",
      "epoch 74 loss = 1.902320\n",
      "epoch 75 loss = 1.988450\n",
      "epoch 76 loss = 1.938920\n",
      "epoch 77 loss = 1.860379\n",
      "epoch 78 loss = 2.058292\n",
      "epoch 79 loss = 1.887964\n",
      "epoch 80 loss = 1.973533\n",
      "epoch 81 loss = 1.925041\n",
      "epoch 82 loss = 1.984025\n",
      "epoch 83 loss = 1.869736\n",
      "epoch 84 loss = 2.067046\n",
      "epoch 85 loss = 1.779602\n",
      "epoch 86 loss = 1.908800\n",
      "epoch 87 loss = 2.034901\n",
      "epoch 88 loss = 2.084219\n",
      "epoch 89 loss = 2.067061\n",
      "epoch 90 loss = 1.874364\n",
      "epoch 91 loss = 1.988597\n",
      "epoch 92 loss = 1.738451\n",
      "epoch 93 loss = 2.008754\n",
      "epoch 94 loss = 2.074481\n",
      "epoch 95 loss = 2.052561\n",
      "epoch 96 loss = 1.936780\n",
      "epoch 97 loss = 1.956405\n",
      "epoch 98 loss = 1.997244\n",
      "epoch 99 loss = 1.839889\n",
      "epoch 100 loss = 1.821241\n",
      "epoch 101 loss = 1.955179\n",
      "epoch 102 loss = 1.757677\n",
      "epoch 103 loss = 1.963316\n",
      "epoch 104 loss = 1.822303\n",
      "epoch 105 loss = 1.873537\n",
      "epoch 106 loss = 2.002445\n",
      "epoch 107 loss = 1.879405\n",
      "epoch 108 loss = 2.105764\n",
      "epoch 109 loss = 1.877723\n",
      "epoch 110 loss = 2.063264\n",
      "epoch 111 loss = 2.019946\n",
      "epoch 112 loss = 2.034182\n",
      "epoch 113 loss = 2.032544\n",
      "epoch 114 loss = 1.882419\n",
      "epoch 115 loss = 1.921108\n",
      "epoch 116 loss = 2.070272\n",
      "epoch 117 loss = 1.820289\n",
      "epoch 118 loss = 2.048882\n",
      "epoch 119 loss = 2.052998\n",
      "epoch 120 loss = 1.948007\n",
      "epoch 121 loss = 1.920800\n",
      "epoch 122 loss = 1.785509\n",
      "epoch 123 loss = 2.145728\n",
      "epoch 124 loss = 1.799481\n",
      "epoch 125 loss = 1.874918\n",
      "epoch 126 loss = 1.938726\n",
      "epoch 127 loss = 1.826824\n",
      "epoch 128 loss = 1.828879\n",
      "epoch 129 loss = 1.841071\n",
      "epoch 130 loss = 1.855137\n",
      "epoch 131 loss = 1.714059\n",
      "epoch 132 loss = 1.663560\n",
      "epoch 133 loss = 1.866394\n",
      "epoch 134 loss = 1.889406\n",
      "epoch 135 loss = 2.050253\n",
      "epoch 136 loss = 1.911845\n",
      "epoch 137 loss = 1.808041\n",
      "epoch 138 loss = 1.882580\n",
      "epoch 139 loss = 1.971843\n",
      "epoch 140 loss = 1.982540\n",
      "epoch 141 loss = 2.149983\n",
      "epoch 142 loss = 1.782647\n",
      "epoch 143 loss = 1.793460\n",
      "epoch 144 loss = 2.141174\n",
      "epoch 145 loss = 1.881895\n",
      "epoch 146 loss = 1.760869\n",
      "epoch 147 loss = 1.998638\n",
      "epoch 148 loss = 1.954077\n",
      "epoch 149 loss = 1.866859\n",
      "epoch 150 loss = 2.018229\n",
      "epoch 151 loss = 2.076003\n",
      "epoch 152 loss = 1.977694\n",
      "epoch 153 loss = 1.870128\n",
      "epoch 154 loss = 2.193737\n",
      "epoch 155 loss = 2.004250\n",
      "epoch 156 loss = 2.027527\n",
      "epoch 157 loss = 1.869443\n",
      "epoch 158 loss = 1.811828\n",
      "epoch 159 loss = 1.875993\n",
      "epoch 160 loss = 1.965350\n",
      "epoch 161 loss = 1.962085\n",
      "epoch 162 loss = 1.940417\n",
      "epoch 163 loss = 2.006108\n",
      "epoch 164 loss = 2.047631\n",
      "epoch 165 loss = 1.914126\n",
      "epoch 166 loss = 1.881772\n",
      "epoch 167 loss = 1.916405\n",
      "epoch 168 loss = 1.971447\n",
      "epoch 169 loss = 2.055716\n",
      "epoch 170 loss = 1.847062\n",
      "epoch 171 loss = 1.783732\n",
      "epoch 172 loss = 1.990417\n",
      "epoch 173 loss = 2.026457\n",
      "epoch 174 loss = 1.947397\n",
      "epoch 175 loss = 1.958696\n",
      "epoch 176 loss = 2.116427\n",
      "epoch 177 loss = 1.867924\n",
      "epoch 178 loss = 1.946251\n",
      "epoch 179 loss = 1.975739\n",
      "epoch 180 loss = 1.850343\n",
      "epoch 181 loss = 1.933543\n",
      "epoch 182 loss = 2.054829\n",
      "epoch 183 loss = 1.980071\n",
      "epoch 184 loss = 1.959705\n",
      "epoch 185 loss = 1.944569\n",
      "epoch 186 loss = 1.960777\n",
      "epoch 187 loss = 1.762493\n",
      "epoch 188 loss = 2.035628\n",
      "epoch 189 loss = 1.852266\n",
      "epoch 190 loss = 1.673747\n",
      "epoch 191 loss = 2.203561\n",
      "epoch 192 loss = 2.003590\n",
      "epoch 193 loss = 2.109124\n",
      "epoch 194 loss = 1.983891\n",
      "epoch 195 loss = 2.001851\n",
      "epoch 196 loss = 1.891316\n",
      "epoch 197 loss = 1.806303\n",
      "epoch 198 loss = 2.036864\n",
      "epoch 199 loss = 1.934015\n",
      "epoch 200 loss = 1.779591\n",
      "epoch 201 loss = 1.980922\n",
      "epoch 202 loss = 1.995509\n",
      "epoch 203 loss = 2.037650\n",
      "epoch 204 loss = 1.780630\n",
      "epoch 205 loss = 1.941059\n",
      "epoch 206 loss = 2.068636\n",
      "epoch 207 loss = 1.934612\n",
      "epoch 208 loss = 1.992671\n",
      "epoch 209 loss = 1.876608\n",
      "epoch 210 loss = 2.061602\n",
      "epoch 211 loss = 1.973694\n",
      "epoch 212 loss = 1.754069\n",
      "epoch 213 loss = 2.004257\n",
      "epoch 214 loss = 2.002201\n",
      "epoch 215 loss = 1.883777\n",
      "epoch 216 loss = 1.960609\n",
      "epoch 217 loss = 2.019645\n",
      "epoch 218 loss = 2.078259\n",
      "epoch 219 loss = 1.744247\n",
      "epoch 220 loss = 1.973055\n",
      "epoch 221 loss = 1.773069\n",
      "epoch 222 loss = 1.924312\n",
      "epoch 223 loss = 1.873822\n",
      "epoch 224 loss = 1.871058\n",
      "epoch 225 loss = 1.833350\n",
      "epoch 226 loss = 1.971881\n",
      "epoch 227 loss = 1.722098\n",
      "epoch 228 loss = 1.917064\n",
      "epoch 229 loss = 1.842852\n",
      "epoch 230 loss = 1.992623\n",
      "epoch 231 loss = 1.951441\n",
      "epoch 232 loss = 1.863448\n",
      "epoch 233 loss = 1.950132\n",
      "epoch 234 loss = 1.789631\n",
      "epoch 235 loss = 2.010161\n",
      "epoch 236 loss = 1.881921\n",
      "epoch 237 loss = 1.812428\n",
      "epoch 238 loss = 1.902174\n",
      "epoch 239 loss = 1.782435\n",
      "epoch 240 loss = 1.865956\n",
      "epoch 241 loss = 1.946990\n",
      "epoch 242 loss = 1.816383\n",
      "epoch 243 loss = 1.932288\n",
      "epoch 244 loss = 1.869282\n",
      "epoch 245 loss = 1.890201\n",
      "epoch 246 loss = 1.905134\n",
      "epoch 247 loss = 1.920147\n",
      "epoch 248 loss = 1.994017\n",
      "epoch 249 loss = 1.730651\n",
      "epoch 250 loss = 1.933421\n",
      "epoch 251 loss = 1.827258\n",
      "epoch 252 loss = 1.911419\n",
      "epoch 253 loss = 1.912881\n",
      "epoch 254 loss = 2.060839\n",
      "epoch 255 loss = 1.960896\n",
      "epoch 256 loss = 1.832854\n",
      "epoch 257 loss = 1.957908\n",
      "epoch 258 loss = 1.882206\n",
      "epoch 259 loss = 1.742774\n",
      "epoch 260 loss = 1.885494\n",
      "epoch 261 loss = 1.860950\n",
      "epoch 262 loss = 1.968578\n",
      "epoch 263 loss = 1.975086\n",
      "epoch 264 loss = 1.659741\n",
      "epoch 265 loss = 2.123151\n",
      "epoch 266 loss = 1.865857\n",
      "epoch 267 loss = 1.899839\n",
      "epoch 268 loss = 2.019955\n",
      "epoch 269 loss = 1.698885\n",
      "epoch 270 loss = 1.798378\n",
      "epoch 271 loss = 1.769520\n",
      "epoch 272 loss = 1.856074\n",
      "epoch 273 loss = 1.922870\n",
      "epoch 274 loss = 1.853914\n",
      "epoch 275 loss = 1.962692\n",
      "epoch 276 loss = 1.994043\n",
      "epoch 277 loss = 1.892942\n",
      "epoch 278 loss = 1.653590\n",
      "epoch 279 loss = 1.886168\n",
      "epoch 280 loss = 1.944062\n",
      "epoch 281 loss = 1.850388\n",
      "epoch 282 loss = 1.830650\n",
      "epoch 283 loss = 1.879735\n",
      "epoch 284 loss = 1.867577\n",
      "epoch 285 loss = 1.890983\n",
      "epoch 286 loss = 1.980445\n",
      "epoch 287 loss = 1.977573\n",
      "epoch 288 loss = 1.821874\n",
      "epoch 289 loss = 1.817209\n",
      "epoch 290 loss = 2.012934\n",
      "epoch 291 loss = 1.888183\n",
      "epoch 292 loss = 2.103334\n",
      "epoch 293 loss = 1.970013\n",
      "epoch 294 loss = 1.965361\n",
      "epoch 295 loss = 1.795613\n",
      "epoch 296 loss = 1.970309\n",
      "epoch 297 loss = 1.779736\n",
      "epoch 298 loss = 1.971930\n",
      "epoch 299 loss = 1.824017\n",
      "epoch 300 loss = 1.858393\n",
      "epoch 301 loss = 1.852128\n",
      "epoch 302 loss = 1.951800\n",
      "epoch 303 loss = 1.709900\n",
      "epoch 304 loss = 2.053720\n",
      "epoch 305 loss = 1.668933\n",
      "epoch 306 loss = 2.048121\n",
      "epoch 307 loss = 2.032276\n",
      "epoch 308 loss = 2.014509\n",
      "epoch 309 loss = 1.826589\n",
      "epoch 310 loss = 1.928242\n",
      "epoch 311 loss = 1.742725\n",
      "epoch 312 loss = 2.080147\n",
      "epoch 313 loss = 2.027016\n",
      "epoch 314 loss = 1.867629\n",
      "epoch 315 loss = 1.957952\n",
      "epoch 316 loss = 2.097680\n",
      "epoch 317 loss = 1.992959\n",
      "epoch 318 loss = 2.059817\n",
      "epoch 319 loss = 1.959717\n",
      "epoch 320 loss = 1.969595\n",
      "epoch 321 loss = 1.869586\n",
      "epoch 322 loss = 2.119950\n",
      "epoch 323 loss = 1.804383\n",
      "epoch 324 loss = 2.008323\n",
      "epoch 325 loss = 2.025995\n",
      "epoch 326 loss = 1.786928\n",
      "epoch 327 loss = 1.765532\n",
      "epoch 328 loss = 1.943984\n",
      "epoch 329 loss = 1.786018\n",
      "epoch 330 loss = 1.958132\n",
      "epoch 331 loss = 1.714355\n",
      "epoch 332 loss = 1.933015\n",
      "epoch 333 loss = 2.006159\n",
      "epoch 334 loss = 2.031331\n",
      "epoch 335 loss = 1.957927\n",
      "epoch 336 loss = 1.954782\n",
      "epoch 337 loss = 1.875640\n",
      "epoch 338 loss = 1.991870\n",
      "epoch 339 loss = 1.977920\n",
      "epoch 340 loss = 1.966270\n",
      "epoch 341 loss = 1.878890\n",
      "epoch 342 loss = 1.753376\n",
      "epoch 343 loss = 2.036715\n",
      "epoch 344 loss = 1.952790\n",
      "epoch 345 loss = 1.752229\n",
      "epoch 346 loss = 1.856294\n",
      "epoch 347 loss = 1.995448\n",
      "epoch 348 loss = 2.080250\n",
      "epoch 349 loss = 1.871716\n",
      "epoch 350 loss = 1.904187\n",
      "epoch 351 loss = 2.129007\n",
      "epoch 352 loss = 1.796424\n",
      "epoch 353 loss = 1.990536\n",
      "epoch 354 loss = 2.039909\n",
      "epoch 355 loss = 1.902850\n",
      "epoch 356 loss = 1.886078\n",
      "epoch 357 loss = 1.968044\n",
      "epoch 358 loss = 1.904577\n",
      "epoch 359 loss = 1.838321\n",
      "epoch 360 loss = 2.007811\n",
      "epoch 361 loss = 1.924062\n",
      "epoch 362 loss = 2.077775\n",
      "epoch 363 loss = 1.719396\n",
      "epoch 364 loss = 1.714618\n",
      "epoch 365 loss = 1.933734\n",
      "epoch 366 loss = 2.054898\n",
      "epoch 367 loss = 1.792353\n",
      "epoch 368 loss = 1.632290\n",
      "epoch 369 loss = 2.137059\n",
      "epoch 370 loss = 1.829722\n",
      "epoch 371 loss = 1.733917\n",
      "epoch 372 loss = 2.127042\n",
      "epoch 373 loss = 1.873546\n",
      "epoch 374 loss = 1.928471\n",
      "epoch 375 loss = 1.986952\n",
      "epoch 376 loss = 1.884690\n",
      "epoch 377 loss = 2.054298\n",
      "epoch 378 loss = 1.851928\n",
      "epoch 379 loss = 2.034282\n",
      "epoch 380 loss = 2.050785\n",
      "epoch 381 loss = 1.862713\n",
      "epoch 382 loss = 1.982390\n",
      "epoch 383 loss = 2.070374\n",
      "epoch 384 loss = 1.965381\n",
      "epoch 385 loss = 1.921931\n",
      "epoch 386 loss = 1.930893\n",
      "epoch 387 loss = 1.861933\n",
      "epoch 388 loss = 1.856685\n",
      "epoch 389 loss = 1.979165\n",
      "epoch 390 loss = 2.111631\n",
      "epoch 391 loss = 1.705597\n",
      "epoch 392 loss = 1.732854\n",
      "epoch 393 loss = 2.047849\n",
      "epoch 394 loss = 1.984489\n",
      "epoch 395 loss = 1.816979\n",
      "epoch 396 loss = 1.983194\n",
      "epoch 397 loss = 1.759806\n",
      "epoch 398 loss = 1.963902\n",
      "epoch 399 loss = 1.808342\n",
      "epoch 400 loss = 1.962347\n",
      "epoch 401 loss = 1.955124\n",
      "epoch 402 loss = 1.993741\n",
      "epoch 403 loss = 1.879060\n",
      "epoch 404 loss = 1.888967\n",
      "epoch 405 loss = 2.104411\n",
      "epoch 406 loss = 1.933806\n",
      "epoch 407 loss = 2.032982\n",
      "epoch 408 loss = 1.890225\n",
      "epoch 409 loss = 1.971533\n",
      "epoch 410 loss = 2.035717\n",
      "epoch 411 loss = 1.994369\n",
      "epoch 412 loss = 1.860124\n",
      "epoch 413 loss = 1.859467\n",
      "epoch 414 loss = 1.783254\n",
      "epoch 415 loss = 2.015602\n",
      "epoch 416 loss = 1.896215\n",
      "epoch 417 loss = 2.020710\n",
      "epoch 418 loss = 1.865135\n",
      "epoch 419 loss = 1.711383\n",
      "epoch 420 loss = 1.824721\n",
      "epoch 421 loss = 1.950063\n",
      "epoch 422 loss = 1.635865\n",
      "epoch 423 loss = 1.946390\n",
      "epoch 424 loss = 1.880282\n",
      "epoch 425 loss = 1.905129\n",
      "epoch 426 loss = 1.948418\n",
      "epoch 427 loss = 1.686646\n",
      "epoch 428 loss = 1.819892\n",
      "epoch 429 loss = 1.769296\n",
      "epoch 430 loss = 2.133129\n",
      "epoch 431 loss = 1.806442\n",
      "epoch 432 loss = 1.981201\n",
      "epoch 433 loss = 1.757058\n",
      "epoch 434 loss = 1.895322\n",
      "epoch 435 loss = 1.617363\n",
      "epoch 436 loss = 1.708356\n",
      "epoch 437 loss = 1.918407\n",
      "epoch 438 loss = 1.878104\n",
      "epoch 439 loss = 1.618296\n",
      "epoch 440 loss = 1.910352\n",
      "epoch 441 loss = 1.966723\n",
      "epoch 442 loss = 1.896733\n",
      "epoch 443 loss = 1.753334\n",
      "epoch 444 loss = 2.297641\n",
      "epoch 445 loss = 1.911036\n",
      "epoch 446 loss = 1.941130\n",
      "epoch 447 loss = 1.974535\n",
      "epoch 448 loss = 1.971457\n",
      "epoch 449 loss = 1.859351\n",
      "epoch 450 loss = 2.041775\n",
      "epoch 451 loss = 1.905695\n",
      "epoch 452 loss = 1.793296\n",
      "epoch 453 loss = 1.788298\n",
      "epoch 454 loss = 1.925588\n",
      "epoch 455 loss = 1.763438\n",
      "epoch 456 loss = 1.907074\n",
      "epoch 457 loss = 2.123951\n",
      "epoch 458 loss = 1.962477\n",
      "epoch 459 loss = 1.931895\n",
      "epoch 460 loss = 1.796514\n",
      "epoch 461 loss = 2.008327\n",
      "epoch 462 loss = 2.001383\n",
      "epoch 463 loss = 1.752571\n",
      "epoch 464 loss = 2.019323\n",
      "epoch 465 loss = 1.855226\n",
      "epoch 466 loss = 1.857455\n",
      "epoch 467 loss = 1.977800\n",
      "epoch 468 loss = 1.939868\n",
      "epoch 469 loss = 1.932624\n",
      "epoch 470 loss = 2.054523\n",
      "epoch 471 loss = 1.932114\n",
      "epoch 472 loss = 2.156450\n",
      "epoch 473 loss = 1.905551\n",
      "epoch 474 loss = 1.992518\n",
      "epoch 475 loss = 1.803042\n",
      "epoch 476 loss = 1.799305\n",
      "epoch 477 loss = 2.064058\n",
      "epoch 478 loss = 1.923875\n",
      "epoch 479 loss = 2.055013\n",
      "epoch 480 loss = 1.904372\n",
      "epoch 481 loss = 1.809535\n",
      "epoch 482 loss = 1.827332\n",
      "epoch 483 loss = 1.914762\n",
      "epoch 484 loss = 1.956565\n",
      "epoch 485 loss = 1.933443\n",
      "epoch 486 loss = 2.074187\n",
      "epoch 487 loss = 1.947333\n",
      "epoch 488 loss = 1.854737\n",
      "epoch 489 loss = 2.186148\n",
      "epoch 490 loss = 1.790298\n",
      "epoch 491 loss = 1.900864\n",
      "epoch 492 loss = 2.058388\n",
      "epoch 493 loss = 1.940921\n",
      "epoch 494 loss = 1.811539\n",
      "epoch 495 loss = 1.875905\n",
      "epoch 496 loss = 1.946148\n",
      "epoch 497 loss = 1.639430\n",
      "epoch 498 loss = 2.097032\n",
      "epoch 499 loss = 1.855618\n",
      "final loss = 1.855618\n",
      "accuracy_mc = tensor(0.2418, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2090, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0047, device='cuda:0')\n",
      "training time = 171.56209707260132 seconds\n",
      "testing time = 1.8778622150421143 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = 2.288440\n",
      "epoch 1 loss = 2.316383\n",
      "epoch 2 loss = 2.252005\n",
      "epoch 3 loss = 2.297094\n",
      "epoch 4 loss = 2.206080\n",
      "epoch 5 loss = 2.183303\n",
      "epoch 6 loss = 2.249502\n",
      "epoch 7 loss = 2.238325\n",
      "epoch 8 loss = 2.216774\n",
      "epoch 9 loss = 2.089267\n",
      "epoch 10 loss = 2.096343\n",
      "epoch 11 loss = 2.213371\n",
      "epoch 12 loss = 2.081334\n",
      "epoch 13 loss = 2.060001\n",
      "epoch 14 loss = 2.210559\n",
      "epoch 15 loss = 2.127266\n",
      "epoch 16 loss = 2.126149\n",
      "epoch 17 loss = 2.142197\n",
      "epoch 18 loss = 2.173874\n",
      "epoch 19 loss = 2.112154\n",
      "epoch 20 loss = 2.229128\n",
      "epoch 21 loss = 2.144064\n",
      "epoch 22 loss = 2.183405\n",
      "epoch 23 loss = 2.072139\n",
      "epoch 24 loss = 2.201595\n",
      "epoch 25 loss = 2.174298\n",
      "epoch 26 loss = 2.201594\n",
      "epoch 27 loss = 2.125851\n",
      "epoch 28 loss = 2.112009\n",
      "epoch 29 loss = 2.226489\n",
      "epoch 30 loss = 1.970915\n",
      "epoch 31 loss = 2.116525\n",
      "epoch 32 loss = 1.875748\n",
      "epoch 33 loss = 2.200150\n",
      "epoch 34 loss = 2.026308\n",
      "epoch 35 loss = 2.089354\n",
      "epoch 36 loss = 2.047559\n",
      "epoch 37 loss = 2.049715\n",
      "epoch 38 loss = 2.122080\n",
      "epoch 39 loss = 2.091234\n",
      "epoch 40 loss = 2.188774\n",
      "epoch 41 loss = 2.097889\n",
      "epoch 42 loss = 2.161479\n",
      "epoch 43 loss = 1.982068\n",
      "epoch 44 loss = 2.179995\n",
      "epoch 45 loss = 2.104522\n",
      "epoch 46 loss = 2.141855\n",
      "epoch 47 loss = 2.052433\n",
      "epoch 48 loss = 2.070271\n",
      "epoch 49 loss = 1.986026\n",
      "epoch 50 loss = 2.073337\n",
      "epoch 51 loss = 2.137139\n",
      "epoch 52 loss = 2.048322\n",
      "epoch 53 loss = 1.910668\n",
      "epoch 54 loss = 2.037322\n",
      "epoch 55 loss = 1.965668\n",
      "epoch 56 loss = 1.983247\n",
      "epoch 57 loss = 1.961979\n",
      "epoch 58 loss = 2.110456\n",
      "epoch 59 loss = 2.085985\n",
      "epoch 60 loss = 2.135466\n",
      "epoch 61 loss = 1.954945\n",
      "epoch 62 loss = 2.146251\n",
      "epoch 63 loss = 2.232874\n",
      "epoch 64 loss = 2.019642\n",
      "epoch 65 loss = 2.043856\n",
      "epoch 66 loss = 2.140180\n",
      "epoch 67 loss = 2.023956\n",
      "epoch 68 loss = 2.130331\n",
      "epoch 69 loss = 2.180632\n",
      "epoch 70 loss = 1.978617\n",
      "epoch 71 loss = 2.177829\n",
      "epoch 72 loss = 2.135890\n",
      "epoch 73 loss = 1.908728\n",
      "epoch 74 loss = 2.002123\n",
      "epoch 75 loss = 2.125633\n",
      "epoch 76 loss = 2.118040\n",
      "epoch 77 loss = 1.999493\n",
      "epoch 78 loss = 2.069814\n",
      "epoch 79 loss = 2.197789\n",
      "epoch 80 loss = 1.987071\n",
      "epoch 81 loss = 1.965283\n",
      "epoch 82 loss = 1.904786\n",
      "epoch 83 loss = 1.971948\n",
      "epoch 84 loss = 2.141603\n",
      "epoch 85 loss = 1.996378\n",
      "epoch 86 loss = 1.871171\n",
      "epoch 87 loss = 2.141806\n",
      "epoch 88 loss = 2.119711\n",
      "epoch 89 loss = 2.105842\n",
      "epoch 90 loss = 1.955172\n",
      "epoch 91 loss = 2.000774\n",
      "epoch 92 loss = 1.957660\n",
      "epoch 93 loss = 1.988249\n",
      "epoch 94 loss = 2.086825\n",
      "epoch 95 loss = 1.999841\n",
      "epoch 96 loss = 2.069589\n",
      "epoch 97 loss = 2.192516\n",
      "epoch 98 loss = 2.010882\n",
      "epoch 99 loss = 2.080859\n",
      "epoch 100 loss = 2.006359\n",
      "epoch 101 loss = 2.132715\n",
      "epoch 102 loss = 2.054513\n",
      "epoch 103 loss = 1.903332\n",
      "epoch 104 loss = 2.206873\n",
      "epoch 105 loss = 1.896895\n",
      "epoch 106 loss = 1.912354\n",
      "epoch 107 loss = 2.195318\n",
      "epoch 108 loss = 1.934039\n",
      "epoch 109 loss = 2.156915\n",
      "epoch 110 loss = 1.782180\n",
      "epoch 111 loss = 1.886735\n",
      "epoch 112 loss = 2.304999\n",
      "epoch 113 loss = 2.156178\n",
      "epoch 114 loss = 1.701464\n",
      "epoch 115 loss = 2.009832\n",
      "epoch 116 loss = 2.047675\n",
      "epoch 117 loss = 1.903007\n",
      "epoch 118 loss = 2.098210\n",
      "epoch 119 loss = 2.010918\n",
      "epoch 120 loss = 1.990850\n",
      "epoch 121 loss = 2.075429\n",
      "epoch 122 loss = 1.934163\n",
      "epoch 123 loss = 1.965047\n",
      "epoch 124 loss = 2.135131\n",
      "epoch 125 loss = 2.250279\n",
      "epoch 126 loss = 2.324374\n",
      "epoch 127 loss = 1.915166\n",
      "epoch 128 loss = 1.914993\n",
      "epoch 129 loss = 1.930678\n",
      "epoch 130 loss = 2.129442\n",
      "epoch 131 loss = 1.893305\n",
      "epoch 132 loss = 2.156756\n",
      "epoch 133 loss = 2.186778\n",
      "epoch 134 loss = 2.124305\n",
      "epoch 135 loss = 2.019454\n",
      "epoch 136 loss = 2.118359\n",
      "epoch 137 loss = 2.164393\n",
      "epoch 138 loss = 1.983171\n",
      "epoch 139 loss = 2.134094\n",
      "epoch 140 loss = 1.903413\n",
      "epoch 141 loss = 1.968909\n",
      "epoch 142 loss = 1.928859\n",
      "epoch 143 loss = 2.075886\n",
      "epoch 144 loss = 1.942802\n",
      "epoch 145 loss = 2.094110\n",
      "epoch 146 loss = 1.941896\n",
      "epoch 147 loss = 2.134720\n",
      "epoch 148 loss = 1.986946\n",
      "epoch 149 loss = 2.017951\n",
      "epoch 150 loss = 2.203475\n",
      "epoch 151 loss = 2.107308\n",
      "epoch 152 loss = 1.966316\n",
      "epoch 153 loss = 2.016632\n",
      "epoch 154 loss = 2.150868\n",
      "epoch 155 loss = 2.041718\n",
      "epoch 156 loss = 2.126073\n",
      "epoch 157 loss = 1.768982\n",
      "epoch 158 loss = 2.068768\n",
      "epoch 159 loss = 1.929610\n",
      "epoch 160 loss = 2.037365\n",
      "epoch 161 loss = 1.996477\n",
      "epoch 162 loss = 2.105868\n",
      "epoch 163 loss = 1.916551\n",
      "epoch 164 loss = 1.899908\n",
      "epoch 165 loss = 2.211340\n",
      "epoch 166 loss = 2.057296\n",
      "epoch 167 loss = 2.034432\n",
      "epoch 168 loss = 1.922743\n",
      "epoch 169 loss = 2.049341\n",
      "epoch 170 loss = 2.094825\n",
      "epoch 171 loss = 2.011196\n",
      "epoch 172 loss = 2.044922\n",
      "epoch 173 loss = 1.882299\n",
      "epoch 174 loss = 2.004787\n",
      "epoch 175 loss = 1.907379\n",
      "epoch 176 loss = 2.017869\n",
      "epoch 177 loss = 1.870453\n",
      "epoch 178 loss = 1.988633\n",
      "epoch 179 loss = 2.039929\n",
      "epoch 180 loss = 1.853909\n",
      "epoch 181 loss = 1.933038\n",
      "epoch 182 loss = 1.826508\n",
      "epoch 183 loss = 2.080459\n",
      "epoch 184 loss = 1.886411\n",
      "epoch 185 loss = 2.217243\n",
      "epoch 186 loss = 1.990842\n",
      "epoch 187 loss = 2.008461\n",
      "epoch 188 loss = 1.772173\n",
      "epoch 189 loss = 2.282753\n",
      "epoch 190 loss = 1.923885\n",
      "epoch 191 loss = 2.014933\n",
      "epoch 192 loss = 1.847556\n",
      "epoch 193 loss = 2.066361\n",
      "epoch 194 loss = 2.028866\n",
      "epoch 195 loss = 1.977244\n",
      "epoch 196 loss = 1.982094\n",
      "epoch 197 loss = 2.138247\n",
      "epoch 198 loss = 2.134894\n",
      "epoch 199 loss = 1.869895\n",
      "epoch 200 loss = 1.960555\n",
      "epoch 201 loss = 1.995658\n",
      "epoch 202 loss = 1.793834\n",
      "epoch 203 loss = 2.067812\n",
      "epoch 204 loss = 1.884328\n",
      "epoch 205 loss = 1.974700\n",
      "epoch 206 loss = 1.907555\n",
      "epoch 207 loss = 1.886587\n",
      "epoch 208 loss = 2.120837\n",
      "epoch 209 loss = 1.981539\n",
      "epoch 210 loss = 1.910656\n",
      "epoch 211 loss = 1.935157\n",
      "epoch 212 loss = 2.274296\n",
      "epoch 213 loss = 1.897439\n",
      "epoch 214 loss = 1.869893\n",
      "epoch 215 loss = 1.993164\n",
      "epoch 216 loss = 1.946656\n",
      "epoch 217 loss = 1.930010\n",
      "epoch 218 loss = 1.965637\n",
      "epoch 219 loss = 2.135706\n",
      "epoch 220 loss = 2.061924\n",
      "epoch 221 loss = 2.071514\n",
      "epoch 222 loss = 1.943483\n",
      "epoch 223 loss = 2.022084\n",
      "epoch 224 loss = 2.022582\n",
      "epoch 225 loss = 2.123555\n",
      "epoch 226 loss = 2.154658\n",
      "epoch 227 loss = 1.943550\n",
      "epoch 228 loss = 1.986226\n",
      "epoch 229 loss = 2.031321\n",
      "epoch 230 loss = 1.870183\n",
      "epoch 231 loss = 1.746720\n",
      "epoch 232 loss = 1.995228\n",
      "epoch 233 loss = 2.221255\n",
      "epoch 234 loss = 2.060056\n",
      "epoch 235 loss = 1.945417\n",
      "epoch 236 loss = 1.849710\n",
      "epoch 237 loss = 2.147882\n",
      "epoch 238 loss = 1.958205\n",
      "epoch 239 loss = 1.932970\n",
      "epoch 240 loss = 2.122588\n",
      "epoch 241 loss = 2.134017\n",
      "epoch 242 loss = 1.915100\n",
      "epoch 243 loss = 1.888882\n",
      "epoch 244 loss = 1.927264\n",
      "epoch 245 loss = 2.060231\n",
      "epoch 246 loss = 1.892372\n",
      "epoch 247 loss = 1.942927\n",
      "epoch 248 loss = 1.958713\n",
      "epoch 249 loss = 1.896705\n",
      "epoch 250 loss = 2.078425\n",
      "epoch 251 loss = 2.088624\n",
      "epoch 252 loss = 2.117963\n",
      "epoch 253 loss = 1.949601\n",
      "epoch 254 loss = 1.978769\n",
      "epoch 255 loss = 2.148533\n",
      "epoch 256 loss = 2.042027\n",
      "epoch 257 loss = 1.912213\n",
      "epoch 258 loss = 2.108678\n",
      "epoch 259 loss = 2.192050\n",
      "epoch 260 loss = 2.051650\n",
      "epoch 261 loss = 2.029398\n",
      "epoch 262 loss = 1.989141\n",
      "epoch 263 loss = 1.990698\n",
      "epoch 264 loss = 1.948866\n",
      "epoch 265 loss = 1.910785\n",
      "epoch 266 loss = 1.910188\n",
      "epoch 267 loss = 2.105855\n",
      "epoch 268 loss = 1.804614\n",
      "epoch 269 loss = 2.069155\n",
      "epoch 270 loss = 1.910770\n",
      "epoch 271 loss = 2.045607\n",
      "epoch 272 loss = 1.878180\n",
      "epoch 273 loss = 1.877684\n",
      "epoch 274 loss = 2.048688\n",
      "epoch 275 loss = 1.986413\n",
      "epoch 276 loss = 1.871627\n",
      "epoch 277 loss = 1.926030\n",
      "epoch 278 loss = 1.933412\n",
      "epoch 279 loss = 2.078649\n",
      "epoch 280 loss = 2.101122\n",
      "epoch 281 loss = 2.108610\n",
      "epoch 282 loss = 2.165274\n",
      "epoch 283 loss = 1.840286\n",
      "epoch 284 loss = 1.931755\n",
      "epoch 285 loss = 2.101900\n",
      "epoch 286 loss = 2.208876\n",
      "epoch 287 loss = 1.935115\n",
      "epoch 288 loss = 1.978909\n",
      "epoch 289 loss = 2.003070\n",
      "epoch 290 loss = 1.998935\n",
      "epoch 291 loss = 1.966315\n",
      "epoch 292 loss = 1.927378\n",
      "epoch 293 loss = 1.972937\n",
      "epoch 294 loss = 1.966036\n",
      "epoch 295 loss = 1.999910\n",
      "epoch 296 loss = 2.151664\n",
      "epoch 297 loss = 2.134038\n",
      "epoch 298 loss = 2.001173\n",
      "epoch 299 loss = 1.951871\n",
      "epoch 300 loss = 1.800990\n",
      "epoch 301 loss = 1.874139\n",
      "epoch 302 loss = 1.866278\n",
      "epoch 303 loss = 2.125468\n",
      "epoch 304 loss = 2.152185\n",
      "epoch 305 loss = 1.998554\n",
      "epoch 306 loss = 2.158314\n",
      "epoch 307 loss = 1.958660\n",
      "epoch 308 loss = 1.813065\n",
      "epoch 309 loss = 2.012797\n",
      "epoch 310 loss = 1.931271\n",
      "epoch 311 loss = 2.205726\n",
      "epoch 312 loss = 1.916317\n",
      "epoch 313 loss = 2.135655\n",
      "epoch 314 loss = 1.917275\n",
      "epoch 315 loss = 1.979676\n",
      "epoch 316 loss = 2.043396\n",
      "epoch 317 loss = 2.064754\n",
      "epoch 318 loss = 2.099853\n",
      "epoch 319 loss = 2.018706\n",
      "epoch 320 loss = 1.988139\n",
      "epoch 321 loss = 2.047437\n",
      "epoch 322 loss = 1.908489\n",
      "epoch 323 loss = 1.933780\n",
      "epoch 324 loss = 1.984699\n",
      "epoch 325 loss = 2.067882\n",
      "epoch 326 loss = 2.106336\n",
      "epoch 327 loss = 1.939653\n",
      "epoch 328 loss = 2.048311\n",
      "epoch 329 loss = 2.001689\n",
      "epoch 330 loss = 2.144818\n",
      "epoch 331 loss = 2.018128\n",
      "epoch 332 loss = 1.778394\n",
      "epoch 333 loss = 1.998148\n",
      "epoch 334 loss = 1.923384\n",
      "epoch 335 loss = 2.108854\n",
      "epoch 336 loss = 2.054937\n",
      "epoch 337 loss = 1.818970\n",
      "epoch 338 loss = 1.811366\n",
      "epoch 339 loss = 2.167237\n",
      "epoch 340 loss = 2.032316\n",
      "epoch 341 loss = 2.038795\n",
      "epoch 342 loss = 2.026061\n",
      "epoch 343 loss = 2.198953\n",
      "epoch 344 loss = 2.012900\n",
      "epoch 345 loss = 2.002718\n",
      "epoch 346 loss = 2.150872\n",
      "epoch 347 loss = 2.030645\n",
      "epoch 348 loss = 2.142350\n",
      "epoch 349 loss = 1.891088\n",
      "epoch 350 loss = 2.119313\n",
      "epoch 351 loss = 2.019865\n",
      "epoch 352 loss = 2.130935\n",
      "epoch 353 loss = 1.840196\n",
      "epoch 354 loss = 2.008934\n",
      "epoch 355 loss = 1.969486\n",
      "epoch 356 loss = 1.783484\n",
      "epoch 357 loss = 1.846523\n",
      "epoch 358 loss = 2.034025\n",
      "epoch 359 loss = 1.822328\n",
      "epoch 360 loss = 1.822991\n",
      "epoch 361 loss = 1.981778\n",
      "epoch 362 loss = 2.009798\n",
      "epoch 363 loss = 2.013005\n",
      "epoch 364 loss = 1.730463\n",
      "epoch 365 loss = 1.782524\n",
      "epoch 366 loss = 1.926052\n",
      "epoch 367 loss = 2.037043\n",
      "epoch 368 loss = 1.859347\n",
      "epoch 369 loss = 1.913750\n",
      "epoch 370 loss = 2.076406\n",
      "epoch 371 loss = 2.072861\n",
      "epoch 372 loss = 1.817005\n",
      "epoch 373 loss = 2.078488\n",
      "epoch 374 loss = 1.886756\n",
      "epoch 375 loss = 2.073443\n",
      "epoch 376 loss = 1.929049\n",
      "epoch 377 loss = 2.038085\n",
      "epoch 378 loss = 2.080444\n",
      "epoch 379 loss = 1.958148\n",
      "epoch 380 loss = 1.804463\n",
      "epoch 381 loss = 2.163471\n",
      "epoch 382 loss = 1.954884\n",
      "epoch 383 loss = 1.859396\n",
      "epoch 384 loss = 1.985051\n",
      "epoch 385 loss = 2.140862\n",
      "epoch 386 loss = 2.115472\n",
      "epoch 387 loss = 1.956461\n",
      "epoch 388 loss = 2.054385\n",
      "epoch 389 loss = 1.940126\n",
      "epoch 390 loss = 2.007918\n",
      "epoch 391 loss = 1.926138\n",
      "epoch 392 loss = 1.999355\n",
      "epoch 393 loss = 1.936403\n",
      "epoch 394 loss = 1.844896\n",
      "epoch 395 loss = 1.963360\n",
      "epoch 396 loss = 1.942508\n",
      "epoch 397 loss = 1.791348\n",
      "epoch 398 loss = 1.884219\n",
      "epoch 399 loss = 2.057598\n",
      "epoch 400 loss = 1.947375\n",
      "epoch 401 loss = 1.736601\n",
      "epoch 402 loss = 2.014457\n",
      "epoch 403 loss = 1.862476\n",
      "epoch 404 loss = 2.072219\n",
      "epoch 405 loss = 1.916067\n",
      "epoch 406 loss = 2.003738\n",
      "epoch 407 loss = 2.160068\n",
      "epoch 408 loss = 2.108098\n",
      "epoch 409 loss = 2.215872\n",
      "epoch 410 loss = 2.063624\n",
      "epoch 411 loss = 1.961354\n",
      "epoch 412 loss = 1.982008\n",
      "epoch 413 loss = 1.885982\n",
      "epoch 414 loss = 2.071763\n",
      "epoch 415 loss = 1.985893\n",
      "epoch 416 loss = 1.847978\n",
      "epoch 417 loss = 2.111420\n",
      "epoch 418 loss = 2.075497\n",
      "epoch 419 loss = 1.903163\n",
      "epoch 420 loss = 1.805828\n",
      "epoch 421 loss = 1.951389\n",
      "epoch 422 loss = 1.878997\n",
      "epoch 423 loss = 1.854636\n",
      "epoch 424 loss = 2.025644\n",
      "epoch 425 loss = 1.980527\n",
      "epoch 426 loss = 2.103217\n",
      "epoch 427 loss = 1.912401\n",
      "epoch 428 loss = 1.970148\n",
      "epoch 429 loss = 2.037714\n",
      "epoch 430 loss = 1.885450\n",
      "epoch 431 loss = 1.812096\n",
      "epoch 432 loss = 2.043406\n",
      "epoch 433 loss = 1.886767\n",
      "epoch 434 loss = 1.903565\n",
      "epoch 435 loss = 1.835969\n",
      "epoch 436 loss = 2.092833\n",
      "epoch 437 loss = 1.974307\n",
      "epoch 438 loss = 2.141362\n",
      "epoch 439 loss = 1.945273\n",
      "epoch 440 loss = 1.814533\n",
      "epoch 441 loss = 1.871130\n",
      "epoch 442 loss = 2.031568\n",
      "epoch 443 loss = 2.052468\n",
      "epoch 444 loss = 1.887921\n",
      "epoch 445 loss = 1.861951\n",
      "epoch 446 loss = 1.979288\n",
      "epoch 447 loss = 1.975634\n",
      "epoch 448 loss = 1.935291\n",
      "epoch 449 loss = 1.946696\n",
      "epoch 450 loss = 1.896391\n",
      "epoch 451 loss = 1.887762\n",
      "epoch 452 loss = 1.922077\n",
      "epoch 453 loss = 1.942562\n",
      "epoch 454 loss = 2.072061\n",
      "epoch 455 loss = 2.041028\n",
      "epoch 456 loss = 1.956263\n",
      "epoch 457 loss = 2.175112\n",
      "epoch 458 loss = 2.128507\n",
      "epoch 459 loss = 2.089705\n",
      "epoch 460 loss = 1.906551\n",
      "epoch 461 loss = 1.924375\n",
      "epoch 462 loss = 1.992339\n",
      "epoch 463 loss = 1.932771\n",
      "epoch 464 loss = 2.068114\n",
      "epoch 465 loss = 2.005361\n",
      "epoch 466 loss = 1.970669\n",
      "epoch 467 loss = 1.981012\n",
      "epoch 468 loss = 1.976790\n",
      "epoch 469 loss = 2.082740\n",
      "epoch 470 loss = 1.973382\n",
      "epoch 471 loss = 1.940111\n",
      "epoch 472 loss = 2.225641\n",
      "epoch 473 loss = 1.908063\n",
      "epoch 474 loss = 2.031536\n",
      "epoch 475 loss = 1.938367\n",
      "epoch 476 loss = 1.995843\n",
      "epoch 477 loss = 1.981586\n",
      "epoch 478 loss = 1.840307\n",
      "epoch 479 loss = 2.123870\n",
      "epoch 480 loss = 1.995768\n",
      "epoch 481 loss = 1.973883\n",
      "epoch 482 loss = 1.814626\n",
      "epoch 483 loss = 2.130868\n",
      "epoch 484 loss = 2.176148\n",
      "epoch 485 loss = 1.980690\n",
      "epoch 486 loss = 1.831839\n",
      "epoch 487 loss = 1.837634\n",
      "epoch 488 loss = 2.114519\n",
      "epoch 489 loss = 1.874142\n",
      "epoch 490 loss = 1.956629\n",
      "epoch 491 loss = 1.940848\n",
      "epoch 492 loss = 1.890199\n",
      "epoch 493 loss = 2.146822\n",
      "epoch 494 loss = 2.174467\n",
      "epoch 495 loss = 1.830373\n",
      "epoch 496 loss = 1.895747\n",
      "epoch 497 loss = 1.945774\n",
      "epoch 498 loss = 1.969124\n",
      "epoch 499 loss = 1.914884\n",
      "final loss = 1.914884\n",
      "accuracy_mc = tensor(0.3119, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3081, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9231, device='cuda:0')\n",
      "training time = 171.45621132850647 seconds\n",
      "testing time = 1.8834831714630127 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = 2.277970\n",
      "epoch 1 loss = 2.289245\n",
      "epoch 2 loss = 2.304932\n",
      "epoch 3 loss = 2.288708\n",
      "epoch 4 loss = 2.309884\n",
      "epoch 5 loss = 2.232653\n",
      "epoch 6 loss = 2.309617\n",
      "epoch 7 loss = 2.326525\n",
      "epoch 8 loss = 2.288528\n",
      "epoch 9 loss = 2.191554\n",
      "epoch 10 loss = 2.301018\n",
      "epoch 11 loss = 2.124231\n",
      "epoch 12 loss = 2.161907\n",
      "epoch 13 loss = 2.212514\n",
      "epoch 14 loss = 2.189793\n",
      "epoch 15 loss = 2.292467\n",
      "epoch 16 loss = 2.118379\n",
      "epoch 17 loss = 2.270482\n",
      "epoch 18 loss = 2.219322\n",
      "epoch 19 loss = 2.289963\n",
      "epoch 20 loss = 2.148139\n",
      "epoch 21 loss = 2.298838\n",
      "epoch 22 loss = 2.191763\n",
      "epoch 23 loss = 2.237598\n",
      "epoch 24 loss = 2.185267\n",
      "epoch 25 loss = 2.161337\n",
      "epoch 26 loss = 2.107774\n",
      "epoch 27 loss = 2.214628\n",
      "epoch 28 loss = 2.195938\n",
      "epoch 29 loss = 2.196385\n",
      "epoch 30 loss = 2.276770\n",
      "epoch 31 loss = 2.166124\n",
      "epoch 32 loss = 2.239378\n",
      "epoch 33 loss = 2.231995\n",
      "epoch 34 loss = 2.156970\n",
      "epoch 35 loss = 2.357543\n",
      "epoch 36 loss = 2.057583\n",
      "epoch 37 loss = 2.291351\n",
      "epoch 38 loss = 2.190243\n",
      "epoch 39 loss = 2.077070\n",
      "epoch 40 loss = 2.375330\n",
      "epoch 41 loss = 2.350155\n",
      "epoch 42 loss = 2.100091\n",
      "epoch 43 loss = 2.176906\n",
      "epoch 44 loss = 2.275964\n",
      "epoch 45 loss = 2.222872\n",
      "epoch 46 loss = 2.035156\n",
      "epoch 47 loss = 2.144255\n",
      "epoch 48 loss = 2.190294\n",
      "epoch 49 loss = 2.123381\n",
      "epoch 50 loss = 2.161936\n",
      "epoch 51 loss = 2.248778\n",
      "epoch 52 loss = 2.249906\n",
      "epoch 53 loss = 2.206021\n",
      "epoch 54 loss = 2.350277\n",
      "epoch 55 loss = 2.365639\n",
      "epoch 56 loss = 2.249725\n",
      "epoch 57 loss = 2.218176\n",
      "epoch 58 loss = 2.166154\n",
      "epoch 59 loss = 2.101987\n",
      "epoch 60 loss = 2.273176\n",
      "epoch 61 loss = 2.110943\n",
      "epoch 62 loss = 2.120805\n",
      "epoch 63 loss = 2.298665\n",
      "epoch 64 loss = 2.226752\n",
      "epoch 65 loss = 2.100889\n",
      "epoch 66 loss = 2.066346\n",
      "epoch 67 loss = 2.077338\n",
      "epoch 68 loss = 2.191054\n",
      "epoch 69 loss = 2.151588\n",
      "epoch 70 loss = 2.061170\n",
      "epoch 71 loss = 2.254387\n",
      "epoch 72 loss = 2.133229\n",
      "epoch 73 loss = 2.189928\n",
      "epoch 74 loss = 2.217354\n",
      "epoch 75 loss = 2.079116\n",
      "epoch 76 loss = 2.075018\n",
      "epoch 77 loss = 2.179070\n",
      "epoch 78 loss = 2.129161\n",
      "epoch 79 loss = 2.188260\n",
      "epoch 80 loss = 2.313044\n",
      "epoch 81 loss = 2.113941\n",
      "epoch 82 loss = 2.155671\n",
      "epoch 83 loss = 2.128319\n",
      "epoch 84 loss = 2.193695\n",
      "epoch 85 loss = 2.335203\n",
      "epoch 86 loss = 2.459042\n",
      "epoch 87 loss = 2.181588\n",
      "epoch 88 loss = 2.275815\n",
      "epoch 89 loss = 2.088778\n",
      "epoch 90 loss = 2.163257\n",
      "epoch 91 loss = 2.200391\n",
      "epoch 92 loss = 2.326452\n",
      "epoch 93 loss = 2.228703\n",
      "epoch 94 loss = 2.009731\n",
      "epoch 95 loss = 2.119311\n",
      "epoch 96 loss = 2.136292\n",
      "epoch 97 loss = 2.416228\n",
      "epoch 98 loss = 2.170346\n",
      "epoch 99 loss = 2.246717\n",
      "epoch 100 loss = 2.294454\n",
      "epoch 101 loss = 2.187380\n",
      "epoch 102 loss = 2.190167\n",
      "epoch 103 loss = 2.245483\n",
      "epoch 104 loss = 2.275511\n",
      "epoch 105 loss = 2.081889\n",
      "epoch 106 loss = 2.207743\n",
      "epoch 107 loss = 2.175517\n",
      "epoch 108 loss = 2.136951\n",
      "epoch 109 loss = 2.237287\n",
      "epoch 110 loss = 2.313379\n",
      "epoch 111 loss = 2.182271\n",
      "epoch 112 loss = 2.116763\n",
      "epoch 113 loss = 2.053259\n",
      "epoch 114 loss = 1.975818\n",
      "epoch 115 loss = 2.306178\n",
      "epoch 116 loss = 2.131302\n",
      "epoch 117 loss = 2.165919\n",
      "epoch 118 loss = 2.173632\n",
      "epoch 119 loss = 2.242884\n",
      "epoch 120 loss = 2.103061\n",
      "epoch 121 loss = 2.225289\n",
      "epoch 122 loss = 2.142654\n",
      "epoch 123 loss = 2.264024\n",
      "epoch 124 loss = 2.167583\n",
      "epoch 125 loss = 2.164678\n",
      "epoch 126 loss = 2.194117\n",
      "epoch 127 loss = 2.129830\n",
      "epoch 128 loss = 2.157559\n",
      "epoch 129 loss = 2.181291\n",
      "epoch 130 loss = 2.337587\n",
      "epoch 131 loss = 2.275432\n",
      "epoch 132 loss = 2.169263\n",
      "epoch 133 loss = 2.167485\n",
      "epoch 134 loss = 2.147785\n",
      "epoch 135 loss = 2.420910\n",
      "epoch 136 loss = 2.038246\n",
      "epoch 137 loss = 2.265564\n",
      "epoch 138 loss = 2.067790\n",
      "epoch 139 loss = 2.237436\n",
      "epoch 140 loss = 2.139390\n",
      "epoch 141 loss = 2.183139\n",
      "epoch 142 loss = 2.305746\n",
      "epoch 143 loss = 2.158665\n",
      "epoch 144 loss = 2.093482\n",
      "epoch 145 loss = 2.324134\n",
      "epoch 146 loss = 2.126722\n",
      "epoch 147 loss = 2.198152\n",
      "epoch 148 loss = 2.089162\n",
      "epoch 149 loss = 2.240978\n",
      "epoch 150 loss = 2.102450\n",
      "epoch 151 loss = 2.160832\n",
      "epoch 152 loss = 2.086731\n",
      "epoch 153 loss = 2.171117\n",
      "epoch 154 loss = 2.176555\n",
      "epoch 155 loss = 2.134567\n",
      "epoch 156 loss = 2.194272\n",
      "epoch 157 loss = 2.196356\n",
      "epoch 158 loss = 2.219094\n",
      "epoch 159 loss = 2.067640\n",
      "epoch 160 loss = 2.117029\n",
      "epoch 161 loss = 2.132062\n",
      "epoch 162 loss = 2.033224\n",
      "epoch 163 loss = 2.355021\n",
      "epoch 164 loss = 2.081975\n",
      "epoch 165 loss = 2.097703\n",
      "epoch 166 loss = 2.329137\n",
      "epoch 167 loss = 2.077097\n",
      "epoch 168 loss = 2.217667\n",
      "epoch 169 loss = 1.889094\n",
      "epoch 170 loss = 2.398520\n",
      "epoch 171 loss = 2.002897\n",
      "epoch 172 loss = 2.130962\n",
      "epoch 173 loss = 2.277566\n",
      "epoch 174 loss = 2.130959\n",
      "epoch 175 loss = 1.988762\n",
      "epoch 176 loss = 2.122575\n",
      "epoch 177 loss = 2.235263\n",
      "epoch 178 loss = 2.261299\n",
      "epoch 179 loss = 2.229763\n",
      "epoch 180 loss = 2.230253\n",
      "epoch 181 loss = 2.215784\n",
      "epoch 182 loss = 2.449008\n",
      "epoch 183 loss = 2.154056\n",
      "epoch 184 loss = 1.968476\n",
      "epoch 185 loss = 2.459719\n",
      "epoch 186 loss = 2.131735\n",
      "epoch 187 loss = 2.201378\n",
      "epoch 188 loss = 2.192293\n",
      "epoch 189 loss = 2.162302\n",
      "epoch 190 loss = 2.191030\n",
      "epoch 191 loss = 2.210074\n",
      "epoch 192 loss = 1.978945\n",
      "epoch 193 loss = 2.213515\n",
      "epoch 194 loss = 2.092011\n",
      "epoch 195 loss = 2.052210\n",
      "epoch 196 loss = 2.296740\n",
      "epoch 197 loss = 2.078698\n",
      "epoch 198 loss = 2.203298\n",
      "epoch 199 loss = 2.170774\n",
      "epoch 200 loss = 2.154255\n",
      "epoch 201 loss = 2.076318\n",
      "epoch 202 loss = 2.277056\n",
      "epoch 203 loss = 2.050721\n",
      "epoch 204 loss = 2.078980\n",
      "epoch 205 loss = 2.274917\n",
      "epoch 206 loss = 2.237765\n",
      "epoch 207 loss = 2.110144\n",
      "epoch 208 loss = 2.088935\n",
      "epoch 209 loss = 2.264368\n",
      "epoch 210 loss = 1.975771\n",
      "epoch 211 loss = 2.155310\n",
      "epoch 212 loss = 2.216336\n",
      "epoch 213 loss = 1.977810\n",
      "epoch 214 loss = 2.162617\n",
      "epoch 215 loss = 2.232229\n",
      "epoch 216 loss = 1.949597\n",
      "epoch 217 loss = 2.163151\n",
      "epoch 218 loss = 2.163672\n",
      "epoch 219 loss = 2.081403\n",
      "epoch 220 loss = 2.278265\n",
      "epoch 221 loss = 2.263346\n",
      "epoch 222 loss = 2.023657\n",
      "epoch 223 loss = 2.204934\n",
      "epoch 224 loss = 2.151443\n",
      "epoch 225 loss = 2.246790\n",
      "epoch 226 loss = 2.129142\n",
      "epoch 227 loss = 2.195080\n",
      "epoch 228 loss = 2.043732\n",
      "epoch 229 loss = 2.048835\n",
      "epoch 230 loss = 2.169186\n",
      "epoch 231 loss = 2.140612\n",
      "epoch 232 loss = 2.263863\n",
      "epoch 233 loss = 2.269871\n",
      "epoch 234 loss = 2.222529\n",
      "epoch 235 loss = 2.142050\n",
      "epoch 236 loss = 2.088875\n",
      "epoch 237 loss = 2.164104\n",
      "epoch 238 loss = 2.026962\n",
      "epoch 239 loss = 2.077602\n",
      "epoch 240 loss = 2.207469\n",
      "epoch 241 loss = 2.125030\n",
      "epoch 242 loss = 2.306855\n",
      "epoch 243 loss = 2.085649\n",
      "epoch 244 loss = 2.181552\n",
      "epoch 245 loss = 2.103292\n",
      "epoch 246 loss = 2.274671\n",
      "epoch 247 loss = 2.160777\n",
      "epoch 248 loss = 2.321824\n",
      "epoch 249 loss = 2.178691\n",
      "epoch 250 loss = 2.097319\n",
      "epoch 251 loss = 1.898432\n",
      "epoch 252 loss = 2.139196\n",
      "epoch 253 loss = 2.221187\n",
      "epoch 254 loss = 2.328222\n",
      "epoch 255 loss = 2.222759\n",
      "epoch 256 loss = 2.093011\n",
      "epoch 257 loss = 2.213705\n",
      "epoch 258 loss = 2.068236\n",
      "epoch 259 loss = 2.136029\n",
      "epoch 260 loss = 2.264439\n",
      "epoch 261 loss = 2.253999\n",
      "epoch 262 loss = 2.073843\n",
      "epoch 263 loss = 2.133294\n",
      "epoch 264 loss = 2.047296\n",
      "epoch 265 loss = 2.233912\n",
      "epoch 266 loss = 2.188292\n",
      "epoch 267 loss = 2.218019\n",
      "epoch 268 loss = 2.177952\n",
      "epoch 269 loss = 2.130305\n",
      "epoch 270 loss = 2.086682\n",
      "epoch 271 loss = 2.190069\n",
      "epoch 272 loss = 2.148786\n",
      "epoch 273 loss = 2.218005\n",
      "epoch 274 loss = 2.347831\n",
      "epoch 275 loss = 2.114871\n",
      "epoch 276 loss = 2.137327\n",
      "epoch 277 loss = 1.954807\n",
      "epoch 278 loss = 2.128004\n",
      "epoch 279 loss = 2.109637\n",
      "epoch 280 loss = 2.058957\n",
      "epoch 281 loss = 1.977644\n",
      "epoch 282 loss = 2.138385\n",
      "epoch 283 loss = 2.184516\n",
      "epoch 284 loss = 2.098051\n",
      "epoch 285 loss = 2.195344\n",
      "epoch 286 loss = 2.282621\n",
      "epoch 287 loss = 2.107972\n",
      "epoch 288 loss = 2.187726\n",
      "epoch 289 loss = 2.071764\n",
      "epoch 290 loss = 2.310324\n",
      "epoch 291 loss = 2.025877\n",
      "epoch 292 loss = 2.225233\n",
      "epoch 293 loss = 2.073133\n",
      "epoch 294 loss = 2.025228\n",
      "epoch 295 loss = 2.368657\n",
      "epoch 296 loss = 2.216519\n",
      "epoch 297 loss = 2.137423\n",
      "epoch 298 loss = 2.299503\n",
      "epoch 299 loss = 2.004487\n",
      "epoch 300 loss = 2.225469\n",
      "epoch 301 loss = 2.177453\n",
      "epoch 302 loss = 2.454368\n",
      "epoch 303 loss = 2.476028\n",
      "epoch 304 loss = 2.264318\n",
      "epoch 305 loss = 2.147592\n",
      "epoch 306 loss = 2.170046\n",
      "epoch 307 loss = 2.102092\n",
      "epoch 308 loss = 2.445898\n",
      "epoch 309 loss = 2.219547\n",
      "epoch 310 loss = 2.090977\n",
      "epoch 311 loss = 2.194325\n",
      "epoch 312 loss = 2.226413\n",
      "epoch 313 loss = 1.974214\n",
      "epoch 314 loss = 2.227971\n",
      "epoch 315 loss = 2.028846\n",
      "epoch 316 loss = 2.089424\n",
      "epoch 317 loss = 2.134405\n",
      "epoch 318 loss = 2.116714\n",
      "epoch 319 loss = 2.160959\n",
      "epoch 320 loss = 2.011043\n",
      "epoch 321 loss = 2.142387\n",
      "epoch 322 loss = 1.824877\n",
      "epoch 323 loss = 2.106538\n",
      "epoch 324 loss = 2.019723\n",
      "epoch 325 loss = 2.276643\n",
      "epoch 326 loss = 1.953062\n",
      "epoch 327 loss = 2.075226\n",
      "epoch 328 loss = 2.116581\n",
      "epoch 329 loss = 2.082600\n",
      "epoch 330 loss = 2.175949\n",
      "epoch 331 loss = 2.248054\n",
      "epoch 332 loss = 2.160200\n",
      "epoch 333 loss = 2.032848\n",
      "epoch 334 loss = 2.291670\n",
      "epoch 335 loss = 2.009124\n",
      "epoch 336 loss = 2.040877\n",
      "epoch 337 loss = 2.257252\n",
      "epoch 338 loss = 2.263283\n",
      "epoch 339 loss = 2.068082\n",
      "epoch 340 loss = 2.094319\n",
      "epoch 341 loss = 2.162921\n",
      "epoch 342 loss = 2.071157\n",
      "epoch 343 loss = 1.976725\n",
      "epoch 344 loss = 2.012370\n",
      "epoch 345 loss = 2.278679\n",
      "epoch 346 loss = 2.234669\n",
      "epoch 347 loss = 2.202459\n",
      "epoch 348 loss = 2.178809\n",
      "epoch 349 loss = 2.086618\n",
      "epoch 350 loss = 2.270623\n",
      "epoch 351 loss = 2.119720\n",
      "epoch 352 loss = 1.936725\n",
      "epoch 353 loss = 2.312855\n",
      "epoch 354 loss = 2.338060\n",
      "epoch 355 loss = 1.937657\n",
      "epoch 356 loss = 1.994872\n",
      "epoch 357 loss = 2.056326\n",
      "epoch 358 loss = 2.277178\n",
      "epoch 359 loss = 2.109829\n",
      "epoch 360 loss = 1.952657\n",
      "epoch 361 loss = 2.098484\n",
      "epoch 362 loss = 2.132324\n",
      "epoch 363 loss = 2.216251\n",
      "epoch 364 loss = 2.141305\n",
      "epoch 365 loss = 2.115423\n",
      "epoch 366 loss = 1.786966\n",
      "epoch 367 loss = 2.180428\n",
      "epoch 368 loss = 2.119743\n",
      "epoch 369 loss = 1.988785\n",
      "epoch 370 loss = 2.125732\n",
      "epoch 371 loss = 2.200203\n",
      "epoch 372 loss = 2.296891\n",
      "epoch 373 loss = 2.226427\n",
      "epoch 374 loss = 2.174552\n",
      "epoch 375 loss = 1.962737\n",
      "epoch 376 loss = 2.236900\n",
      "epoch 377 loss = 1.986242\n",
      "epoch 378 loss = 2.269508\n",
      "epoch 379 loss = 2.107719\n",
      "epoch 380 loss = 2.082975\n",
      "epoch 381 loss = 2.030375\n",
      "epoch 382 loss = 2.095654\n",
      "epoch 383 loss = 2.164060\n",
      "epoch 384 loss = 2.260499\n",
      "epoch 385 loss = 2.192654\n",
      "epoch 386 loss = 2.198347\n",
      "epoch 387 loss = 2.226946\n",
      "epoch 388 loss = 2.476387\n",
      "epoch 389 loss = 2.284684\n",
      "epoch 390 loss = 2.025178\n",
      "epoch 391 loss = 2.127427\n",
      "epoch 392 loss = 1.988587\n",
      "epoch 393 loss = 2.195468\n",
      "epoch 394 loss = 2.147359\n",
      "epoch 395 loss = 2.163145\n",
      "epoch 396 loss = 2.157524\n",
      "epoch 397 loss = 2.024583\n",
      "epoch 398 loss = 2.256439\n",
      "epoch 399 loss = 2.242800\n",
      "epoch 400 loss = 1.979795\n",
      "epoch 401 loss = 2.114415\n",
      "epoch 402 loss = 2.048289\n",
      "epoch 403 loss = 1.952705\n",
      "epoch 404 loss = 1.980998\n",
      "epoch 405 loss = 2.101420\n",
      "epoch 406 loss = 1.918676\n",
      "epoch 407 loss = 2.317862\n",
      "epoch 408 loss = 2.116424\n",
      "epoch 409 loss = 2.242559\n",
      "epoch 410 loss = 2.144801\n",
      "epoch 411 loss = 2.215659\n",
      "epoch 412 loss = 2.057496\n",
      "epoch 413 loss = 2.117937\n",
      "epoch 414 loss = 2.060271\n",
      "epoch 415 loss = 2.065596\n",
      "epoch 416 loss = 2.419675\n",
      "epoch 417 loss = 1.996792\n",
      "epoch 418 loss = 2.255761\n",
      "epoch 419 loss = 2.240196\n",
      "epoch 420 loss = 2.060180\n",
      "epoch 421 loss = 2.179906\n",
      "epoch 422 loss = 2.004531\n",
      "epoch 423 loss = 2.159652\n",
      "epoch 424 loss = 2.188741\n",
      "epoch 425 loss = 2.413804\n",
      "epoch 426 loss = 2.139317\n",
      "epoch 427 loss = 2.018170\n",
      "epoch 428 loss = 2.115389\n",
      "epoch 429 loss = 2.142935\n",
      "epoch 430 loss = 2.141712\n",
      "epoch 431 loss = 2.337402\n",
      "epoch 432 loss = 2.020574\n",
      "epoch 433 loss = 2.151697\n",
      "epoch 434 loss = 2.266676\n",
      "epoch 435 loss = 2.055725\n",
      "epoch 436 loss = 2.277664\n",
      "epoch 437 loss = 1.923888\n",
      "epoch 438 loss = 2.166172\n",
      "epoch 439 loss = 1.979031\n",
      "epoch 440 loss = 2.191290\n",
      "epoch 441 loss = 2.130711\n",
      "epoch 442 loss = 2.095375\n",
      "epoch 443 loss = 2.190664\n",
      "epoch 444 loss = 2.041511\n",
      "epoch 445 loss = 2.263277\n",
      "epoch 446 loss = 2.173137\n",
      "epoch 447 loss = 2.186267\n",
      "epoch 448 loss = 2.004409\n",
      "epoch 449 loss = 2.273588\n",
      "epoch 450 loss = 2.017312\n",
      "epoch 451 loss = 1.974089\n",
      "epoch 452 loss = 1.973889\n",
      "epoch 453 loss = 2.035741\n",
      "epoch 454 loss = 2.247047\n",
      "epoch 455 loss = 2.347014\n",
      "epoch 456 loss = 2.289801\n",
      "epoch 457 loss = 2.103489\n",
      "epoch 458 loss = 2.127431\n",
      "epoch 459 loss = 2.154212\n",
      "epoch 460 loss = 2.167443\n",
      "epoch 461 loss = 2.274142\n",
      "epoch 462 loss = 2.118449\n",
      "epoch 463 loss = 1.968145\n",
      "epoch 464 loss = 1.835556\n",
      "epoch 465 loss = 2.177342\n",
      "epoch 466 loss = 2.347733\n",
      "epoch 467 loss = 2.087202\n",
      "epoch 468 loss = 1.957032\n",
      "epoch 469 loss = 1.997855\n",
      "epoch 470 loss = 2.093875\n",
      "epoch 471 loss = 2.080332\n",
      "epoch 472 loss = 2.122827\n",
      "epoch 473 loss = 2.237519\n",
      "epoch 474 loss = 2.291389\n",
      "epoch 475 loss = 2.219674\n",
      "epoch 476 loss = 2.350884\n",
      "epoch 477 loss = 2.034513\n",
      "epoch 478 loss = 2.092743\n",
      "epoch 479 loss = 2.251719\n",
      "epoch 480 loss = 2.318746\n",
      "epoch 481 loss = 2.078441\n",
      "epoch 482 loss = 2.141184\n",
      "epoch 483 loss = 2.206691\n",
      "epoch 484 loss = 1.982776\n",
      "epoch 485 loss = 2.225957\n",
      "epoch 486 loss = 2.134828\n",
      "epoch 487 loss = 1.925849\n",
      "epoch 488 loss = 2.282996\n",
      "epoch 489 loss = 2.186351\n",
      "epoch 490 loss = 2.143819\n",
      "epoch 491 loss = 2.034325\n",
      "epoch 492 loss = 2.243284\n",
      "epoch 493 loss = 2.210538\n",
      "epoch 494 loss = 2.129841\n",
      "epoch 495 loss = 1.992926\n",
      "epoch 496 loss = 2.187006\n",
      "epoch 497 loss = 2.288246\n",
      "epoch 498 loss = 2.235250\n",
      "epoch 499 loss = 2.088588\n",
      "final loss = 2.088588\n",
      "accuracy_mc = tensor(0.3132, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2816, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9804, device='cuda:0')\n",
      "training time = 171.09922337532043 seconds\n",
      "testing time = 1.846445083618164 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = 2.429081\n",
      "epoch 1 loss = 2.299401\n",
      "epoch 2 loss = 2.304993\n",
      "epoch 3 loss = 2.271720\n",
      "epoch 4 loss = 2.266487\n",
      "epoch 5 loss = 2.295219\n",
      "epoch 6 loss = 2.261291\n",
      "epoch 7 loss = 2.307121\n",
      "epoch 8 loss = 2.232764\n",
      "epoch 9 loss = 2.246140\n",
      "epoch 10 loss = 2.285955\n",
      "epoch 11 loss = 2.208400\n",
      "epoch 12 loss = 2.217806\n",
      "epoch 13 loss = 2.176486\n",
      "epoch 14 loss = 2.118025\n",
      "epoch 15 loss = 2.154125\n",
      "epoch 16 loss = 2.254257\n",
      "epoch 17 loss = 2.265847\n",
      "epoch 18 loss = 2.326116\n",
      "epoch 19 loss = 2.265989\n",
      "epoch 20 loss = 2.214239\n",
      "epoch 21 loss = 2.171702\n",
      "epoch 22 loss = 2.316521\n",
      "epoch 23 loss = 2.294634\n",
      "epoch 24 loss = 2.156473\n",
      "epoch 25 loss = 2.283716\n",
      "epoch 26 loss = 2.428676\n",
      "epoch 27 loss = 2.241131\n",
      "epoch 28 loss = 2.129615\n",
      "epoch 29 loss = 2.277642\n",
      "epoch 30 loss = 2.287050\n",
      "epoch 31 loss = 2.114541\n",
      "epoch 32 loss = 2.323826\n",
      "epoch 33 loss = 2.375499\n",
      "epoch 34 loss = 2.182529\n",
      "epoch 35 loss = 2.529239\n",
      "epoch 36 loss = 2.296668\n",
      "epoch 37 loss = 2.347663\n",
      "epoch 38 loss = 2.118968\n",
      "epoch 39 loss = 2.379469\n",
      "epoch 40 loss = 2.201411\n",
      "epoch 41 loss = 2.337606\n",
      "epoch 42 loss = 2.233722\n",
      "epoch 43 loss = 2.060643\n",
      "epoch 44 loss = 2.367852\n",
      "epoch 45 loss = 2.322316\n",
      "epoch 46 loss = 2.209098\n",
      "epoch 47 loss = 2.206955\n",
      "epoch 48 loss = 2.264181\n",
      "epoch 49 loss = 2.325142\n",
      "epoch 50 loss = 2.361249\n",
      "epoch 51 loss = 2.345857\n",
      "epoch 52 loss = 2.311240\n",
      "epoch 53 loss = 2.469470\n",
      "epoch 54 loss = 2.251933\n",
      "epoch 55 loss = 2.343935\n",
      "epoch 56 loss = 2.346380\n",
      "epoch 57 loss = 2.368669\n",
      "epoch 58 loss = 2.336321\n",
      "epoch 59 loss = 2.473598\n",
      "epoch 60 loss = 2.202837\n",
      "epoch 61 loss = 2.344330\n",
      "epoch 62 loss = 2.293138\n",
      "epoch 63 loss = 2.276474\n",
      "epoch 64 loss = 2.272584\n",
      "epoch 65 loss = 2.453071\n",
      "epoch 66 loss = 2.183408\n",
      "epoch 67 loss = 2.576691\n",
      "epoch 68 loss = 2.246888\n",
      "epoch 69 loss = 2.162465\n",
      "epoch 70 loss = 2.293690\n",
      "epoch 71 loss = 2.262513\n",
      "epoch 72 loss = 2.170525\n",
      "epoch 73 loss = 1.948143\n",
      "epoch 74 loss = 2.345353\n",
      "epoch 75 loss = 2.156311\n",
      "epoch 76 loss = 2.224428\n",
      "epoch 77 loss = 2.521941\n",
      "epoch 78 loss = 2.226725\n",
      "epoch 79 loss = 2.148165\n",
      "epoch 80 loss = 2.248351\n",
      "epoch 81 loss = 2.327201\n",
      "epoch 82 loss = 2.383456\n",
      "epoch 83 loss = 2.284653\n",
      "epoch 84 loss = 2.236986\n",
      "epoch 85 loss = 2.437677\n",
      "epoch 86 loss = 2.526348\n",
      "epoch 87 loss = 2.228444\n",
      "epoch 88 loss = 2.365988\n",
      "epoch 89 loss = 2.278489\n",
      "epoch 90 loss = 2.520320\n",
      "epoch 91 loss = 2.303603\n",
      "epoch 92 loss = 2.281164\n",
      "epoch 93 loss = 2.191634\n",
      "epoch 94 loss = 2.277328\n",
      "epoch 95 loss = 2.298928\n",
      "epoch 96 loss = 2.153658\n",
      "epoch 97 loss = 2.568221\n",
      "epoch 98 loss = 2.347112\n",
      "epoch 99 loss = 2.248737\n",
      "epoch 100 loss = 2.316286\n",
      "epoch 101 loss = 2.223331\n",
      "epoch 102 loss = 2.217509\n",
      "epoch 103 loss = 2.172766\n",
      "epoch 104 loss = 2.167980\n",
      "epoch 105 loss = 2.387741\n",
      "epoch 106 loss = 2.388728\n",
      "epoch 107 loss = 2.135317\n",
      "epoch 108 loss = 2.086644\n",
      "epoch 109 loss = 2.268996\n",
      "epoch 110 loss = 2.094293\n",
      "epoch 111 loss = 2.106446\n",
      "epoch 112 loss = 2.180802\n",
      "epoch 113 loss = 2.130664\n",
      "epoch 114 loss = 1.956213\n",
      "epoch 115 loss = 2.365739\n",
      "epoch 116 loss = 2.288959\n",
      "epoch 117 loss = 1.990082\n",
      "epoch 118 loss = 2.164048\n",
      "epoch 119 loss = 2.225807\n",
      "epoch 120 loss = 2.386629\n",
      "epoch 121 loss = 2.063637\n",
      "epoch 122 loss = 2.030093\n",
      "epoch 123 loss = 2.225665\n",
      "epoch 124 loss = 2.550885\n",
      "epoch 125 loss = 2.268628\n",
      "epoch 126 loss = 2.216177\n",
      "epoch 127 loss = 2.503925\n",
      "epoch 128 loss = 2.291218\n",
      "epoch 129 loss = 2.226945\n",
      "epoch 130 loss = 2.160892\n",
      "epoch 131 loss = 2.445379\n",
      "epoch 132 loss = 2.021021\n",
      "epoch 133 loss = 2.205478\n",
      "epoch 134 loss = 2.113726\n",
      "epoch 135 loss = 2.079116\n",
      "epoch 136 loss = 2.146903\n",
      "epoch 137 loss = 2.143136\n",
      "epoch 138 loss = 2.127627\n",
      "epoch 139 loss = 2.057747\n",
      "epoch 140 loss = 2.297727\n",
      "epoch 141 loss = 2.111413\n",
      "epoch 142 loss = 2.256328\n",
      "epoch 143 loss = 2.374824\n",
      "epoch 144 loss = 2.106515\n",
      "epoch 145 loss = 2.187588\n",
      "epoch 146 loss = 2.144685\n",
      "epoch 147 loss = 2.189552\n",
      "epoch 148 loss = 2.263794\n",
      "epoch 149 loss = 2.173882\n",
      "epoch 150 loss = 2.222923\n",
      "epoch 151 loss = 2.187372\n",
      "epoch 152 loss = 2.048578\n",
      "epoch 153 loss = 2.170133\n",
      "epoch 154 loss = 2.161654\n",
      "epoch 155 loss = 2.197196\n",
      "epoch 156 loss = 2.125783\n",
      "epoch 157 loss = 2.299923\n",
      "epoch 158 loss = 2.163880\n",
      "epoch 159 loss = 2.612573\n",
      "epoch 160 loss = 2.234156\n",
      "epoch 161 loss = 2.338759\n",
      "epoch 162 loss = 2.197444\n",
      "epoch 163 loss = 2.272079\n",
      "epoch 164 loss = 2.110078\n",
      "epoch 165 loss = 2.215179\n",
      "epoch 166 loss = 2.105711\n",
      "epoch 167 loss = 2.222546\n",
      "epoch 168 loss = 2.300865\n",
      "epoch 169 loss = 2.165406\n",
      "epoch 170 loss = 2.149228\n",
      "epoch 171 loss = 2.110781\n",
      "epoch 172 loss = 2.356888\n",
      "epoch 173 loss = 2.153311\n",
      "epoch 174 loss = 2.303054\n",
      "epoch 175 loss = 2.290737\n",
      "epoch 176 loss = 2.332929\n",
      "epoch 177 loss = 2.130099\n",
      "epoch 178 loss = 2.165020\n",
      "epoch 179 loss = 2.166338\n",
      "epoch 180 loss = 2.304351\n",
      "epoch 181 loss = 2.228776\n",
      "epoch 182 loss = 2.263357\n",
      "epoch 183 loss = 2.260937\n",
      "epoch 184 loss = 2.027568\n",
      "epoch 185 loss = 2.211781\n",
      "epoch 186 loss = 2.275961\n",
      "epoch 187 loss = 2.044281\n",
      "epoch 188 loss = 2.144200\n",
      "epoch 189 loss = 2.219789\n",
      "epoch 190 loss = 2.219877\n",
      "epoch 191 loss = 2.116467\n",
      "epoch 192 loss = 2.253508\n",
      "epoch 193 loss = 2.281896\n",
      "epoch 194 loss = 2.117062\n",
      "epoch 195 loss = 2.231407\n",
      "epoch 196 loss = 2.239993\n",
      "epoch 197 loss = 2.116206\n",
      "epoch 198 loss = 2.350885\n",
      "epoch 199 loss = 2.242756\n",
      "epoch 200 loss = 2.157559\n",
      "epoch 201 loss = 2.158641\n",
      "epoch 202 loss = 2.165510\n",
      "epoch 203 loss = 2.274569\n",
      "epoch 204 loss = 2.266857\n",
      "epoch 205 loss = 2.218560\n",
      "epoch 206 loss = 2.100323\n",
      "epoch 207 loss = 2.316164\n",
      "epoch 208 loss = 2.174423\n",
      "epoch 209 loss = 2.171301\n",
      "epoch 210 loss = 2.355111\n",
      "epoch 211 loss = 2.246876\n",
      "epoch 212 loss = 2.096599\n",
      "epoch 213 loss = 2.357229\n",
      "epoch 214 loss = 2.189662\n",
      "epoch 215 loss = 2.214977\n",
      "epoch 216 loss = 2.474876\n",
      "epoch 217 loss = 2.230391\n",
      "epoch 218 loss = 2.128978\n",
      "epoch 219 loss = 2.244908\n",
      "epoch 220 loss = 2.215703\n",
      "epoch 221 loss = 2.306765\n",
      "epoch 222 loss = 2.031536\n",
      "epoch 223 loss = 2.236763\n",
      "epoch 224 loss = 2.247606\n",
      "epoch 225 loss = 2.132941\n",
      "epoch 226 loss = 2.182914\n",
      "epoch 227 loss = 2.156115\n",
      "epoch 228 loss = 2.110516\n",
      "epoch 229 loss = 2.166267\n",
      "epoch 230 loss = 2.169695\n",
      "epoch 231 loss = 2.191932\n",
      "epoch 232 loss = 2.129983\n",
      "epoch 233 loss = 2.258734\n",
      "epoch 234 loss = 2.204573\n",
      "epoch 235 loss = 2.185745\n",
      "epoch 236 loss = 2.024779\n",
      "epoch 237 loss = 2.221425\n",
      "epoch 238 loss = 2.477170\n",
      "epoch 239 loss = 1.962869\n",
      "epoch 240 loss = 1.985611\n",
      "epoch 241 loss = 2.128750\n",
      "epoch 242 loss = 2.254812\n",
      "epoch 243 loss = 2.098519\n",
      "epoch 244 loss = 2.308079\n",
      "epoch 245 loss = 2.190797\n",
      "epoch 246 loss = 2.520546\n",
      "epoch 247 loss = 1.877312\n",
      "epoch 248 loss = 2.224193\n",
      "epoch 249 loss = 2.374526\n",
      "epoch 250 loss = 2.252069\n",
      "epoch 251 loss = 2.268922\n",
      "epoch 252 loss = 2.026496\n",
      "epoch 253 loss = 2.024867\n",
      "epoch 254 loss = 2.111510\n",
      "epoch 255 loss = 2.180604\n",
      "epoch 256 loss = 2.290915\n",
      "epoch 257 loss = 2.138701\n",
      "epoch 258 loss = 1.975107\n",
      "epoch 259 loss = 2.136947\n",
      "epoch 260 loss = 2.106340\n",
      "epoch 261 loss = 1.945710\n",
      "epoch 262 loss = 2.120615\n",
      "epoch 263 loss = 2.333038\n",
      "epoch 264 loss = 2.112638\n",
      "epoch 265 loss = 2.355875\n",
      "epoch 266 loss = 2.170995\n",
      "epoch 267 loss = 2.286919\n",
      "epoch 268 loss = 2.119421\n",
      "epoch 269 loss = 2.219685\n",
      "epoch 270 loss = 2.220852\n",
      "epoch 271 loss = 2.066687\n",
      "epoch 272 loss = 2.138367\n",
      "epoch 273 loss = 2.094805\n",
      "epoch 274 loss = 2.170238\n",
      "epoch 275 loss = 2.218937\n",
      "epoch 276 loss = 2.259581\n",
      "epoch 277 loss = 2.239198\n",
      "epoch 278 loss = 2.055238\n",
      "epoch 279 loss = 2.054803\n",
      "epoch 280 loss = 2.086689\n",
      "epoch 281 loss = 2.298782\n",
      "epoch 282 loss = 2.116148\n",
      "epoch 283 loss = 2.028086\n",
      "epoch 284 loss = 2.119662\n",
      "epoch 285 loss = 2.121325\n",
      "epoch 286 loss = 2.126147\n",
      "epoch 287 loss = 2.033755\n",
      "epoch 288 loss = 2.206256\n",
      "epoch 289 loss = 2.146753\n",
      "epoch 290 loss = 1.933648\n",
      "epoch 291 loss = 2.099009\n",
      "epoch 292 loss = 2.255399\n",
      "epoch 293 loss = 2.156943\n",
      "epoch 294 loss = 2.086416\n",
      "epoch 295 loss = 2.083058\n",
      "epoch 296 loss = 2.025362\n",
      "epoch 297 loss = 2.234083\n",
      "epoch 298 loss = 2.171945\n",
      "epoch 299 loss = 2.104393\n",
      "epoch 300 loss = 2.177948\n",
      "epoch 301 loss = 2.248578\n",
      "epoch 302 loss = 2.273296\n",
      "epoch 303 loss = 2.172446\n",
      "epoch 304 loss = 2.265193\n",
      "epoch 305 loss = 2.061166\n",
      "epoch 306 loss = 2.166026\n",
      "epoch 307 loss = 1.967525\n",
      "epoch 308 loss = 2.117472\n",
      "epoch 309 loss = 2.140962\n",
      "epoch 310 loss = 2.172660\n",
      "epoch 311 loss = 2.148186\n",
      "epoch 312 loss = 1.988135\n",
      "epoch 313 loss = 2.032669\n",
      "epoch 314 loss = 2.307065\n",
      "epoch 315 loss = 2.083134\n",
      "epoch 316 loss = 2.090249\n",
      "epoch 317 loss = 2.126704\n",
      "epoch 318 loss = 1.961596\n",
      "epoch 319 loss = 2.198466\n",
      "epoch 320 loss = 2.248928\n",
      "epoch 321 loss = 1.986122\n",
      "epoch 322 loss = 2.177912\n",
      "epoch 323 loss = 2.031370\n",
      "epoch 324 loss = 2.155057\n",
      "epoch 325 loss = 2.263303\n",
      "epoch 326 loss = 2.086517\n",
      "epoch 327 loss = 2.047827\n",
      "epoch 328 loss = 2.039307\n",
      "epoch 329 loss = 2.356124\n",
      "epoch 330 loss = 2.265106\n",
      "epoch 331 loss = 2.033797\n",
      "epoch 332 loss = 2.114090\n",
      "epoch 333 loss = 1.959970\n",
      "epoch 334 loss = 2.086521\n",
      "epoch 335 loss = 2.216988\n",
      "epoch 336 loss = 2.233065\n",
      "epoch 337 loss = 2.146337\n",
      "epoch 338 loss = 2.074695\n",
      "epoch 339 loss = 2.099986\n",
      "epoch 340 loss = 2.109233\n",
      "epoch 341 loss = 2.177482\n",
      "epoch 342 loss = 2.077777\n",
      "epoch 343 loss = 2.091252\n",
      "epoch 344 loss = 2.128916\n",
      "epoch 345 loss = 2.228522\n",
      "epoch 346 loss = 2.235334\n",
      "epoch 347 loss = 2.119809\n",
      "epoch 348 loss = 2.037905\n",
      "epoch 349 loss = 2.252736\n",
      "epoch 350 loss = 2.064296\n",
      "epoch 351 loss = 2.063711\n",
      "epoch 352 loss = 2.162793\n",
      "epoch 353 loss = 1.966127\n",
      "epoch 354 loss = 2.107655\n",
      "epoch 355 loss = 2.219662\n",
      "epoch 356 loss = 2.269125\n",
      "epoch 357 loss = 1.967403\n",
      "epoch 358 loss = 2.109560\n",
      "epoch 359 loss = 2.211553\n",
      "epoch 360 loss = 1.920442\n",
      "epoch 361 loss = 2.346796\n",
      "epoch 362 loss = 1.969572\n",
      "epoch 363 loss = 2.184093\n",
      "epoch 364 loss = 2.133258\n",
      "epoch 365 loss = 2.129405\n",
      "epoch 366 loss = 2.242100\n",
      "epoch 367 loss = 2.119366\n",
      "epoch 368 loss = 1.872091\n",
      "epoch 369 loss = 2.338368\n",
      "epoch 370 loss = 1.884015\n",
      "epoch 371 loss = 2.464857\n",
      "epoch 372 loss = 2.222641\n",
      "epoch 373 loss = 2.130520\n",
      "epoch 374 loss = 2.150280\n",
      "epoch 375 loss = 2.097641\n",
      "epoch 376 loss = 2.218036\n",
      "epoch 377 loss = 2.027562\n",
      "epoch 378 loss = 2.111552\n",
      "epoch 379 loss = 2.067788\n",
      "epoch 380 loss = 2.107285\n",
      "epoch 381 loss = 2.353930\n",
      "epoch 382 loss = 2.119764\n",
      "epoch 383 loss = 2.119655\n",
      "epoch 384 loss = 2.008413\n",
      "epoch 385 loss = 2.241888\n",
      "epoch 386 loss = 2.449473\n",
      "epoch 387 loss = 2.192619\n",
      "epoch 388 loss = 2.206295\n",
      "epoch 389 loss = 2.150920\n",
      "epoch 390 loss = 2.028891\n",
      "epoch 391 loss = 1.909509\n",
      "epoch 392 loss = 2.240722\n",
      "epoch 393 loss = 2.187817\n",
      "epoch 394 loss = 2.002512\n",
      "epoch 395 loss = 2.005189\n",
      "epoch 396 loss = 2.185367\n",
      "epoch 397 loss = 2.130667\n",
      "epoch 398 loss = 2.210229\n",
      "epoch 399 loss = 2.145145\n",
      "epoch 400 loss = 2.112603\n",
      "epoch 401 loss = 2.383354\n",
      "epoch 402 loss = 1.925381\n",
      "epoch 403 loss = 2.396704\n",
      "epoch 404 loss = 2.441171\n",
      "epoch 405 loss = 2.179788\n",
      "epoch 406 loss = 2.280304\n",
      "epoch 407 loss = 2.171725\n",
      "epoch 408 loss = 2.269040\n",
      "epoch 409 loss = 1.997196\n",
      "epoch 410 loss = 2.030984\n",
      "epoch 411 loss = 2.266471\n",
      "epoch 412 loss = 2.029831\n",
      "epoch 413 loss = 2.377148\n",
      "epoch 414 loss = 2.297466\n",
      "epoch 415 loss = 2.055980\n",
      "epoch 416 loss = 2.275970\n",
      "epoch 417 loss = 2.048293\n",
      "epoch 418 loss = 1.948129\n",
      "epoch 419 loss = 2.300007\n",
      "epoch 420 loss = 2.072211\n",
      "epoch 421 loss = 2.209211\n",
      "epoch 422 loss = 2.206535\n",
      "epoch 423 loss = 2.128600\n",
      "epoch 424 loss = 2.235953\n",
      "epoch 425 loss = 2.035831\n",
      "epoch 426 loss = 2.268519\n",
      "epoch 427 loss = 1.962575\n",
      "epoch 428 loss = 1.947640\n",
      "epoch 429 loss = 2.170083\n",
      "epoch 430 loss = 2.017648\n",
      "epoch 431 loss = 2.176610\n",
      "epoch 432 loss = 2.161159\n",
      "epoch 433 loss = 2.245324\n",
      "epoch 434 loss = 2.084018\n",
      "epoch 435 loss = 2.151604\n",
      "epoch 436 loss = 2.207273\n",
      "epoch 437 loss = 2.173331\n",
      "epoch 438 loss = 2.211970\n",
      "epoch 439 loss = 2.310278\n",
      "epoch 440 loss = 2.079301\n",
      "epoch 441 loss = 2.159681\n",
      "epoch 442 loss = 1.983331\n",
      "epoch 443 loss = 2.128256\n",
      "epoch 444 loss = 2.196082\n",
      "epoch 445 loss = 2.022488\n",
      "epoch 446 loss = 2.329446\n",
      "epoch 447 loss = 2.157526\n",
      "epoch 448 loss = 2.142490\n",
      "epoch 449 loss = 2.196929\n",
      "epoch 450 loss = 2.191212\n",
      "epoch 451 loss = 2.346632\n",
      "epoch 452 loss = 1.935913\n",
      "epoch 453 loss = 2.089596\n",
      "epoch 454 loss = 2.241776\n",
      "epoch 455 loss = 2.113419\n",
      "epoch 456 loss = 2.093576\n",
      "epoch 457 loss = 1.946119\n",
      "epoch 458 loss = 2.055140\n",
      "epoch 459 loss = 2.054727\n",
      "epoch 460 loss = 1.961699\n",
      "epoch 461 loss = 1.935747\n",
      "epoch 462 loss = 2.153961\n",
      "epoch 463 loss = 2.191146\n",
      "epoch 464 loss = 2.020888\n",
      "epoch 465 loss = 2.113279\n",
      "epoch 466 loss = 2.196532\n",
      "epoch 467 loss = 2.106583\n",
      "epoch 468 loss = 2.246312\n",
      "epoch 469 loss = 2.498558\n",
      "epoch 470 loss = 2.180650\n",
      "epoch 471 loss = 2.243092\n",
      "epoch 472 loss = 2.168768\n",
      "epoch 473 loss = 2.196022\n",
      "epoch 474 loss = 2.221500\n",
      "epoch 475 loss = 2.112992\n",
      "epoch 476 loss = 2.191374\n",
      "epoch 477 loss = 2.247973\n",
      "epoch 478 loss = 2.060935\n",
      "epoch 479 loss = 1.994001\n",
      "epoch 480 loss = 2.195294\n",
      "epoch 481 loss = 2.126018\n",
      "epoch 482 loss = 2.073069\n",
      "epoch 483 loss = 1.986804\n",
      "epoch 484 loss = 2.287964\n",
      "epoch 485 loss = 2.177649\n",
      "epoch 486 loss = 2.152915\n",
      "epoch 487 loss = 2.091598\n",
      "epoch 488 loss = 2.143048\n",
      "epoch 489 loss = 2.209709\n",
      "epoch 490 loss = 2.261430\n",
      "epoch 491 loss = 2.285322\n",
      "epoch 492 loss = 2.326579\n",
      "epoch 493 loss = 1.997864\n",
      "epoch 494 loss = 2.179488\n",
      "epoch 495 loss = 1.993793\n",
      "epoch 496 loss = 2.258893\n",
      "epoch 497 loss = 2.047029\n",
      "epoch 498 loss = 2.194772\n",
      "epoch 499 loss = 2.119688\n",
      "final loss = 2.119688\n",
      "accuracy_mc = tensor(0.3428, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3026, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.8990, device='cuda:0')\n",
      "training time = 171.03489685058594 seconds\n",
      "testing time = 1.8735463619232178 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = 2.303532\n",
      "epoch 1 loss = 2.292159\n",
      "epoch 2 loss = 2.280284\n",
      "epoch 3 loss = 2.261982\n",
      "epoch 4 loss = 2.167783\n",
      "epoch 5 loss = 2.190870\n",
      "epoch 6 loss = 2.186076\n",
      "epoch 7 loss = 2.129551\n",
      "epoch 8 loss = 2.145375\n",
      "epoch 9 loss = 2.187947\n",
      "epoch 10 loss = 2.096708\n",
      "epoch 11 loss = 2.160300\n",
      "epoch 12 loss = 2.054193\n",
      "epoch 13 loss = 2.085636\n",
      "epoch 14 loss = 2.199857\n",
      "epoch 15 loss = 2.119193\n",
      "epoch 16 loss = 2.025095\n",
      "epoch 17 loss = 2.115017\n",
      "epoch 18 loss = 2.098063\n",
      "epoch 19 loss = 2.208143\n",
      "epoch 20 loss = 1.985458\n",
      "epoch 21 loss = 2.121749\n",
      "epoch 22 loss = 1.951123\n",
      "epoch 23 loss = 2.026753\n",
      "epoch 24 loss = 2.094635\n",
      "epoch 25 loss = 2.116372\n",
      "epoch 26 loss = 2.207555\n",
      "epoch 27 loss = 2.232237\n",
      "epoch 28 loss = 2.159882\n",
      "epoch 29 loss = 2.047968\n",
      "epoch 30 loss = 1.955792\n",
      "epoch 31 loss = 2.097631\n",
      "epoch 32 loss = 2.258099\n",
      "epoch 33 loss = 2.110087\n",
      "epoch 34 loss = 2.138890\n",
      "epoch 35 loss = 1.994726\n",
      "epoch 36 loss = 1.959300\n",
      "epoch 37 loss = 2.069741\n",
      "epoch 38 loss = 2.133392\n",
      "epoch 39 loss = 2.065145\n",
      "epoch 40 loss = 2.131632\n",
      "epoch 41 loss = 2.128880\n",
      "epoch 42 loss = 2.201779\n",
      "epoch 43 loss = 2.223125\n",
      "epoch 44 loss = 2.254604\n",
      "epoch 45 loss = 2.260569\n",
      "epoch 46 loss = 2.147800\n",
      "epoch 47 loss = 2.101438\n",
      "epoch 48 loss = 2.139765\n",
      "epoch 49 loss = 2.207193\n",
      "epoch 50 loss = 2.137029\n",
      "epoch 51 loss = 2.075397\n",
      "epoch 52 loss = 2.147707\n",
      "epoch 53 loss = 2.134147\n",
      "epoch 54 loss = 2.040359\n",
      "epoch 55 loss = 2.036917\n",
      "epoch 56 loss = 2.130243\n",
      "epoch 57 loss = 2.104769\n",
      "epoch 58 loss = 2.002759\n",
      "epoch 59 loss = 2.082972\n",
      "epoch 60 loss = 2.079311\n",
      "epoch 61 loss = 2.214980\n",
      "epoch 62 loss = 2.106146\n",
      "epoch 63 loss = 2.066508\n",
      "epoch 64 loss = 2.004425\n",
      "epoch 65 loss = 2.024522\n",
      "epoch 66 loss = 1.947913\n",
      "epoch 67 loss = 1.974555\n",
      "epoch 68 loss = 1.954033\n",
      "epoch 69 loss = 2.053036\n",
      "epoch 70 loss = 2.134423\n",
      "epoch 71 loss = 2.103623\n",
      "epoch 72 loss = 2.055011\n",
      "epoch 73 loss = 2.122406\n",
      "epoch 74 loss = 1.950707\n",
      "epoch 75 loss = 2.071958\n",
      "epoch 76 loss = 2.116914\n",
      "epoch 77 loss = 2.120076\n",
      "epoch 78 loss = 2.078614\n",
      "epoch 79 loss = 2.126474\n",
      "epoch 80 loss = 1.944944\n",
      "epoch 81 loss = 2.177011\n",
      "epoch 82 loss = 1.965733\n",
      "epoch 83 loss = 2.098647\n",
      "epoch 84 loss = 2.019487\n",
      "epoch 85 loss = 2.078608\n",
      "epoch 86 loss = 2.206789\n",
      "epoch 87 loss = 2.025350\n",
      "epoch 88 loss = 1.898679\n",
      "epoch 89 loss = 2.155813\n",
      "epoch 90 loss = 1.965984\n",
      "epoch 91 loss = 2.131809\n",
      "epoch 92 loss = 1.947927\n",
      "epoch 93 loss = 1.918813\n",
      "epoch 94 loss = 2.126527\n",
      "epoch 95 loss = 2.076664\n",
      "epoch 96 loss = 2.038990\n",
      "epoch 97 loss = 2.011409\n",
      "epoch 98 loss = 2.145273\n",
      "epoch 99 loss = 1.962935\n",
      "epoch 100 loss = 2.101137\n",
      "epoch 101 loss = 2.210235\n",
      "epoch 102 loss = 1.957213\n",
      "epoch 103 loss = 2.145829\n",
      "epoch 104 loss = 2.034080\n",
      "epoch 105 loss = 2.084963\n",
      "epoch 106 loss = 2.117547\n",
      "epoch 107 loss = 2.043829\n",
      "epoch 108 loss = 2.066921\n",
      "epoch 109 loss = 1.921919\n",
      "epoch 110 loss = 1.990361\n",
      "epoch 111 loss = 2.070894\n",
      "epoch 112 loss = 1.995104\n",
      "epoch 113 loss = 2.086020\n",
      "epoch 114 loss = 2.084587\n",
      "epoch 115 loss = 2.074889\n",
      "epoch 116 loss = 1.985631\n",
      "epoch 117 loss = 2.128723\n",
      "epoch 118 loss = 2.203456\n",
      "epoch 119 loss = 2.192759\n",
      "epoch 120 loss = 2.031517\n",
      "epoch 121 loss = 2.032410\n",
      "epoch 122 loss = 2.021222\n",
      "epoch 123 loss = 1.863790\n",
      "epoch 124 loss = 2.007063\n",
      "epoch 125 loss = 2.057804\n",
      "epoch 126 loss = 2.049678\n",
      "epoch 127 loss = 2.045639\n",
      "epoch 128 loss = 2.139418\n",
      "epoch 129 loss = 1.947383\n",
      "epoch 130 loss = 2.005876\n",
      "epoch 131 loss = 2.125870\n",
      "epoch 132 loss = 2.061036\n",
      "epoch 133 loss = 2.098049\n",
      "epoch 134 loss = 2.009135\n",
      "epoch 135 loss = 2.049354\n",
      "epoch 136 loss = 2.012984\n",
      "epoch 137 loss = 1.996874\n",
      "epoch 138 loss = 1.865794\n",
      "epoch 139 loss = 2.016371\n",
      "epoch 140 loss = 2.040364\n",
      "epoch 141 loss = 2.232621\n",
      "epoch 142 loss = 2.014694\n",
      "epoch 143 loss = 2.086025\n",
      "epoch 144 loss = 2.121197\n",
      "epoch 145 loss = 2.159610\n",
      "epoch 146 loss = 2.109433\n",
      "epoch 147 loss = 2.024060\n",
      "epoch 148 loss = 1.970233\n",
      "epoch 149 loss = 2.049086\n",
      "epoch 150 loss = 1.964647\n",
      "epoch 151 loss = 2.023758\n",
      "epoch 152 loss = 1.940807\n",
      "epoch 153 loss = 2.000637\n",
      "epoch 154 loss = 1.997742\n",
      "epoch 155 loss = 2.046759\n",
      "epoch 156 loss = 1.976960\n",
      "epoch 157 loss = 2.084285\n",
      "epoch 158 loss = 1.984014\n",
      "epoch 159 loss = 2.030555\n",
      "epoch 160 loss = 2.044778\n",
      "epoch 161 loss = 2.016682\n",
      "epoch 162 loss = 2.068873\n",
      "epoch 163 loss = 2.228130\n",
      "epoch 164 loss = 1.955146\n",
      "epoch 165 loss = 2.037019\n",
      "epoch 166 loss = 2.054705\n",
      "epoch 167 loss = 2.052194\n",
      "epoch 168 loss = 2.049648\n",
      "epoch 169 loss = 2.022547\n",
      "epoch 170 loss = 2.143651\n",
      "epoch 171 loss = 2.033760\n",
      "epoch 172 loss = 2.177592\n",
      "epoch 173 loss = 2.037479\n",
      "epoch 174 loss = 1.806075\n",
      "epoch 175 loss = 2.065521\n",
      "epoch 176 loss = 1.953158\n",
      "epoch 177 loss = 1.983151\n",
      "epoch 178 loss = 2.027693\n",
      "epoch 179 loss = 2.060091\n",
      "epoch 180 loss = 1.945213\n",
      "epoch 181 loss = 2.033980\n",
      "epoch 182 loss = 2.106469\n",
      "epoch 183 loss = 2.103489\n",
      "epoch 184 loss = 2.067496\n",
      "epoch 185 loss = 1.965909\n",
      "epoch 186 loss = 2.057435\n",
      "epoch 187 loss = 1.969938\n",
      "epoch 188 loss = 2.082533\n",
      "epoch 189 loss = 2.073785\n",
      "epoch 190 loss = 2.109790\n",
      "epoch 191 loss = 1.985990\n",
      "epoch 192 loss = 1.940851\n",
      "epoch 193 loss = 1.939547\n",
      "epoch 194 loss = 2.061111\n",
      "epoch 195 loss = 2.065669\n",
      "epoch 196 loss = 2.091899\n",
      "epoch 197 loss = 2.070909\n",
      "epoch 198 loss = 2.104901\n",
      "epoch 199 loss = 1.990312\n",
      "epoch 200 loss = 1.891354\n",
      "epoch 201 loss = 2.090235\n",
      "epoch 202 loss = 2.050591\n",
      "epoch 203 loss = 1.977457\n",
      "epoch 204 loss = 2.031627\n",
      "epoch 205 loss = 2.158999\n",
      "epoch 206 loss = 2.061187\n",
      "epoch 207 loss = 2.226502\n",
      "epoch 208 loss = 2.207643\n",
      "epoch 209 loss = 1.940648\n",
      "epoch 210 loss = 1.843265\n",
      "epoch 211 loss = 1.994276\n",
      "epoch 212 loss = 2.049954\n",
      "epoch 213 loss = 2.079353\n",
      "epoch 214 loss = 2.050269\n",
      "epoch 215 loss = 2.114890\n",
      "epoch 216 loss = 1.945839\n",
      "epoch 217 loss = 1.977909\n",
      "epoch 218 loss = 1.974199\n",
      "epoch 219 loss = 2.230922\n",
      "epoch 220 loss = 2.010162\n",
      "epoch 221 loss = 2.039132\n",
      "epoch 222 loss = 1.990621\n",
      "epoch 223 loss = 2.094702\n",
      "epoch 224 loss = 1.948938\n",
      "epoch 225 loss = 1.890240\n",
      "epoch 226 loss = 1.886571\n",
      "epoch 227 loss = 1.966345\n",
      "epoch 228 loss = 1.952454\n",
      "epoch 229 loss = 1.993844\n",
      "epoch 230 loss = 2.036485\n",
      "epoch 231 loss = 1.937504\n",
      "epoch 232 loss = 1.993332\n",
      "epoch 233 loss = 2.123888\n",
      "epoch 234 loss = 2.103508\n",
      "epoch 235 loss = 2.001289\n",
      "epoch 236 loss = 2.205474\n",
      "epoch 237 loss = 2.052304\n",
      "epoch 238 loss = 2.145967\n",
      "epoch 239 loss = 2.060632\n",
      "epoch 240 loss = 2.023807\n",
      "epoch 241 loss = 2.019770\n",
      "epoch 242 loss = 2.062216\n",
      "epoch 243 loss = 2.037138\n",
      "epoch 244 loss = 1.921259\n",
      "epoch 245 loss = 2.168480\n",
      "epoch 246 loss = 1.864925\n",
      "epoch 247 loss = 1.954647\n",
      "epoch 248 loss = 1.867208\n",
      "epoch 249 loss = 1.773415\n",
      "epoch 250 loss = 2.051865\n",
      "epoch 251 loss = 1.882397\n",
      "epoch 252 loss = 2.082588\n",
      "epoch 253 loss = 1.947358\n",
      "epoch 254 loss = 2.030968\n",
      "epoch 255 loss = 1.840981\n",
      "epoch 256 loss = 1.943303\n",
      "epoch 257 loss = 1.908308\n",
      "epoch 258 loss = 1.915818\n",
      "epoch 259 loss = 2.006159\n",
      "epoch 260 loss = 2.044044\n",
      "epoch 261 loss = 1.978864\n",
      "epoch 262 loss = 1.983126\n",
      "epoch 263 loss = 2.113230\n",
      "epoch 264 loss = 1.945152\n",
      "epoch 265 loss = 2.071571\n",
      "epoch 266 loss = 1.925205\n",
      "epoch 267 loss = 1.934863\n",
      "epoch 268 loss = 2.009855\n",
      "epoch 269 loss = 2.201393\n",
      "epoch 270 loss = 1.878898\n",
      "epoch 271 loss = 1.832119\n",
      "epoch 272 loss = 2.015971\n",
      "epoch 273 loss = 2.095293\n",
      "epoch 274 loss = 2.091575\n",
      "epoch 275 loss = 2.079189\n",
      "epoch 276 loss = 2.073155\n",
      "epoch 277 loss = 2.037433\n",
      "epoch 278 loss = 2.106344\n",
      "epoch 279 loss = 2.122662\n",
      "epoch 280 loss = 1.999493\n",
      "epoch 281 loss = 1.777978\n",
      "epoch 282 loss = 1.983375\n",
      "epoch 283 loss = 1.945304\n",
      "epoch 284 loss = 1.985453\n",
      "epoch 285 loss = 1.973472\n",
      "epoch 286 loss = 2.023697\n",
      "epoch 287 loss = 2.004003\n",
      "epoch 288 loss = 1.989796\n",
      "epoch 289 loss = 2.121900\n",
      "epoch 290 loss = 1.998550\n",
      "epoch 291 loss = 1.988185\n",
      "epoch 292 loss = 2.036894\n",
      "epoch 293 loss = 2.029753\n",
      "epoch 294 loss = 2.138614\n",
      "epoch 295 loss = 1.853771\n",
      "epoch 296 loss = 1.997202\n",
      "epoch 297 loss = 1.897160\n",
      "epoch 298 loss = 2.139125\n",
      "epoch 299 loss = 2.121483\n",
      "epoch 300 loss = 2.145622\n",
      "epoch 301 loss = 1.834427\n",
      "epoch 302 loss = 2.032566\n",
      "epoch 303 loss = 1.932709\n",
      "epoch 304 loss = 2.013169\n",
      "epoch 305 loss = 1.908728\n",
      "epoch 306 loss = 2.096076\n",
      "epoch 307 loss = 1.905376\n",
      "epoch 308 loss = 2.013919\n",
      "epoch 309 loss = 1.934245\n",
      "epoch 310 loss = 1.977882\n",
      "epoch 311 loss = 1.952889\n",
      "epoch 312 loss = 1.950490\n",
      "epoch 313 loss = 1.984102\n",
      "epoch 314 loss = 1.831052\n",
      "epoch 315 loss = 1.829798\n",
      "epoch 316 loss = 2.028076\n",
      "epoch 317 loss = 1.897554\n",
      "epoch 318 loss = 1.881574\n",
      "epoch 319 loss = 2.045082\n",
      "epoch 320 loss = 1.896278\n",
      "epoch 321 loss = 2.087567\n",
      "epoch 322 loss = 1.992616\n",
      "epoch 323 loss = 1.920168\n",
      "epoch 324 loss = 1.970272\n",
      "epoch 325 loss = 2.171701\n",
      "epoch 326 loss = 1.875624\n",
      "epoch 327 loss = 2.042865\n",
      "epoch 328 loss = 1.943704\n",
      "epoch 329 loss = 2.209270\n",
      "epoch 330 loss = 2.018275\n",
      "epoch 331 loss = 2.039063\n",
      "epoch 332 loss = 2.006662\n",
      "epoch 333 loss = 1.828694\n",
      "epoch 334 loss = 1.853708\n",
      "epoch 335 loss = 1.993453\n",
      "epoch 336 loss = 1.969869\n",
      "epoch 337 loss = 2.007852\n",
      "epoch 338 loss = 2.120298\n",
      "epoch 339 loss = 2.194538\n",
      "epoch 340 loss = 1.948183\n",
      "epoch 341 loss = 1.924808\n",
      "epoch 342 loss = 1.987641\n",
      "epoch 343 loss = 2.080229\n",
      "epoch 344 loss = 2.014690\n",
      "epoch 345 loss = 2.206583\n",
      "epoch 346 loss = 2.053979\n",
      "epoch 347 loss = 2.125891\n",
      "epoch 348 loss = 2.108866\n",
      "epoch 349 loss = 2.015135\n",
      "epoch 350 loss = 2.133116\n",
      "epoch 351 loss = 2.111007\n",
      "epoch 352 loss = 2.032401\n",
      "epoch 353 loss = 2.036743\n",
      "epoch 354 loss = 2.119362\n",
      "epoch 355 loss = 2.010284\n",
      "epoch 356 loss = 2.144356\n",
      "epoch 357 loss = 2.024235\n",
      "epoch 358 loss = 2.096029\n",
      "epoch 359 loss = 2.085607\n",
      "epoch 360 loss = 1.874937\n",
      "epoch 361 loss = 2.052157\n",
      "epoch 362 loss = 1.922086\n",
      "epoch 363 loss = 2.055233\n",
      "epoch 364 loss = 1.934609\n",
      "epoch 365 loss = 1.965211\n",
      "epoch 366 loss = 2.136021\n",
      "epoch 367 loss = 1.992036\n",
      "epoch 368 loss = 1.988737\n",
      "epoch 369 loss = 2.091902\n",
      "epoch 370 loss = 1.954669\n",
      "epoch 371 loss = 1.990936\n",
      "epoch 372 loss = 1.981696\n",
      "epoch 373 loss = 2.040857\n",
      "epoch 374 loss = 1.860193\n",
      "epoch 375 loss = 2.055820\n",
      "epoch 376 loss = 1.892234\n",
      "epoch 377 loss = 2.062796\n",
      "epoch 378 loss = 1.907160\n",
      "epoch 379 loss = 1.903013\n",
      "epoch 380 loss = 1.924762\n",
      "epoch 381 loss = 2.090327\n",
      "epoch 382 loss = 2.132926\n",
      "epoch 383 loss = 1.987646\n",
      "epoch 384 loss = 1.867033\n",
      "epoch 385 loss = 1.992799\n",
      "epoch 386 loss = 2.073643\n",
      "epoch 387 loss = 1.909110\n",
      "epoch 388 loss = 2.130970\n",
      "epoch 389 loss = 2.113104\n",
      "epoch 390 loss = 2.079534\n",
      "epoch 391 loss = 1.873572\n",
      "epoch 392 loss = 2.124980\n",
      "epoch 393 loss = 2.141428\n",
      "epoch 394 loss = 2.039973\n",
      "epoch 395 loss = 1.674715\n",
      "epoch 396 loss = 2.066478\n",
      "epoch 397 loss = 2.135459\n",
      "epoch 398 loss = 2.015496\n",
      "epoch 399 loss = 1.870424\n",
      "epoch 400 loss = 1.930750\n",
      "epoch 401 loss = 1.981108\n",
      "epoch 402 loss = 2.134439\n",
      "epoch 403 loss = 2.167628\n",
      "epoch 404 loss = 1.990272\n",
      "epoch 405 loss = 1.943532\n",
      "epoch 406 loss = 1.929193\n",
      "epoch 407 loss = 2.169622\n",
      "epoch 408 loss = 2.083912\n",
      "epoch 409 loss = 2.028396\n",
      "epoch 410 loss = 1.825644\n",
      "epoch 411 loss = 1.996015\n",
      "epoch 412 loss = 1.943631\n",
      "epoch 413 loss = 1.842828\n",
      "epoch 414 loss = 1.906229\n",
      "epoch 415 loss = 2.186071\n",
      "epoch 416 loss = 1.894204\n",
      "epoch 417 loss = 1.941586\n",
      "epoch 418 loss = 2.173240\n",
      "epoch 419 loss = 1.995923\n",
      "epoch 420 loss = 1.928238\n",
      "epoch 421 loss = 2.128235\n",
      "epoch 422 loss = 1.924387\n",
      "epoch 423 loss = 2.085498\n",
      "epoch 424 loss = 2.081248\n",
      "epoch 425 loss = 1.982886\n",
      "epoch 426 loss = 2.119895\n",
      "epoch 427 loss = 1.884245\n",
      "epoch 428 loss = 2.014372\n",
      "epoch 429 loss = 1.999700\n",
      "epoch 430 loss = 1.981041\n",
      "epoch 431 loss = 2.118629\n",
      "epoch 432 loss = 2.012526\n",
      "epoch 433 loss = 2.023567\n",
      "epoch 434 loss = 2.018391\n",
      "epoch 435 loss = 1.951534\n",
      "epoch 436 loss = 1.864673\n",
      "epoch 437 loss = 1.900267\n",
      "epoch 438 loss = 1.725711\n",
      "epoch 439 loss = 1.931973\n",
      "epoch 440 loss = 1.968210\n",
      "epoch 441 loss = 2.109400\n",
      "epoch 442 loss = 1.882544\n",
      "epoch 443 loss = 2.095839\n",
      "epoch 444 loss = 2.100726\n",
      "epoch 445 loss = 2.012686\n",
      "epoch 446 loss = 2.126728\n",
      "epoch 447 loss = 1.832609\n",
      "epoch 448 loss = 2.135764\n",
      "epoch 449 loss = 1.973537\n",
      "epoch 450 loss = 1.955455\n",
      "epoch 451 loss = 1.923447\n",
      "epoch 452 loss = 1.886771\n",
      "epoch 453 loss = 1.904891\n",
      "epoch 454 loss = 1.959897\n",
      "epoch 455 loss = 2.157526\n",
      "epoch 456 loss = 2.064346\n",
      "epoch 457 loss = 2.289485\n",
      "epoch 458 loss = 1.996792\n",
      "epoch 459 loss = 2.024771\n",
      "epoch 460 loss = 2.082289\n",
      "epoch 461 loss = 2.088725\n",
      "epoch 462 loss = 2.250269\n",
      "epoch 463 loss = 2.029813\n",
      "epoch 464 loss = 1.942522\n",
      "epoch 465 loss = 1.935526\n",
      "epoch 466 loss = 1.910440\n",
      "epoch 467 loss = 1.882610\n",
      "epoch 468 loss = 1.994613\n",
      "epoch 469 loss = 1.911992\n",
      "epoch 470 loss = 1.904698\n",
      "epoch 471 loss = 1.814853\n",
      "epoch 472 loss = 1.908850\n",
      "epoch 473 loss = 2.009158\n",
      "epoch 474 loss = 1.985064\n",
      "epoch 475 loss = 1.785074\n",
      "epoch 476 loss = 1.933341\n",
      "epoch 477 loss = 2.147610\n",
      "epoch 478 loss = 2.065567\n",
      "epoch 479 loss = 1.955767\n",
      "epoch 480 loss = 2.072253\n",
      "epoch 481 loss = 1.870211\n",
      "epoch 482 loss = 2.055187\n",
      "epoch 483 loss = 1.974541\n",
      "epoch 484 loss = 1.989581\n",
      "epoch 485 loss = 1.923396\n",
      "epoch 486 loss = 2.051948\n",
      "epoch 487 loss = 2.075454\n",
      "epoch 488 loss = 1.859374\n",
      "epoch 489 loss = 2.145517\n",
      "epoch 490 loss = 2.091066\n",
      "epoch 491 loss = 1.964725\n",
      "epoch 492 loss = 2.130217\n",
      "epoch 493 loss = 1.973921\n",
      "epoch 494 loss = 2.073251\n",
      "epoch 495 loss = 2.088293\n",
      "epoch 496 loss = 1.894934\n",
      "epoch 497 loss = 2.114240\n",
      "epoch 498 loss = 1.992125\n",
      "epoch 499 loss = 1.874314\n",
      "final loss = 1.874314\n",
      "accuracy_mc = tensor(0.3208, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.3135, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9471, device='cuda:0')\n",
      "training time = 172.68258213996887 seconds\n",
      "testing time = 1.8960974216461182 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = 2.286351\n",
      "epoch 1 loss = 2.328704\n",
      "epoch 2 loss = 2.330649\n",
      "epoch 3 loss = 2.294309\n",
      "epoch 4 loss = 2.357220\n",
      "epoch 5 loss = 2.182778\n",
      "epoch 6 loss = 2.256249\n",
      "epoch 7 loss = 2.118839\n",
      "epoch 8 loss = 2.294279\n",
      "epoch 9 loss = 2.275580\n",
      "epoch 10 loss = 2.187303\n",
      "epoch 11 loss = 2.245476\n",
      "epoch 12 loss = 2.208181\n",
      "epoch 13 loss = 2.178308\n",
      "epoch 14 loss = 2.136378\n",
      "epoch 15 loss = 2.184169\n",
      "epoch 16 loss = 2.222964\n",
      "epoch 17 loss = 2.214023\n",
      "epoch 18 loss = 2.188903\n",
      "epoch 19 loss = 2.196404\n",
      "epoch 20 loss = 2.027304\n",
      "epoch 21 loss = 2.219725\n",
      "epoch 22 loss = 2.167121\n",
      "epoch 23 loss = 2.157179\n",
      "epoch 24 loss = 2.095643\n",
      "epoch 25 loss = 2.207188\n",
      "epoch 26 loss = 2.196418\n",
      "epoch 27 loss = 2.114417\n",
      "epoch 28 loss = 2.097255\n",
      "epoch 29 loss = 2.162056\n",
      "epoch 30 loss = 2.113858\n",
      "epoch 31 loss = 2.189355\n",
      "epoch 32 loss = 2.183038\n",
      "epoch 33 loss = 2.077760\n",
      "epoch 34 loss = 2.143745\n",
      "epoch 35 loss = 2.151301\n",
      "epoch 36 loss = 2.198653\n",
      "epoch 37 loss = 2.121330\n",
      "epoch 38 loss = 2.167451\n",
      "epoch 39 loss = 2.101101\n",
      "epoch 40 loss = 2.168787\n",
      "epoch 41 loss = 2.264277\n",
      "epoch 42 loss = 2.066299\n",
      "epoch 43 loss = 2.149757\n",
      "epoch 44 loss = 2.156869\n",
      "epoch 45 loss = 2.121817\n",
      "epoch 46 loss = 2.253546\n",
      "epoch 47 loss = 2.027264\n",
      "epoch 48 loss = 2.115813\n",
      "epoch 49 loss = 2.118600\n",
      "epoch 50 loss = 2.246032\n",
      "epoch 51 loss = 2.041285\n",
      "epoch 52 loss = 2.171350\n",
      "epoch 53 loss = 2.202539\n",
      "epoch 54 loss = 2.103283\n",
      "epoch 55 loss = 2.214796\n",
      "epoch 56 loss = 2.194655\n",
      "epoch 57 loss = 2.133060\n",
      "epoch 58 loss = 2.192631\n",
      "epoch 59 loss = 2.195050\n",
      "epoch 60 loss = 2.069522\n",
      "epoch 61 loss = 2.146485\n",
      "epoch 62 loss = 2.075882\n",
      "epoch 63 loss = 2.166212\n",
      "epoch 64 loss = 2.106893\n",
      "epoch 65 loss = 2.111473\n",
      "epoch 66 loss = 2.134103\n",
      "epoch 67 loss = 2.346474\n",
      "epoch 68 loss = 2.051775\n",
      "epoch 69 loss = 2.100029\n",
      "epoch 70 loss = 2.263736\n",
      "epoch 71 loss = 2.241667\n",
      "epoch 72 loss = 2.200828\n",
      "epoch 73 loss = 2.013971\n",
      "epoch 74 loss = 2.162688\n",
      "epoch 75 loss = 2.074232\n",
      "epoch 76 loss = 2.085662\n",
      "epoch 77 loss = 2.055196\n",
      "epoch 78 loss = 2.095854\n",
      "epoch 79 loss = 1.951842\n",
      "epoch 80 loss = 2.180262\n",
      "epoch 81 loss = 2.123714\n",
      "epoch 82 loss = 2.276865\n",
      "epoch 83 loss = 2.186794\n",
      "epoch 84 loss = 2.174366\n",
      "epoch 85 loss = 2.115959\n",
      "epoch 86 loss = 2.141007\n",
      "epoch 87 loss = 2.083096\n",
      "epoch 88 loss = 2.164247\n",
      "epoch 89 loss = 2.076727\n",
      "epoch 90 loss = 2.187394\n",
      "epoch 91 loss = 2.145218\n",
      "epoch 92 loss = 2.065624\n",
      "epoch 93 loss = 2.078288\n",
      "epoch 94 loss = 2.135673\n",
      "epoch 95 loss = 2.054549\n",
      "epoch 96 loss = 2.170719\n",
      "epoch 97 loss = 2.123845\n",
      "epoch 98 loss = 2.061747\n",
      "epoch 99 loss = 2.143910\n",
      "epoch 100 loss = 2.230979\n",
      "epoch 101 loss = 2.162757\n",
      "epoch 102 loss = 2.133071\n",
      "epoch 103 loss = 2.047586\n",
      "epoch 104 loss = 2.072652\n",
      "epoch 105 loss = 2.145800\n",
      "epoch 106 loss = 2.145470\n",
      "epoch 107 loss = 2.043131\n",
      "epoch 108 loss = 2.252584\n",
      "epoch 109 loss = 2.216381\n",
      "epoch 110 loss = 2.103558\n",
      "epoch 111 loss = 2.130171\n",
      "epoch 112 loss = 2.096192\n",
      "epoch 113 loss = 2.315813\n",
      "epoch 114 loss = 2.089645\n",
      "epoch 115 loss = 2.139615\n",
      "epoch 116 loss = 2.087727\n",
      "epoch 117 loss = 2.186753\n",
      "epoch 118 loss = 2.128947\n",
      "epoch 119 loss = 2.112076\n",
      "epoch 120 loss = 2.063576\n",
      "epoch 121 loss = 2.173097\n",
      "epoch 122 loss = 2.180991\n",
      "epoch 123 loss = 2.177651\n",
      "epoch 124 loss = 2.102867\n",
      "epoch 125 loss = 1.994445\n",
      "epoch 126 loss = 2.047851\n",
      "epoch 127 loss = 2.343708\n",
      "epoch 128 loss = 2.131567\n",
      "epoch 129 loss = 2.116363\n",
      "epoch 130 loss = 2.108447\n",
      "epoch 131 loss = 2.061835\n",
      "epoch 132 loss = 2.094666\n",
      "epoch 133 loss = 1.965427\n",
      "epoch 134 loss = 2.218394\n",
      "epoch 135 loss = 2.151040\n",
      "epoch 136 loss = 2.175391\n",
      "epoch 137 loss = 2.200158\n",
      "epoch 138 loss = 2.026406\n",
      "epoch 139 loss = 2.162402\n",
      "epoch 140 loss = 2.176741\n",
      "epoch 141 loss = 2.167533\n",
      "epoch 142 loss = 2.026209\n",
      "epoch 143 loss = 2.198135\n",
      "epoch 144 loss = 2.084096\n",
      "epoch 145 loss = 2.043813\n",
      "epoch 146 loss = 2.042854\n",
      "epoch 147 loss = 2.138518\n",
      "epoch 148 loss = 2.122766\n",
      "epoch 149 loss = 2.047436\n",
      "epoch 150 loss = 2.217010\n",
      "epoch 151 loss = 2.212838\n",
      "epoch 152 loss = 2.047843\n",
      "epoch 153 loss = 2.306311\n",
      "epoch 154 loss = 2.147672\n",
      "epoch 155 loss = 2.090639\n",
      "epoch 156 loss = 2.185019\n",
      "epoch 157 loss = 2.180341\n",
      "epoch 158 loss = 2.093427\n",
      "epoch 159 loss = 2.116426\n",
      "epoch 160 loss = 2.152435\n",
      "epoch 161 loss = 2.071165\n",
      "epoch 162 loss = 2.068734\n",
      "epoch 163 loss = 2.129923\n",
      "epoch 164 loss = 2.119568\n",
      "epoch 165 loss = 2.179183\n",
      "epoch 166 loss = 2.171820\n",
      "epoch 167 loss = 2.085661\n",
      "epoch 168 loss = 2.114987\n",
      "epoch 169 loss = 2.037872\n",
      "epoch 170 loss = 2.122829\n",
      "epoch 171 loss = 2.216984\n",
      "epoch 172 loss = 2.086245\n",
      "epoch 173 loss = 2.150595\n",
      "epoch 174 loss = 2.165487\n",
      "epoch 175 loss = 2.124058\n",
      "epoch 176 loss = 2.165618\n",
      "epoch 177 loss = 2.135901\n",
      "epoch 178 loss = 2.166702\n",
      "epoch 179 loss = 2.164815\n",
      "epoch 180 loss = 2.115841\n",
      "epoch 181 loss = 2.204098\n",
      "epoch 182 loss = 2.014508\n",
      "epoch 183 loss = 2.127950\n",
      "epoch 184 loss = 2.096484\n",
      "epoch 185 loss = 2.287423\n",
      "epoch 186 loss = 1.985798\n",
      "epoch 187 loss = 2.133518\n",
      "epoch 188 loss = 2.089493\n",
      "epoch 189 loss = 2.095122\n",
      "epoch 190 loss = 2.088179\n",
      "epoch 191 loss = 2.040424\n",
      "epoch 192 loss = 2.107598\n",
      "epoch 193 loss = 2.134612\n",
      "epoch 194 loss = 2.170793\n",
      "epoch 195 loss = 2.183553\n",
      "epoch 196 loss = 2.085546\n",
      "epoch 197 loss = 2.107184\n",
      "epoch 198 loss = 2.182130\n",
      "epoch 199 loss = 2.120445\n",
      "epoch 200 loss = 2.045440\n",
      "epoch 201 loss = 2.125836\n",
      "epoch 202 loss = 2.194560\n",
      "epoch 203 loss = 2.123424\n",
      "epoch 204 loss = 2.074120\n",
      "epoch 205 loss = 2.148245\n",
      "epoch 206 loss = 2.193312\n",
      "epoch 207 loss = 2.096420\n",
      "epoch 208 loss = 1.994397\n",
      "epoch 209 loss = 1.987985\n",
      "epoch 210 loss = 2.082626\n",
      "epoch 211 loss = 2.082331\n",
      "epoch 212 loss = 2.121107\n",
      "epoch 213 loss = 2.049198\n",
      "epoch 214 loss = 1.987902\n",
      "epoch 215 loss = 2.141499\n",
      "epoch 216 loss = 2.045445\n",
      "epoch 217 loss = 2.183022\n",
      "epoch 218 loss = 2.177161\n",
      "epoch 219 loss = 2.082109\n",
      "epoch 220 loss = 2.143333\n",
      "epoch 221 loss = 2.044219\n",
      "epoch 222 loss = 2.289623\n",
      "epoch 223 loss = 2.118864\n",
      "epoch 224 loss = 2.105451\n",
      "epoch 225 loss = 2.076369\n",
      "epoch 226 loss = 2.125113\n",
      "epoch 227 loss = 2.128816\n",
      "epoch 228 loss = 2.220477\n",
      "epoch 229 loss = 2.196653\n",
      "epoch 230 loss = 2.176439\n",
      "epoch 231 loss = 2.072427\n",
      "epoch 232 loss = 2.163311\n",
      "epoch 233 loss = 2.064363\n",
      "epoch 234 loss = 2.148043\n",
      "epoch 235 loss = 1.938127\n",
      "epoch 236 loss = 2.096594\n",
      "epoch 237 loss = 2.202703\n",
      "epoch 238 loss = 2.088830\n",
      "epoch 239 loss = 2.073498\n",
      "epoch 240 loss = 2.107241\n",
      "epoch 241 loss = 1.889965\n",
      "epoch 242 loss = 2.152112\n",
      "epoch 243 loss = 2.146918\n",
      "epoch 244 loss = 2.099838\n",
      "epoch 245 loss = 2.169445\n",
      "epoch 246 loss = 2.267076\n",
      "epoch 247 loss = 2.202085\n",
      "epoch 248 loss = 2.182510\n",
      "epoch 249 loss = 2.115511\n",
      "epoch 250 loss = 2.176826\n",
      "epoch 251 loss = 2.138062\n",
      "epoch 252 loss = 2.144421\n",
      "epoch 253 loss = 2.152785\n",
      "epoch 254 loss = 2.203528\n",
      "epoch 255 loss = 2.118843\n",
      "epoch 256 loss = 2.231446\n",
      "epoch 257 loss = 2.131432\n",
      "epoch 258 loss = 2.134447\n",
      "epoch 259 loss = 2.168884\n",
      "epoch 260 loss = 2.126129\n",
      "epoch 261 loss = 2.171172\n",
      "epoch 262 loss = 2.229315\n",
      "epoch 263 loss = 2.091890\n",
      "epoch 264 loss = 2.229122\n",
      "epoch 265 loss = 2.063065\n",
      "epoch 266 loss = 2.156320\n",
      "epoch 267 loss = 2.177062\n",
      "epoch 268 loss = 2.207102\n",
      "epoch 269 loss = 2.110126\n",
      "epoch 270 loss = 2.200392\n",
      "epoch 271 loss = 2.102454\n",
      "epoch 272 loss = 2.106274\n",
      "epoch 273 loss = 2.126316\n",
      "epoch 274 loss = 2.161752\n",
      "epoch 275 loss = 2.203461\n",
      "epoch 276 loss = 2.103643\n",
      "epoch 277 loss = 2.080711\n",
      "epoch 278 loss = 2.038569\n",
      "epoch 279 loss = 2.110088\n",
      "epoch 280 loss = 2.044947\n",
      "epoch 281 loss = 2.069722\n",
      "epoch 282 loss = 2.149183\n",
      "epoch 283 loss = 2.208287\n",
      "epoch 284 loss = 2.106221\n",
      "epoch 285 loss = 2.160918\n",
      "epoch 286 loss = 2.133231\n",
      "epoch 287 loss = 2.101109\n",
      "epoch 288 loss = 2.208656\n",
      "epoch 289 loss = 2.163439\n",
      "epoch 290 loss = 2.149644\n",
      "epoch 291 loss = 2.079528\n",
      "epoch 292 loss = 2.087461\n",
      "epoch 293 loss = 2.135824\n",
      "epoch 294 loss = 2.079340\n",
      "epoch 295 loss = 2.265252\n",
      "epoch 296 loss = 2.092908\n",
      "epoch 297 loss = 2.055758\n",
      "epoch 298 loss = 2.101850\n",
      "epoch 299 loss = 1.997247\n",
      "epoch 300 loss = 1.986820\n",
      "epoch 301 loss = 2.081548\n",
      "epoch 302 loss = 2.166803\n",
      "epoch 303 loss = 2.129056\n",
      "epoch 304 loss = 2.028026\n",
      "epoch 305 loss = 2.199196\n",
      "epoch 306 loss = 2.101144\n",
      "epoch 307 loss = 1.921634\n",
      "epoch 308 loss = 2.155554\n",
      "epoch 309 loss = 2.176910\n",
      "epoch 310 loss = 2.207748\n",
      "epoch 311 loss = 2.234627\n",
      "epoch 312 loss = 2.144723\n",
      "epoch 313 loss = 2.137966\n",
      "epoch 314 loss = 2.064858\n",
      "epoch 315 loss = 2.096752\n",
      "epoch 316 loss = 2.133604\n",
      "epoch 317 loss = 2.110093\n",
      "epoch 318 loss = 2.126309\n",
      "epoch 319 loss = 2.121253\n",
      "epoch 320 loss = 2.077387\n",
      "epoch 321 loss = 2.165825\n",
      "epoch 322 loss = 2.023182\n",
      "epoch 323 loss = 2.213287\n",
      "epoch 324 loss = 1.995837\n",
      "epoch 325 loss = 2.109972\n",
      "epoch 326 loss = 2.107167\n",
      "epoch 327 loss = 2.012584\n",
      "epoch 328 loss = 2.180357\n",
      "epoch 329 loss = 1.959776\n",
      "epoch 330 loss = 2.153763\n",
      "epoch 331 loss = 2.240745\n",
      "epoch 332 loss = 2.161121\n",
      "epoch 333 loss = 1.955978\n",
      "epoch 334 loss = 2.145612\n",
      "epoch 335 loss = 2.198493\n",
      "epoch 336 loss = 1.991444\n",
      "epoch 337 loss = 2.192508\n",
      "epoch 338 loss = 2.024798\n",
      "epoch 339 loss = 2.169771\n",
      "epoch 340 loss = 2.071057\n",
      "epoch 341 loss = 2.214442\n",
      "epoch 342 loss = 2.111038\n",
      "epoch 343 loss = 2.223289\n",
      "epoch 344 loss = 2.195595\n",
      "epoch 345 loss = 2.062620\n",
      "epoch 346 loss = 2.124849\n",
      "epoch 347 loss = 2.094542\n",
      "epoch 348 loss = 2.159611\n",
      "epoch 349 loss = 2.180830\n",
      "epoch 350 loss = 1.820588\n",
      "epoch 351 loss = 2.068903\n",
      "epoch 352 loss = 2.230042\n",
      "epoch 353 loss = 2.102570\n",
      "epoch 354 loss = 2.083382\n",
      "epoch 355 loss = 2.115258\n",
      "epoch 356 loss = 2.051641\n",
      "epoch 357 loss = 2.163455\n",
      "epoch 358 loss = 2.074345\n",
      "epoch 359 loss = 2.008690\n",
      "epoch 360 loss = 1.951263\n",
      "epoch 361 loss = 2.028746\n",
      "epoch 362 loss = 2.114282\n",
      "epoch 363 loss = 1.993239\n",
      "epoch 364 loss = 2.062813\n",
      "epoch 365 loss = 2.109268\n",
      "epoch 366 loss = 2.144457\n",
      "epoch 367 loss = 2.184898\n",
      "epoch 368 loss = 2.161770\n",
      "epoch 369 loss = 2.131378\n",
      "epoch 370 loss = 2.008557\n",
      "epoch 371 loss = 2.099163\n",
      "epoch 372 loss = 2.237974\n",
      "epoch 373 loss = 2.271901\n",
      "epoch 374 loss = 2.136983\n",
      "epoch 375 loss = 1.976793\n",
      "epoch 376 loss = 2.121220\n",
      "epoch 377 loss = 2.097700\n",
      "epoch 378 loss = 2.049034\n",
      "epoch 379 loss = 2.255544\n",
      "epoch 380 loss = 2.082989\n",
      "epoch 381 loss = 2.091830\n",
      "epoch 382 loss = 2.067921\n",
      "epoch 383 loss = 2.116300\n",
      "epoch 384 loss = 2.064061\n",
      "epoch 385 loss = 2.092207\n",
      "epoch 386 loss = 2.255774\n",
      "epoch 387 loss = 2.130368\n",
      "epoch 388 loss = 2.209529\n",
      "epoch 389 loss = 2.069029\n",
      "epoch 390 loss = 2.114430\n",
      "epoch 391 loss = 2.103736\n",
      "epoch 392 loss = 2.156928\n",
      "epoch 393 loss = 2.076658\n",
      "epoch 394 loss = 2.209058\n",
      "epoch 395 loss = 2.218626\n",
      "epoch 396 loss = 2.062083\n",
      "epoch 397 loss = 2.140847\n",
      "epoch 398 loss = 2.105337\n",
      "epoch 399 loss = 2.023912\n",
      "epoch 400 loss = 1.959474\n",
      "epoch 401 loss = 2.032883\n",
      "epoch 402 loss = 2.283633\n",
      "epoch 403 loss = 1.992954\n",
      "epoch 404 loss = 2.159846\n",
      "epoch 405 loss = 2.146808\n",
      "epoch 406 loss = 2.173214\n",
      "epoch 407 loss = 2.090743\n",
      "epoch 408 loss = 2.192169\n",
      "epoch 409 loss = 2.117664\n",
      "epoch 410 loss = 2.154166\n",
      "epoch 411 loss = 2.080884\n",
      "epoch 412 loss = 2.086001\n",
      "epoch 413 loss = 2.186830\n",
      "epoch 414 loss = 2.179135\n",
      "epoch 415 loss = 2.203325\n",
      "epoch 416 loss = 2.019880\n",
      "epoch 417 loss = 2.055374\n",
      "epoch 418 loss = 2.172104\n",
      "epoch 419 loss = 2.212886\n",
      "epoch 420 loss = 2.028324\n",
      "epoch 421 loss = 2.265750\n",
      "epoch 422 loss = 2.189842\n",
      "epoch 423 loss = 2.012932\n",
      "epoch 424 loss = 2.165288\n",
      "epoch 425 loss = 2.148178\n",
      "epoch 426 loss = 2.166303\n",
      "epoch 427 loss = 2.089345\n",
      "epoch 428 loss = 2.213051\n",
      "epoch 429 loss = 2.113051\n",
      "epoch 430 loss = 2.133783\n",
      "epoch 431 loss = 2.021794\n",
      "epoch 432 loss = 2.106794\n",
      "epoch 433 loss = 2.124059\n",
      "epoch 434 loss = 2.136085\n",
      "epoch 435 loss = 2.233376\n",
      "epoch 436 loss = 2.112604\n",
      "epoch 437 loss = 2.031664\n",
      "epoch 438 loss = 2.040460\n",
      "epoch 439 loss = 2.109418\n",
      "epoch 440 loss = 2.115042\n",
      "epoch 441 loss = 2.200324\n",
      "epoch 442 loss = 2.008980\n",
      "epoch 443 loss = 2.155517\n",
      "epoch 444 loss = 2.107180\n",
      "epoch 445 loss = 2.282904\n",
      "epoch 446 loss = 2.112959\n",
      "epoch 447 loss = 2.147890\n",
      "epoch 448 loss = 2.205546\n",
      "epoch 449 loss = 2.018931\n",
      "epoch 450 loss = 2.187147\n",
      "epoch 451 loss = 2.098789\n",
      "epoch 452 loss = 2.175047\n",
      "epoch 453 loss = 2.128720\n",
      "epoch 454 loss = 2.092576\n",
      "epoch 455 loss = 2.178792\n",
      "epoch 456 loss = 2.172091\n",
      "epoch 457 loss = 2.053530\n",
      "epoch 458 loss = 2.037648\n",
      "epoch 459 loss = 2.134079\n",
      "epoch 460 loss = 2.243316\n",
      "epoch 461 loss = 2.148756\n",
      "epoch 462 loss = 2.114741\n",
      "epoch 463 loss = 2.157641\n",
      "epoch 464 loss = 2.154855\n",
      "epoch 465 loss = 2.030492\n",
      "epoch 466 loss = 2.077123\n",
      "epoch 467 loss = 2.069515\n",
      "epoch 468 loss = 2.004304\n",
      "epoch 469 loss = 2.255367\n",
      "epoch 470 loss = 2.092691\n",
      "epoch 471 loss = 2.113493\n",
      "epoch 472 loss = 2.021476\n",
      "epoch 473 loss = 2.106857\n",
      "epoch 474 loss = 2.039446\n",
      "epoch 475 loss = 2.134275\n",
      "epoch 476 loss = 1.971924\n",
      "epoch 477 loss = 2.057696\n",
      "epoch 478 loss = 2.047129\n",
      "epoch 479 loss = 2.058789\n",
      "epoch 480 loss = 2.149831\n",
      "epoch 481 loss = 2.040276\n",
      "epoch 482 loss = 2.215111\n",
      "epoch 483 loss = 2.190399\n",
      "epoch 484 loss = 1.964987\n",
      "epoch 485 loss = 2.074345\n",
      "epoch 486 loss = 2.166605\n",
      "epoch 487 loss = 2.095398\n",
      "epoch 488 loss = 2.180788\n",
      "epoch 489 loss = 2.209593\n",
      "epoch 490 loss = 2.228398\n",
      "epoch 491 loss = 2.085461\n",
      "epoch 492 loss = 2.105857\n",
      "epoch 493 loss = 1.938951\n",
      "epoch 494 loss = 2.133346\n",
      "epoch 495 loss = 2.051917\n",
      "epoch 496 loss = 2.198529\n",
      "epoch 497 loss = 2.188395\n",
      "epoch 498 loss = 1.939367\n",
      "epoch 499 loss = 2.157851\n",
      "final loss = 2.157851\n",
      "accuracy_mc = tensor(0.2531, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2115, device='cuda:0')\n",
      "test_ll_mc = tensor(-2.0502, device='cuda:0')\n",
      "training time = 171.72358179092407 seconds\n",
      "testing time = 1.9084346294403076 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = 2.338722\n",
      "epoch 1 loss = 2.286467\n",
      "epoch 2 loss = 2.271491\n",
      "epoch 3 loss = 2.296510\n",
      "epoch 4 loss = 2.261904\n",
      "epoch 5 loss = 2.258931\n",
      "epoch 6 loss = 2.288520\n",
      "epoch 7 loss = 2.230298\n",
      "epoch 8 loss = 2.218976\n",
      "epoch 9 loss = 2.244329\n",
      "epoch 10 loss = 2.262226\n",
      "epoch 11 loss = 2.205820\n",
      "epoch 12 loss = 2.097557\n",
      "epoch 13 loss = 2.314126\n",
      "epoch 14 loss = 2.194479\n",
      "epoch 15 loss = 2.171667\n",
      "epoch 16 loss = 2.090651\n",
      "epoch 17 loss = 2.274701\n",
      "epoch 18 loss = 2.202659\n",
      "epoch 19 loss = 2.198570\n",
      "epoch 20 loss = 2.169060\n",
      "epoch 21 loss = 2.224641\n",
      "epoch 22 loss = 2.132071\n",
      "epoch 23 loss = 2.105802\n",
      "epoch 24 loss = 2.230082\n",
      "epoch 25 loss = 2.189591\n",
      "epoch 26 loss = 2.198484\n",
      "epoch 27 loss = 2.244988\n",
      "epoch 28 loss = 2.156353\n",
      "epoch 29 loss = 2.244122\n",
      "epoch 30 loss = 2.163983\n",
      "epoch 31 loss = 2.157937\n",
      "epoch 32 loss = 2.231780\n",
      "epoch 33 loss = 2.175063\n",
      "epoch 34 loss = 2.172698\n",
      "epoch 35 loss = 2.012048\n",
      "epoch 36 loss = 2.126866\n",
      "epoch 37 loss = 2.058527\n",
      "epoch 38 loss = 2.097830\n",
      "epoch 39 loss = 2.118896\n",
      "epoch 40 loss = 2.312839\n",
      "epoch 41 loss = 2.232174\n",
      "epoch 42 loss = 2.153488\n",
      "epoch 43 loss = 2.297237\n",
      "epoch 44 loss = 2.002451\n",
      "epoch 45 loss = 2.067084\n",
      "epoch 46 loss = 2.076773\n",
      "epoch 47 loss = 2.166137\n",
      "epoch 48 loss = 2.126554\n",
      "epoch 49 loss = 2.173026\n",
      "epoch 50 loss = 2.142031\n",
      "epoch 51 loss = 2.142567\n",
      "epoch 52 loss = 2.175200\n",
      "epoch 53 loss = 2.110418\n",
      "epoch 54 loss = 2.099830\n",
      "epoch 55 loss = 2.414108\n",
      "epoch 56 loss = 2.041252\n",
      "epoch 57 loss = 2.065914\n",
      "epoch 58 loss = 2.160983\n",
      "epoch 59 loss = 2.019312\n",
      "epoch 60 loss = 2.193597\n",
      "epoch 61 loss = 1.932323\n",
      "epoch 62 loss = 2.008578\n",
      "epoch 63 loss = 2.088398\n",
      "epoch 64 loss = 2.043523\n",
      "epoch 65 loss = 2.117049\n",
      "epoch 66 loss = 2.172211\n",
      "epoch 67 loss = 2.134533\n",
      "epoch 68 loss = 2.123475\n",
      "epoch 69 loss = 2.197052\n",
      "epoch 70 loss = 2.039424\n",
      "epoch 71 loss = 2.114536\n",
      "epoch 72 loss = 2.149663\n",
      "epoch 73 loss = 2.052434\n",
      "epoch 74 loss = 2.117585\n",
      "epoch 75 loss = 2.099701\n",
      "epoch 76 loss = 2.228766\n",
      "epoch 77 loss = 2.032082\n",
      "epoch 78 loss = 1.990361\n",
      "epoch 79 loss = 2.166912\n",
      "epoch 80 loss = 2.064801\n",
      "epoch 81 loss = 2.052746\n",
      "epoch 82 loss = 2.023968\n",
      "epoch 83 loss = 2.063474\n",
      "epoch 84 loss = 2.215154\n",
      "epoch 85 loss = 2.106637\n",
      "epoch 86 loss = 2.272006\n",
      "epoch 87 loss = 2.205375\n",
      "epoch 88 loss = 2.086999\n",
      "epoch 89 loss = 2.007252\n",
      "epoch 90 loss = 2.029831\n",
      "epoch 91 loss = 2.051996\n",
      "epoch 92 loss = 2.401005\n",
      "epoch 93 loss = 1.957150\n",
      "epoch 94 loss = 2.020832\n",
      "epoch 95 loss = 2.167516\n",
      "epoch 96 loss = 2.107563\n",
      "epoch 97 loss = 1.916928\n",
      "epoch 98 loss = 2.193521\n",
      "epoch 99 loss = 2.113132\n",
      "epoch 100 loss = 2.170271\n",
      "epoch 101 loss = 2.149472\n",
      "epoch 102 loss = 2.168079\n",
      "epoch 103 loss = 2.158595\n",
      "epoch 104 loss = 1.940893\n",
      "epoch 105 loss = 1.931011\n",
      "epoch 106 loss = 2.077510\n",
      "epoch 107 loss = 2.180977\n",
      "epoch 108 loss = 2.110087\n",
      "epoch 109 loss = 2.097418\n",
      "epoch 110 loss = 1.922601\n",
      "epoch 111 loss = 2.057203\n",
      "epoch 112 loss = 2.177342\n",
      "epoch 113 loss = 2.078573\n",
      "epoch 114 loss = 1.879271\n",
      "epoch 115 loss = 2.031914\n",
      "epoch 116 loss = 2.059349\n",
      "epoch 117 loss = 2.108086\n",
      "epoch 118 loss = 2.112088\n",
      "epoch 119 loss = 1.961103\n",
      "epoch 120 loss = 1.984845\n",
      "epoch 121 loss = 1.941759\n",
      "epoch 122 loss = 2.138164\n",
      "epoch 123 loss = 2.081215\n",
      "epoch 124 loss = 2.187761\n",
      "epoch 125 loss = 1.981363\n",
      "epoch 126 loss = 2.163884\n",
      "epoch 127 loss = 2.255405\n",
      "epoch 128 loss = 1.803270\n",
      "epoch 129 loss = 2.076544\n",
      "epoch 130 loss = 2.134451\n",
      "epoch 131 loss = 1.968813\n",
      "epoch 132 loss = 2.063253\n",
      "epoch 133 loss = 1.849073\n",
      "epoch 134 loss = 2.103600\n",
      "epoch 135 loss = 2.115447\n",
      "epoch 136 loss = 1.995471\n",
      "epoch 137 loss = 2.071326\n",
      "epoch 138 loss = 2.042024\n",
      "epoch 139 loss = 2.011418\n",
      "epoch 140 loss = 2.142808\n",
      "epoch 141 loss = 2.273628\n",
      "epoch 142 loss = 2.085810\n",
      "epoch 143 loss = 2.096868\n",
      "epoch 144 loss = 1.992546\n",
      "epoch 145 loss = 2.217693\n",
      "epoch 146 loss = 2.059373\n",
      "epoch 147 loss = 2.042138\n",
      "epoch 148 loss = 2.124536\n",
      "epoch 149 loss = 2.070818\n",
      "epoch 150 loss = 2.103698\n",
      "epoch 151 loss = 2.201762\n",
      "epoch 152 loss = 1.859687\n",
      "epoch 153 loss = 2.081398\n",
      "epoch 154 loss = 2.090282\n",
      "epoch 155 loss = 2.109141\n",
      "epoch 156 loss = 1.962145\n",
      "epoch 157 loss = 1.896656\n",
      "epoch 158 loss = 2.073731\n",
      "epoch 159 loss = 1.899973\n",
      "epoch 160 loss = 2.012871\n",
      "epoch 161 loss = 2.179812\n",
      "epoch 162 loss = 2.023040\n",
      "epoch 163 loss = 1.995377\n",
      "epoch 164 loss = 1.973619\n",
      "epoch 165 loss = 1.961456\n",
      "epoch 166 loss = 2.001911\n",
      "epoch 167 loss = 2.195910\n",
      "epoch 168 loss = 2.127156\n",
      "epoch 169 loss = 1.916532\n",
      "epoch 170 loss = 2.006337\n",
      "epoch 171 loss = 2.011565\n",
      "epoch 172 loss = 1.749501\n",
      "epoch 173 loss = 2.081052\n",
      "epoch 174 loss = 2.008987\n",
      "epoch 175 loss = 2.054058\n",
      "epoch 176 loss = 2.049938\n",
      "epoch 177 loss = 2.154309\n",
      "epoch 178 loss = 1.938650\n",
      "epoch 179 loss = 2.204750\n",
      "epoch 180 loss = 1.923806\n",
      "epoch 181 loss = 1.797958\n",
      "epoch 182 loss = 1.971254\n",
      "epoch 183 loss = 2.046554\n",
      "epoch 184 loss = 2.025113\n",
      "epoch 185 loss = 1.932467\n",
      "epoch 186 loss = 1.889811\n",
      "epoch 187 loss = 1.957620\n",
      "epoch 188 loss = 2.183458\n",
      "epoch 189 loss = 2.010732\n",
      "epoch 190 loss = 2.107722\n",
      "epoch 191 loss = 2.030300\n",
      "epoch 192 loss = 1.879543\n",
      "epoch 193 loss = 2.158679\n",
      "epoch 194 loss = 1.875727\n",
      "epoch 195 loss = 2.163234\n",
      "epoch 196 loss = 1.940327\n",
      "epoch 197 loss = 1.879972\n",
      "epoch 198 loss = 2.170197\n",
      "epoch 199 loss = 1.992295\n",
      "epoch 200 loss = 2.087396\n",
      "epoch 201 loss = 2.051579\n",
      "epoch 202 loss = 1.913154\n",
      "epoch 203 loss = 1.734493\n",
      "epoch 204 loss = 1.812186\n",
      "epoch 205 loss = 2.066153\n",
      "epoch 206 loss = 1.962640\n",
      "epoch 207 loss = 1.971250\n",
      "epoch 208 loss = 1.914259\n",
      "epoch 209 loss = 2.043174\n",
      "epoch 210 loss = 2.051528\n",
      "epoch 211 loss = 1.724020\n",
      "epoch 212 loss = 1.792214\n",
      "epoch 213 loss = 2.057677\n",
      "epoch 214 loss = 2.042860\n",
      "epoch 215 loss = 2.057415\n",
      "epoch 216 loss = 1.927324\n",
      "epoch 217 loss = 1.937785\n",
      "epoch 218 loss = 1.961375\n",
      "epoch 219 loss = 2.107490\n",
      "epoch 220 loss = 1.940688\n",
      "epoch 221 loss = 2.009182\n",
      "epoch 222 loss = 2.100915\n",
      "epoch 223 loss = 1.985012\n",
      "epoch 224 loss = 1.981260\n",
      "epoch 225 loss = 2.026292\n",
      "epoch 226 loss = 1.981526\n",
      "epoch 227 loss = 1.834781\n",
      "epoch 228 loss = 2.133064\n",
      "epoch 229 loss = 2.090405\n",
      "epoch 230 loss = 2.050989\n",
      "epoch 231 loss = 2.186900\n",
      "epoch 232 loss = 1.877845\n",
      "epoch 233 loss = 1.943184\n",
      "epoch 234 loss = 1.685884\n",
      "epoch 235 loss = 1.850398\n",
      "epoch 236 loss = 1.953115\n",
      "epoch 237 loss = 1.954522\n",
      "epoch 238 loss = 1.990649\n",
      "epoch 239 loss = 2.142695\n",
      "epoch 240 loss = 1.944767\n",
      "epoch 241 loss = 2.066602\n",
      "epoch 242 loss = 2.101925\n",
      "epoch 243 loss = 1.998535\n",
      "epoch 244 loss = 2.091150\n",
      "epoch 245 loss = 1.971152\n",
      "epoch 246 loss = 2.048855\n",
      "epoch 247 loss = 1.981476\n",
      "epoch 248 loss = 2.103539\n",
      "epoch 249 loss = 2.043016\n",
      "epoch 250 loss = 2.097652\n",
      "epoch 251 loss = 1.956831\n",
      "epoch 252 loss = 1.994039\n",
      "epoch 253 loss = 2.047606\n",
      "epoch 254 loss = 2.031493\n",
      "epoch 255 loss = 2.088933\n",
      "epoch 256 loss = 2.033150\n",
      "epoch 257 loss = 1.975027\n",
      "epoch 258 loss = 1.749074\n",
      "epoch 259 loss = 2.119098\n",
      "epoch 260 loss = 1.977829\n",
      "epoch 261 loss = 2.163229\n",
      "epoch 262 loss = 1.960716\n",
      "epoch 263 loss = 2.139197\n",
      "epoch 264 loss = 1.815495\n",
      "epoch 265 loss = 1.966598\n",
      "epoch 266 loss = 1.882910\n",
      "epoch 267 loss = 2.131592\n",
      "epoch 268 loss = 2.031412\n",
      "epoch 269 loss = 2.177013\n",
      "epoch 270 loss = 1.983762\n",
      "epoch 271 loss = 1.991630\n",
      "epoch 272 loss = 1.949196\n",
      "epoch 273 loss = 1.914849\n",
      "epoch 274 loss = 1.918069\n",
      "epoch 275 loss = 2.005367\n",
      "epoch 276 loss = 1.911552\n",
      "epoch 277 loss = 2.107034\n",
      "epoch 278 loss = 1.982631\n",
      "epoch 279 loss = 2.004940\n",
      "epoch 280 loss = 1.818324\n",
      "epoch 281 loss = 2.036453\n",
      "epoch 282 loss = 2.061512\n",
      "epoch 283 loss = 2.046464\n",
      "epoch 284 loss = 2.043292\n",
      "epoch 285 loss = 1.956870\n",
      "epoch 286 loss = 2.038225\n",
      "epoch 287 loss = 1.966598\n",
      "epoch 288 loss = 2.003088\n",
      "epoch 289 loss = 1.966823\n",
      "epoch 290 loss = 1.693526\n",
      "epoch 291 loss = 2.014268\n",
      "epoch 292 loss = 1.891821\n",
      "epoch 293 loss = 2.178309\n",
      "epoch 294 loss = 1.926538\n",
      "epoch 295 loss = 1.932914\n",
      "epoch 296 loss = 1.815699\n",
      "epoch 297 loss = 2.101228\n",
      "epoch 298 loss = 1.936148\n",
      "epoch 299 loss = 1.900547\n",
      "epoch 300 loss = 1.963693\n",
      "epoch 301 loss = 1.889565\n",
      "epoch 302 loss = 2.055686\n",
      "epoch 303 loss = 2.066413\n",
      "epoch 304 loss = 2.186088\n",
      "epoch 305 loss = 2.013418\n",
      "epoch 306 loss = 1.924668\n",
      "epoch 307 loss = 1.872595\n",
      "epoch 308 loss = 2.214700\n",
      "epoch 309 loss = 2.123556\n",
      "epoch 310 loss = 1.915020\n",
      "epoch 311 loss = 2.074075\n",
      "epoch 312 loss = 1.983857\n",
      "epoch 313 loss = 2.126592\n",
      "epoch 314 loss = 2.049339\n",
      "epoch 315 loss = 2.007298\n",
      "epoch 316 loss = 2.082259\n",
      "epoch 317 loss = 1.950319\n",
      "epoch 318 loss = 1.987973\n",
      "epoch 319 loss = 1.887485\n",
      "epoch 320 loss = 2.048084\n",
      "epoch 321 loss = 1.989038\n",
      "epoch 322 loss = 1.845996\n",
      "epoch 323 loss = 1.914458\n",
      "epoch 324 loss = 2.023192\n",
      "epoch 325 loss = 1.814867\n",
      "epoch 326 loss = 2.019244\n",
      "epoch 327 loss = 1.761763\n",
      "epoch 328 loss = 1.992021\n",
      "epoch 329 loss = 1.977187\n",
      "epoch 330 loss = 1.956237\n",
      "epoch 331 loss = 1.866812\n",
      "epoch 332 loss = 1.843237\n",
      "epoch 333 loss = 1.939449\n",
      "epoch 334 loss = 1.921039\n",
      "epoch 335 loss = 1.994746\n",
      "epoch 336 loss = 1.792741\n",
      "epoch 337 loss = 1.783687\n",
      "epoch 338 loss = 1.968743\n",
      "epoch 339 loss = 1.919879\n",
      "epoch 340 loss = 1.800205\n",
      "epoch 341 loss = 1.856326\n",
      "epoch 342 loss = 1.864372\n",
      "epoch 343 loss = 1.965336\n",
      "epoch 344 loss = 2.098824\n",
      "epoch 345 loss = 1.951056\n",
      "epoch 346 loss = 1.997026\n",
      "epoch 347 loss = 1.757100\n",
      "epoch 348 loss = 2.058177\n",
      "epoch 349 loss = 1.869738\n",
      "epoch 350 loss = 2.103360\n",
      "epoch 351 loss = 2.109911\n",
      "epoch 352 loss = 2.055933\n",
      "epoch 353 loss = 1.939061\n",
      "epoch 354 loss = 2.086307\n",
      "epoch 355 loss = 1.929410\n",
      "epoch 356 loss = 1.853986\n",
      "epoch 357 loss = 1.935072\n",
      "epoch 358 loss = 1.874398\n",
      "epoch 359 loss = 1.945245\n",
      "epoch 360 loss = 1.995101\n",
      "epoch 361 loss = 1.940958\n",
      "epoch 362 loss = 1.973491\n",
      "epoch 363 loss = 2.038069\n",
      "epoch 364 loss = 1.899832\n",
      "epoch 365 loss = 1.974949\n",
      "epoch 366 loss = 1.845128\n",
      "epoch 367 loss = 1.990947\n",
      "epoch 368 loss = 1.944288\n",
      "epoch 369 loss = 1.861315\n",
      "epoch 370 loss = 2.059290\n",
      "epoch 371 loss = 1.870248\n",
      "epoch 372 loss = 2.068803\n",
      "epoch 373 loss = 1.963285\n",
      "epoch 374 loss = 1.934573\n",
      "epoch 375 loss = 2.148876\n",
      "epoch 376 loss = 1.971821\n",
      "epoch 377 loss = 2.007873\n",
      "epoch 378 loss = 1.673372\n",
      "epoch 379 loss = 1.966677\n",
      "epoch 380 loss = 2.001701\n",
      "epoch 381 loss = 1.998585\n",
      "epoch 382 loss = 2.281003\n",
      "epoch 383 loss = 1.953259\n",
      "epoch 384 loss = 1.866577\n",
      "epoch 385 loss = 1.950151\n",
      "epoch 386 loss = 1.939689\n",
      "epoch 387 loss = 1.993446\n",
      "epoch 388 loss = 1.988674\n",
      "epoch 389 loss = 1.819612\n",
      "epoch 390 loss = 1.924833\n",
      "epoch 391 loss = 1.827928\n",
      "epoch 392 loss = 1.843512\n",
      "epoch 393 loss = 2.159465\n",
      "epoch 394 loss = 2.100338\n",
      "epoch 395 loss = 1.857581\n",
      "epoch 396 loss = 1.884694\n",
      "epoch 397 loss = 1.939697\n",
      "epoch 398 loss = 2.039242\n",
      "epoch 399 loss = 2.028229\n",
      "epoch 400 loss = 1.880455\n",
      "epoch 401 loss = 1.838458\n",
      "epoch 402 loss = 1.775600\n",
      "epoch 403 loss = 1.958232\n",
      "epoch 404 loss = 1.779805\n",
      "epoch 405 loss = 2.119975\n",
      "epoch 406 loss = 1.812104\n",
      "epoch 407 loss = 1.950507\n",
      "epoch 408 loss = 1.888241\n",
      "epoch 409 loss = 1.963120\n",
      "epoch 410 loss = 1.874503\n",
      "epoch 411 loss = 1.849390\n",
      "epoch 412 loss = 2.012011\n",
      "epoch 413 loss = 2.023086\n",
      "epoch 414 loss = 1.778557\n",
      "epoch 415 loss = 1.967267\n",
      "epoch 416 loss = 1.971272\n",
      "epoch 417 loss = 1.906674\n",
      "epoch 418 loss = 1.978011\n",
      "epoch 419 loss = 1.819959\n",
      "epoch 420 loss = 1.884266\n",
      "epoch 421 loss = 2.069161\n",
      "epoch 422 loss = 1.926499\n",
      "epoch 423 loss = 2.143547\n",
      "epoch 424 loss = 2.000543\n",
      "epoch 425 loss = 1.971253\n",
      "epoch 426 loss = 1.946379\n",
      "epoch 427 loss = 2.000483\n",
      "epoch 428 loss = 2.014528\n",
      "epoch 429 loss = 2.094563\n",
      "epoch 430 loss = 1.831985\n",
      "epoch 431 loss = 1.905592\n",
      "epoch 432 loss = 1.881478\n",
      "epoch 433 loss = 1.858644\n",
      "epoch 434 loss = 2.089903\n",
      "epoch 435 loss = 1.853950\n",
      "epoch 436 loss = 1.613753\n",
      "epoch 437 loss = 2.196540\n",
      "epoch 438 loss = 2.090217\n",
      "epoch 439 loss = 1.880879\n",
      "epoch 440 loss = 1.942207\n",
      "epoch 441 loss = 1.936183\n",
      "epoch 442 loss = 1.884350\n",
      "epoch 443 loss = 1.941429\n",
      "epoch 444 loss = 1.841141\n",
      "epoch 445 loss = 1.865580\n",
      "epoch 446 loss = 1.905085\n",
      "epoch 447 loss = 1.983686\n",
      "epoch 448 loss = 1.979603\n",
      "epoch 449 loss = 1.972249\n",
      "epoch 450 loss = 2.045542\n",
      "epoch 451 loss = 1.824831\n",
      "epoch 452 loss = 2.051522\n",
      "epoch 453 loss = 1.995075\n",
      "epoch 454 loss = 1.944083\n",
      "epoch 455 loss = 1.911354\n",
      "epoch 456 loss = 1.970909\n",
      "epoch 457 loss = 1.977492\n",
      "epoch 458 loss = 2.283820\n",
      "epoch 459 loss = 1.887508\n",
      "epoch 460 loss = 1.903433\n",
      "epoch 461 loss = 1.822384\n",
      "epoch 462 loss = 2.034349\n",
      "epoch 463 loss = 1.872725\n",
      "epoch 464 loss = 1.978513\n",
      "epoch 465 loss = 1.934048\n",
      "epoch 466 loss = 1.878307\n",
      "epoch 467 loss = 1.823176\n",
      "epoch 468 loss = 1.881633\n",
      "epoch 469 loss = 2.047005\n",
      "epoch 470 loss = 2.068612\n",
      "epoch 471 loss = 1.914344\n",
      "epoch 472 loss = 1.939210\n",
      "epoch 473 loss = 1.991631\n",
      "epoch 474 loss = 2.071564\n",
      "epoch 475 loss = 2.012780\n",
      "epoch 476 loss = 1.981924\n",
      "epoch 477 loss = 1.975491\n",
      "epoch 478 loss = 2.188679\n",
      "epoch 479 loss = 1.869814\n",
      "epoch 480 loss = 1.899411\n",
      "epoch 481 loss = 2.004113\n",
      "epoch 482 loss = 2.160990\n",
      "epoch 483 loss = 2.112109\n",
      "epoch 484 loss = 1.983778\n",
      "epoch 485 loss = 1.922470\n",
      "epoch 486 loss = 1.927896\n",
      "epoch 487 loss = 1.849958\n",
      "epoch 488 loss = 1.818211\n",
      "epoch 489 loss = 1.873736\n",
      "epoch 490 loss = 1.977692\n",
      "epoch 491 loss = 1.909177\n",
      "epoch 492 loss = 1.926260\n",
      "epoch 493 loss = 1.948153\n",
      "epoch 494 loss = 1.989875\n",
      "epoch 495 loss = 2.038355\n",
      "epoch 496 loss = 1.920448\n",
      "epoch 497 loss = 2.130185\n",
      "epoch 498 loss = 1.713846\n",
      "epoch 499 loss = 1.997487\n",
      "final loss = 1.997487\n",
      "accuracy_mc = tensor(0.3041, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.2338, device='cuda:0')\n",
      "test_ll_mc = tensor(-1.9206, device='cuda:0')\n",
      "training time = 173.1456639766693 seconds\n",
      "testing time = 1.913116216659546 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.900000, reg_strength 0.050000\n",
      "n_epoch 10\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0696, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0696, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.547417163848877 seconds\n",
      "testing time = 1.8929345607757568 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0880, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0880, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.4408860206604004 seconds\n",
      "testing time = 1.8420860767364502 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0908, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0908, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.436462640762329 seconds\n",
      "testing time = 1.8376951217651367 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0979, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0979, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.495544195175171 seconds\n",
      "testing time = 1.8367376327514648 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1177, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1177, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.514460325241089 seconds\n",
      "testing time = 1.875382423400879 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1261, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1261, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.4942173957824707 seconds\n",
      "testing time = 1.8865118026733398 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1035, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1035, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.451765775680542 seconds\n",
      "testing time = 1.8964588642120361 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0657, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0657, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.525771379470825 seconds\n",
      "testing time = 1.9078316688537598 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1522, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1522, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.5191941261291504 seconds\n",
      "testing time = 1.9654204845428467 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0835, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0835, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 3.521489143371582 seconds\n",
      "testing time = 1.886265754699707 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.900000, reg_strength 0.050000\n",
      "n_epoch 100\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0696, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0696, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.51218795776367 seconds\n",
      "testing time = 1.9055850505828857 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0880, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0880, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.81050252914429 seconds\n",
      "testing time = 1.8498973846435547 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0908, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0908, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.202948331832886 seconds\n",
      "testing time = 1.8924095630645752 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0979, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0979, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.82245373725891 seconds\n",
      "testing time = 1.9160490036010742 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1177, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1177, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.743080139160156 seconds\n",
      "testing time = 1.8936271667480469 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1261, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1261, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.9254686832428 seconds\n",
      "testing time = 1.920466423034668 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1035, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1035, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 35.08139395713806 seconds\n",
      "testing time = 1.8588075637817383 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0657, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0657, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.711021184921265 seconds\n",
      "testing time = 1.9008116722106934 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1522, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1522, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 35.13249063491821 seconds\n",
      "testing time = 1.9618585109710693 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0835, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0835, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 34.9710054397583 seconds\n",
      "testing time = 1.8609318733215332 seconds\n",
      "\n",
      "subset 0.050000, dropout_rate 0.900000, reg_strength 0.050000\n",
      "n_epoch 500\n",
      "\n",
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "Training with split 0\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0696, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0696, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 175.88515901565552 seconds\n",
      "testing time = 1.9297895431518555 seconds\n",
      "\n",
      "Training with split 1\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0880, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0880, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 174.986989736557 seconds\n",
      "testing time = 1.948601484298706 seconds\n",
      "\n",
      "Training with split 2\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0908, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0908, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 173.80884385108948 seconds\n",
      "testing time = 1.9293384552001953 seconds\n",
      "\n",
      "Training with split 3\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0979, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0979, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 172.7291760444641 seconds\n",
      "testing time = 1.910254955291748 seconds\n",
      "\n",
      "Training with split 4\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1177, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1177, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 170.3854911327362 seconds\n",
      "testing time = 1.8267135620117188 seconds\n",
      "\n",
      "Training with split 5\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1261, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1261, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 169.96374320983887 seconds\n",
      "testing time = 1.8587465286254883 seconds\n",
      "\n",
      "Training with split 6\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1035, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1035, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 170.14309215545654 seconds\n",
      "testing time = 1.8773789405822754 seconds\n",
      "\n",
      "Training with split 7\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0657, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0657, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 170.74335741996765 seconds\n",
      "testing time = 1.8832957744598389 seconds\n",
      "\n",
      "Training with split 8\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.1522, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.1522, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 170.05370688438416 seconds\n",
      "testing time = 1.877058982849121 seconds\n",
      "\n",
      "Training with split 9\n",
      "epoch 0 loss = nan\n",
      "epoch 1 loss = nan\n",
      "epoch 2 loss = nan\n",
      "epoch 3 loss = nan\n",
      "epoch 4 loss = nan\n",
      "epoch 5 loss = nan\n",
      "epoch 6 loss = nan\n",
      "epoch 7 loss = nan\n",
      "epoch 8 loss = nan\n",
      "epoch 9 loss = nan\n",
      "epoch 10 loss = nan\n",
      "epoch 11 loss = nan\n",
      "epoch 12 loss = nan\n",
      "epoch 13 loss = nan\n",
      "epoch 14 loss = nan\n",
      "epoch 15 loss = nan\n",
      "epoch 16 loss = nan\n",
      "epoch 17 loss = nan\n",
      "epoch 18 loss = nan\n",
      "epoch 19 loss = nan\n",
      "epoch 20 loss = nan\n",
      "epoch 21 loss = nan\n",
      "epoch 22 loss = nan\n",
      "epoch 23 loss = nan\n",
      "epoch 24 loss = nan\n",
      "epoch 25 loss = nan\n",
      "epoch 26 loss = nan\n",
      "epoch 27 loss = nan\n",
      "epoch 28 loss = nan\n",
      "epoch 29 loss = nan\n",
      "epoch 30 loss = nan\n",
      "epoch 31 loss = nan\n",
      "epoch 32 loss = nan\n",
      "epoch 33 loss = nan\n",
      "epoch 34 loss = nan\n",
      "epoch 35 loss = nan\n",
      "epoch 36 loss = nan\n",
      "epoch 37 loss = nan\n",
      "epoch 38 loss = nan\n",
      "epoch 39 loss = nan\n",
      "epoch 40 loss = nan\n",
      "epoch 41 loss = nan\n",
      "epoch 42 loss = nan\n",
      "epoch 43 loss = nan\n",
      "epoch 44 loss = nan\n",
      "epoch 45 loss = nan\n",
      "epoch 46 loss = nan\n",
      "epoch 47 loss = nan\n",
      "epoch 48 loss = nan\n",
      "epoch 49 loss = nan\n",
      "epoch 50 loss = nan\n",
      "epoch 51 loss = nan\n",
      "epoch 52 loss = nan\n",
      "epoch 53 loss = nan\n",
      "epoch 54 loss = nan\n",
      "epoch 55 loss = nan\n",
      "epoch 56 loss = nan\n",
      "epoch 57 loss = nan\n",
      "epoch 58 loss = nan\n",
      "epoch 59 loss = nan\n",
      "epoch 60 loss = nan\n",
      "epoch 61 loss = nan\n",
      "epoch 62 loss = nan\n",
      "epoch 63 loss = nan\n",
      "epoch 64 loss = nan\n",
      "epoch 65 loss = nan\n",
      "epoch 66 loss = nan\n",
      "epoch 67 loss = nan\n",
      "epoch 68 loss = nan\n",
      "epoch 69 loss = nan\n",
      "epoch 70 loss = nan\n",
      "epoch 71 loss = nan\n",
      "epoch 72 loss = nan\n",
      "epoch 73 loss = nan\n",
      "epoch 74 loss = nan\n",
      "epoch 75 loss = nan\n",
      "epoch 76 loss = nan\n",
      "epoch 77 loss = nan\n",
      "epoch 78 loss = nan\n",
      "epoch 79 loss = nan\n",
      "epoch 80 loss = nan\n",
      "epoch 81 loss = nan\n",
      "epoch 82 loss = nan\n",
      "epoch 83 loss = nan\n",
      "epoch 84 loss = nan\n",
      "epoch 85 loss = nan\n",
      "epoch 86 loss = nan\n",
      "epoch 87 loss = nan\n",
      "epoch 88 loss = nan\n",
      "epoch 89 loss = nan\n",
      "epoch 90 loss = nan\n",
      "epoch 91 loss = nan\n",
      "epoch 92 loss = nan\n",
      "epoch 93 loss = nan\n",
      "epoch 94 loss = nan\n",
      "epoch 95 loss = nan\n",
      "epoch 96 loss = nan\n",
      "epoch 97 loss = nan\n",
      "epoch 98 loss = nan\n",
      "epoch 99 loss = nan\n",
      "epoch 100 loss = nan\n",
      "epoch 101 loss = nan\n",
      "epoch 102 loss = nan\n",
      "epoch 103 loss = nan\n",
      "epoch 104 loss = nan\n",
      "epoch 105 loss = nan\n",
      "epoch 106 loss = nan\n",
      "epoch 107 loss = nan\n",
      "epoch 108 loss = nan\n",
      "epoch 109 loss = nan\n",
      "epoch 110 loss = nan\n",
      "epoch 111 loss = nan\n",
      "epoch 112 loss = nan\n",
      "epoch 113 loss = nan\n",
      "epoch 114 loss = nan\n",
      "epoch 115 loss = nan\n",
      "epoch 116 loss = nan\n",
      "epoch 117 loss = nan\n",
      "epoch 118 loss = nan\n",
      "epoch 119 loss = nan\n",
      "epoch 120 loss = nan\n",
      "epoch 121 loss = nan\n",
      "epoch 122 loss = nan\n",
      "epoch 123 loss = nan\n",
      "epoch 124 loss = nan\n",
      "epoch 125 loss = nan\n",
      "epoch 126 loss = nan\n",
      "epoch 127 loss = nan\n",
      "epoch 128 loss = nan\n",
      "epoch 129 loss = nan\n",
      "epoch 130 loss = nan\n",
      "epoch 131 loss = nan\n",
      "epoch 132 loss = nan\n",
      "epoch 133 loss = nan\n",
      "epoch 134 loss = nan\n",
      "epoch 135 loss = nan\n",
      "epoch 136 loss = nan\n",
      "epoch 137 loss = nan\n",
      "epoch 138 loss = nan\n",
      "epoch 139 loss = nan\n",
      "epoch 140 loss = nan\n",
      "epoch 141 loss = nan\n",
      "epoch 142 loss = nan\n",
      "epoch 143 loss = nan\n",
      "epoch 144 loss = nan\n",
      "epoch 145 loss = nan\n",
      "epoch 146 loss = nan\n",
      "epoch 147 loss = nan\n",
      "epoch 148 loss = nan\n",
      "epoch 149 loss = nan\n",
      "epoch 150 loss = nan\n",
      "epoch 151 loss = nan\n",
      "epoch 152 loss = nan\n",
      "epoch 153 loss = nan\n",
      "epoch 154 loss = nan\n",
      "epoch 155 loss = nan\n",
      "epoch 156 loss = nan\n",
      "epoch 157 loss = nan\n",
      "epoch 158 loss = nan\n",
      "epoch 159 loss = nan\n",
      "epoch 160 loss = nan\n",
      "epoch 161 loss = nan\n",
      "epoch 162 loss = nan\n",
      "epoch 163 loss = nan\n",
      "epoch 164 loss = nan\n",
      "epoch 165 loss = nan\n",
      "epoch 166 loss = nan\n",
      "epoch 167 loss = nan\n",
      "epoch 168 loss = nan\n",
      "epoch 169 loss = nan\n",
      "epoch 170 loss = nan\n",
      "epoch 171 loss = nan\n",
      "epoch 172 loss = nan\n",
      "epoch 173 loss = nan\n",
      "epoch 174 loss = nan\n",
      "epoch 175 loss = nan\n",
      "epoch 176 loss = nan\n",
      "epoch 177 loss = nan\n",
      "epoch 178 loss = nan\n",
      "epoch 179 loss = nan\n",
      "epoch 180 loss = nan\n",
      "epoch 181 loss = nan\n",
      "epoch 182 loss = nan\n",
      "epoch 183 loss = nan\n",
      "epoch 184 loss = nan\n",
      "epoch 185 loss = nan\n",
      "epoch 186 loss = nan\n",
      "epoch 187 loss = nan\n",
      "epoch 188 loss = nan\n",
      "epoch 189 loss = nan\n",
      "epoch 190 loss = nan\n",
      "epoch 191 loss = nan\n",
      "epoch 192 loss = nan\n",
      "epoch 193 loss = nan\n",
      "epoch 194 loss = nan\n",
      "epoch 195 loss = nan\n",
      "epoch 196 loss = nan\n",
      "epoch 197 loss = nan\n",
      "epoch 198 loss = nan\n",
      "epoch 199 loss = nan\n",
      "epoch 200 loss = nan\n",
      "epoch 201 loss = nan\n",
      "epoch 202 loss = nan\n",
      "epoch 203 loss = nan\n",
      "epoch 204 loss = nan\n",
      "epoch 205 loss = nan\n",
      "epoch 206 loss = nan\n",
      "epoch 207 loss = nan\n",
      "epoch 208 loss = nan\n",
      "epoch 209 loss = nan\n",
      "epoch 210 loss = nan\n",
      "epoch 211 loss = nan\n",
      "epoch 212 loss = nan\n",
      "epoch 213 loss = nan\n",
      "epoch 214 loss = nan\n",
      "epoch 215 loss = nan\n",
      "epoch 216 loss = nan\n",
      "epoch 217 loss = nan\n",
      "epoch 218 loss = nan\n",
      "epoch 219 loss = nan\n",
      "epoch 220 loss = nan\n",
      "epoch 221 loss = nan\n",
      "epoch 222 loss = nan\n",
      "epoch 223 loss = nan\n",
      "epoch 224 loss = nan\n",
      "epoch 225 loss = nan\n",
      "epoch 226 loss = nan\n",
      "epoch 227 loss = nan\n",
      "epoch 228 loss = nan\n",
      "epoch 229 loss = nan\n",
      "epoch 230 loss = nan\n",
      "epoch 231 loss = nan\n",
      "epoch 232 loss = nan\n",
      "epoch 233 loss = nan\n",
      "epoch 234 loss = nan\n",
      "epoch 235 loss = nan\n",
      "epoch 236 loss = nan\n",
      "epoch 237 loss = nan\n",
      "epoch 238 loss = nan\n",
      "epoch 239 loss = nan\n",
      "epoch 240 loss = nan\n",
      "epoch 241 loss = nan\n",
      "epoch 242 loss = nan\n",
      "epoch 243 loss = nan\n",
      "epoch 244 loss = nan\n",
      "epoch 245 loss = nan\n",
      "epoch 246 loss = nan\n",
      "epoch 247 loss = nan\n",
      "epoch 248 loss = nan\n",
      "epoch 249 loss = nan\n",
      "epoch 250 loss = nan\n",
      "epoch 251 loss = nan\n",
      "epoch 252 loss = nan\n",
      "epoch 253 loss = nan\n",
      "epoch 254 loss = nan\n",
      "epoch 255 loss = nan\n",
      "epoch 256 loss = nan\n",
      "epoch 257 loss = nan\n",
      "epoch 258 loss = nan\n",
      "epoch 259 loss = nan\n",
      "epoch 260 loss = nan\n",
      "epoch 261 loss = nan\n",
      "epoch 262 loss = nan\n",
      "epoch 263 loss = nan\n",
      "epoch 264 loss = nan\n",
      "epoch 265 loss = nan\n",
      "epoch 266 loss = nan\n",
      "epoch 267 loss = nan\n",
      "epoch 268 loss = nan\n",
      "epoch 269 loss = nan\n",
      "epoch 270 loss = nan\n",
      "epoch 271 loss = nan\n",
      "epoch 272 loss = nan\n",
      "epoch 273 loss = nan\n",
      "epoch 274 loss = nan\n",
      "epoch 275 loss = nan\n",
      "epoch 276 loss = nan\n",
      "epoch 277 loss = nan\n",
      "epoch 278 loss = nan\n",
      "epoch 279 loss = nan\n",
      "epoch 280 loss = nan\n",
      "epoch 281 loss = nan\n",
      "epoch 282 loss = nan\n",
      "epoch 283 loss = nan\n",
      "epoch 284 loss = nan\n",
      "epoch 285 loss = nan\n",
      "epoch 286 loss = nan\n",
      "epoch 287 loss = nan\n",
      "epoch 288 loss = nan\n",
      "epoch 289 loss = nan\n",
      "epoch 290 loss = nan\n",
      "epoch 291 loss = nan\n",
      "epoch 292 loss = nan\n",
      "epoch 293 loss = nan\n",
      "epoch 294 loss = nan\n",
      "epoch 295 loss = nan\n",
      "epoch 296 loss = nan\n",
      "epoch 297 loss = nan\n",
      "epoch 298 loss = nan\n",
      "epoch 299 loss = nan\n",
      "epoch 300 loss = nan\n",
      "epoch 301 loss = nan\n",
      "epoch 302 loss = nan\n",
      "epoch 303 loss = nan\n",
      "epoch 304 loss = nan\n",
      "epoch 305 loss = nan\n",
      "epoch 306 loss = nan\n",
      "epoch 307 loss = nan\n",
      "epoch 308 loss = nan\n",
      "epoch 309 loss = nan\n",
      "epoch 310 loss = nan\n",
      "epoch 311 loss = nan\n",
      "epoch 312 loss = nan\n",
      "epoch 313 loss = nan\n",
      "epoch 314 loss = nan\n",
      "epoch 315 loss = nan\n",
      "epoch 316 loss = nan\n",
      "epoch 317 loss = nan\n",
      "epoch 318 loss = nan\n",
      "epoch 319 loss = nan\n",
      "epoch 320 loss = nan\n",
      "epoch 321 loss = nan\n",
      "epoch 322 loss = nan\n",
      "epoch 323 loss = nan\n",
      "epoch 324 loss = nan\n",
      "epoch 325 loss = nan\n",
      "epoch 326 loss = nan\n",
      "epoch 327 loss = nan\n",
      "epoch 328 loss = nan\n",
      "epoch 329 loss = nan\n",
      "epoch 330 loss = nan\n",
      "epoch 331 loss = nan\n",
      "epoch 332 loss = nan\n",
      "epoch 333 loss = nan\n",
      "epoch 334 loss = nan\n",
      "epoch 335 loss = nan\n",
      "epoch 336 loss = nan\n",
      "epoch 337 loss = nan\n",
      "epoch 338 loss = nan\n",
      "epoch 339 loss = nan\n",
      "epoch 340 loss = nan\n",
      "epoch 341 loss = nan\n",
      "epoch 342 loss = nan\n",
      "epoch 343 loss = nan\n",
      "epoch 344 loss = nan\n",
      "epoch 345 loss = nan\n",
      "epoch 346 loss = nan\n",
      "epoch 347 loss = nan\n",
      "epoch 348 loss = nan\n",
      "epoch 349 loss = nan\n",
      "epoch 350 loss = nan\n",
      "epoch 351 loss = nan\n",
      "epoch 352 loss = nan\n",
      "epoch 353 loss = nan\n",
      "epoch 354 loss = nan\n",
      "epoch 355 loss = nan\n",
      "epoch 356 loss = nan\n",
      "epoch 357 loss = nan\n",
      "epoch 358 loss = nan\n",
      "epoch 359 loss = nan\n",
      "epoch 360 loss = nan\n",
      "epoch 361 loss = nan\n",
      "epoch 362 loss = nan\n",
      "epoch 363 loss = nan\n",
      "epoch 364 loss = nan\n",
      "epoch 365 loss = nan\n",
      "epoch 366 loss = nan\n",
      "epoch 367 loss = nan\n",
      "epoch 368 loss = nan\n",
      "epoch 369 loss = nan\n",
      "epoch 370 loss = nan\n",
      "epoch 371 loss = nan\n",
      "epoch 372 loss = nan\n",
      "epoch 373 loss = nan\n",
      "epoch 374 loss = nan\n",
      "epoch 375 loss = nan\n",
      "epoch 376 loss = nan\n",
      "epoch 377 loss = nan\n",
      "epoch 378 loss = nan\n",
      "epoch 379 loss = nan\n",
      "epoch 380 loss = nan\n",
      "epoch 381 loss = nan\n",
      "epoch 382 loss = nan\n",
      "epoch 383 loss = nan\n",
      "epoch 384 loss = nan\n",
      "epoch 385 loss = nan\n",
      "epoch 386 loss = nan\n",
      "epoch 387 loss = nan\n",
      "epoch 388 loss = nan\n",
      "epoch 389 loss = nan\n",
      "epoch 390 loss = nan\n",
      "epoch 391 loss = nan\n",
      "epoch 392 loss = nan\n",
      "epoch 393 loss = nan\n",
      "epoch 394 loss = nan\n",
      "epoch 395 loss = nan\n",
      "epoch 396 loss = nan\n",
      "epoch 397 loss = nan\n",
      "epoch 398 loss = nan\n",
      "epoch 399 loss = nan\n",
      "epoch 400 loss = nan\n",
      "epoch 401 loss = nan\n",
      "epoch 402 loss = nan\n",
      "epoch 403 loss = nan\n",
      "epoch 404 loss = nan\n",
      "epoch 405 loss = nan\n",
      "epoch 406 loss = nan\n",
      "epoch 407 loss = nan\n",
      "epoch 408 loss = nan\n",
      "epoch 409 loss = nan\n",
      "epoch 410 loss = nan\n",
      "epoch 411 loss = nan\n",
      "epoch 412 loss = nan\n",
      "epoch 413 loss = nan\n",
      "epoch 414 loss = nan\n",
      "epoch 415 loss = nan\n",
      "epoch 416 loss = nan\n",
      "epoch 417 loss = nan\n",
      "epoch 418 loss = nan\n",
      "epoch 419 loss = nan\n",
      "epoch 420 loss = nan\n",
      "epoch 421 loss = nan\n",
      "epoch 422 loss = nan\n",
      "epoch 423 loss = nan\n",
      "epoch 424 loss = nan\n",
      "epoch 425 loss = nan\n",
      "epoch 426 loss = nan\n",
      "epoch 427 loss = nan\n",
      "epoch 428 loss = nan\n",
      "epoch 429 loss = nan\n",
      "epoch 430 loss = nan\n",
      "epoch 431 loss = nan\n",
      "epoch 432 loss = nan\n",
      "epoch 433 loss = nan\n",
      "epoch 434 loss = nan\n",
      "epoch 435 loss = nan\n",
      "epoch 436 loss = nan\n",
      "epoch 437 loss = nan\n",
      "epoch 438 loss = nan\n",
      "epoch 439 loss = nan\n",
      "epoch 440 loss = nan\n",
      "epoch 441 loss = nan\n",
      "epoch 442 loss = nan\n",
      "epoch 443 loss = nan\n",
      "epoch 444 loss = nan\n",
      "epoch 445 loss = nan\n",
      "epoch 446 loss = nan\n",
      "epoch 447 loss = nan\n",
      "epoch 448 loss = nan\n",
      "epoch 449 loss = nan\n",
      "epoch 450 loss = nan\n",
      "epoch 451 loss = nan\n",
      "epoch 452 loss = nan\n",
      "epoch 453 loss = nan\n",
      "epoch 454 loss = nan\n",
      "epoch 455 loss = nan\n",
      "epoch 456 loss = nan\n",
      "epoch 457 loss = nan\n",
      "epoch 458 loss = nan\n",
      "epoch 459 loss = nan\n",
      "epoch 460 loss = nan\n",
      "epoch 461 loss = nan\n",
      "epoch 462 loss = nan\n",
      "epoch 463 loss = nan\n",
      "epoch 464 loss = nan\n",
      "epoch 465 loss = nan\n",
      "epoch 466 loss = nan\n",
      "epoch 467 loss = nan\n",
      "epoch 468 loss = nan\n",
      "epoch 469 loss = nan\n",
      "epoch 470 loss = nan\n",
      "epoch 471 loss = nan\n",
      "epoch 472 loss = nan\n",
      "epoch 473 loss = nan\n",
      "epoch 474 loss = nan\n",
      "epoch 475 loss = nan\n",
      "epoch 476 loss = nan\n",
      "epoch 477 loss = nan\n",
      "epoch 478 loss = nan\n",
      "epoch 479 loss = nan\n",
      "epoch 480 loss = nan\n",
      "epoch 481 loss = nan\n",
      "epoch 482 loss = nan\n",
      "epoch 483 loss = nan\n",
      "epoch 484 loss = nan\n",
      "epoch 485 loss = nan\n",
      "epoch 486 loss = nan\n",
      "epoch 487 loss = nan\n",
      "epoch 488 loss = nan\n",
      "epoch 489 loss = nan\n",
      "epoch 490 loss = nan\n",
      "epoch 491 loss = nan\n",
      "epoch 492 loss = nan\n",
      "epoch 493 loss = nan\n",
      "epoch 494 loss = nan\n",
      "epoch 495 loss = nan\n",
      "epoch 496 loss = nan\n",
      "epoch 497 loss = nan\n",
      "epoch 498 loss = nan\n",
      "epoch 499 loss = nan\n",
      "final loss = nan\n",
      "accuracy_mc = tensor(0.0835, device='cuda:0')\n",
      "accuracy_non_mc = tensor(0.0835, device='cuda:0')\n",
      "test_ll_mc = tensor(nan, device='cuda:0')\n",
      "training time = 169.80181789398193 seconds\n",
      "testing time = 1.8678455352783203 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subset_prop, dropout_rate, reg_strength, n_epoch in itertools.product(\n",
    "    subset_proportions,\n",
    "    dropout_rates, reg_strengths, \n",
    "    n_epochs,\n",
    "):  \n",
    "    # Reset the random number generator for each method (to produce identical results)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    pyro.set_rng_seed(random_seed)\n",
    "\n",
    "    # Print parameter combinations being tested\n",
    "    print(\n",
    "        \"subset %f, dropout_rate %f, reg_strength %f\"\n",
    "        % (subset_prop, dropout_rate, reg_strength))\n",
    "\n",
    "    print(\"n_epoch %d\" % n_epoch)\n",
    "    print()\n",
    "\n",
    "    \"\"\"\n",
    "    Results file storage\n",
    "    \"\"\"\n",
    "\n",
    "    # Create directory to store results for the current test configuration\n",
    "    test_results_path = os.path.join(\n",
    "        './test_results',\n",
    "        'error_convergence_2',\n",
    "        'CIFAR-10',\n",
    "        test_start_time,\n",
    "        (\n",
    "            str(subset_prop)\n",
    "            + '_' + str(dropout_rate) \n",
    "            + '_' + str(reg_strength)\n",
    "            + '_' + str(n_epoch)),\n",
    "    )\n",
    "\n",
    "    os.makedirs(test_results_path, exist_ok=True)\n",
    "\n",
    "    test_results_accuracy_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_mc.txt\"\n",
    "    )\n",
    "\n",
    "    test_results_accuracy_non_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"accuracy_non_mc.txt\"\n",
    "    )\n",
    "\n",
    "    test_results_lls_mc_path = os.path.join(\n",
    "        test_results_path,\n",
    "        \"lls_mc.txt\"\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Dataset multiple splits prep\n",
    "    \"\"\"\n",
    "    # Prepare new subset of the original dataset\n",
    "    subset = datasets.CIFAR10(\n",
    "        root='./datasets_files', limit_size=subset_prop, transform=transform, download=True)\n",
    "\n",
    "    # Determine sizes of training and testing set\n",
    "    train_size = int(dataset_train_size * len(subset))\n",
    "    test_size = len(subset) - train_size\n",
    "\n",
    "    # Print the size of the subset\n",
    "    print(\"subset size = \" + str(subset.data.shape))\n",
    "    print(\"training set size = %d\" % train_size)\n",
    "    print(\"test set size = %d\" % test_size)\n",
    "    print()\n",
    "\n",
    "    # Prepare multiple sets of random train-test splits \n",
    "    # to test the parameter combination\n",
    "    subset_splits = []\n",
    "\n",
    "    for _ in range(n_splits):\n",
    "        train, test = random_split(subset, lengths=[train_size, test_size])\n",
    "        subset_splits.append((train, test))\n",
    "\n",
    "    \"\"\"\n",
    "    Training & testing\n",
    "    \"\"\"\n",
    "\n",
    "    # Try learning with different splits\n",
    "    for s, (train, test) in enumerate(subset_splits):\n",
    "\n",
    "        \"\"\"\n",
    "        Training\n",
    "        \"\"\"\n",
    "\n",
    "        print('Training with split %d' % s)\n",
    "\n",
    "        train_loader = DataLoader(train, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "        # Prepare network\n",
    "        network = models.SimpleCIFAR10MCDropout(\n",
    "        #network = models.SqueezeNetDropout(\n",
    "        #    num_classes=10,\n",
    "            dropout_rate=dropout_rate,\n",
    "            dropout_type='bernoulli',\n",
    "        )\n",
    "\n",
    "        # Send the whole model to the selected torch.device\n",
    "        network.to(torch_device)\n",
    "\n",
    "        # Model to train mode\n",
    "        network.train()\n",
    "\n",
    "        # Adam optimizer\n",
    "        # https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam\n",
    "        # NOTE: Need to set L2 regularization from here\n",
    "        optimizer = optim.SGD(\n",
    "            network.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=reg_strength, # L2 regularization\n",
    "        )\n",
    "\n",
    "        # Record training start time (for this split)\n",
    "        tic = time.time()\n",
    "\n",
    "        for epoch in range(n_epoch): # loop over the dataset multiple times\n",
    "            # Mini-batches\n",
    "            for data in train_loader:\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, targets = data\n",
    "\n",
    "                # Store the batch to torch_device's memory\n",
    "                inputs = inputs.to(torch_device)\n",
    "                targets = targets.to(torch_device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = network(inputs)\n",
    "\n",
    "                loss = objective(outputs, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            print(\"epoch %d loss = %f\" % (epoch, loss.item()))\n",
    "\n",
    "        # Record training end time\n",
    "        toc = time.time()\n",
    "\n",
    "        # Report the final loss\n",
    "        print(\"final loss = %f\" % (loss.item()))\n",
    "\n",
    "        \"\"\"\n",
    "        Testing\n",
    "        \"\"\"\n",
    "\n",
    "        # Model to eval mode\n",
    "        network.eval()\n",
    "\n",
    "        # Store the batch to torch_device's memory\n",
    "        test_loader = DataLoader(test, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "        # Record testing start time\n",
    "        tic_testing = time.time()\n",
    "\n",
    "        _, mean, metrics = network.predict_dist(test_loader, n_prediction, torch_device)\n",
    "\n",
    "        # Record testing end time\n",
    "        toc_testing = time.time()\n",
    "\n",
    "        # Record all the scores to the score files\n",
    "        if len(metrics) > 0:\n",
    "            for key, value in metrics.items():\n",
    "                print(str(key) + \" = \" + str(value))\n",
    "\n",
    "                if key == 'accuracy_mc':\n",
    "                    with open(test_results_accuracy_mc_path, 'a+') as accuracy_mc_file:\n",
    "                        accuracy_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "                elif key == 'accuracy_non_mc':\n",
    "                    with open(test_results_accuracy_non_mc_path, 'a+') as accuracy_non_mc_file:\n",
    "                        accuracy_non_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "                elif key == 'test_ll_mc':\n",
    "                    with open(test_results_lls_mc_path, 'a+') as lls_mc_file:\n",
    "                        lls_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "        # Report the total training time\n",
    "        print(\"training time = \" + str(toc - tic) + \" seconds\")\n",
    "\n",
    "        # Report the total testing time\n",
    "        print(\"testing time = \" + str(toc_testing - tic_testing) + \" seconds\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whZbn3VTTUFa"
   },
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDxkRM5aVrdf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment_error_convergence_2_cifar10_simplecnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
