{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bka_bK83VFHh"
   },
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D5NSPs_cJZe"
   },
   "source": [
    "### Random seed / PyTorch / CUDA related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5979,
     "status": "ok",
     "timestamp": 1575227654566,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "pHbfpytEVFHu",
    "outputId": "fc6e59f5-fe07-4f25-b85a-0c465d4a7fb7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # If there's a package I need to install separately, do it here\n",
    "    !pip install pyro-ppl\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd 'drive/My Drive/Colab Notebooks/bayesian-dl-experiments'\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls\n",
    "\n",
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seeds\n",
    "# Based on https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random_seed = 682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqlpuws9Y-8U"
   },
   "source": [
    "### Third party libraries (NumPy, PyTorch, Pyro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6455,
     "status": "ok",
     "timestamp": 1575227655051,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "2zNVvKmZY-8X",
    "outputId": "e36d839c-aea0-4560-d542-9290e420c9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.7.4 (default, Aug 13 2019, 15:17:50) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "NumPy Version: 1.17.2\n",
      "PyTorch Version: 1.3.1\n",
      "Pyro Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Third party libraries import\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print version information\n",
    "print(\"Python Version: \" + sys.version)\n",
    "print(\"NumPy Version: \" + np.__version__)\n",
    "print(\"PyTorch Version: \" + torch.__version__)\n",
    "print(\"Pyro Version: \" + pyro.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6992,
     "status": "ok",
     "timestamp": 1575227655599,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "uyRIfCC5Y-8g",
    "outputId": "8f130afa-cd6e-4ee3-bc0a-dbd6af2c8070"
   },
   "outputs": [],
   "source": [
    "# More imports...\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader, RandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pyro.infer import SVI, Trace_ELBO, HMC, MCMC\n",
    "\n",
    "# Import model and dataset classes from ronald_bdl\n",
    "from ronald_bdl import models, datasets\n",
    "from ronald_bdl.models import utils\n",
    "\n",
    "# pyplot setting\n",
    "%matplotlib inline\n",
    "\n",
    "# torch.device / CUDA Setup\n",
    "use_cuda = True\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Disable 'benchmark' mode\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    use_pin_memory = True # Faster Host to GPU copies with page-locked memory\n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIFRoH3AcJZn"
   },
   "source": [
    "### Variable settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGRg2u0Q_I3n"
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2gc_i7T_HVw"
   },
   "outputs": [],
   "source": [
    "# CIFAR10 data transformation setting\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Set the proportion of the original dataset to be available as a whole\n",
    "subset_proportions = [0.01, 0.1, 1]\n",
    "\n",
    "# Proportion of the dataset to be used for training\n",
    "dataset_train_size = 0.8\n",
    "\n",
    "# Number of dataset splits\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzxIZiUcA8D8"
   },
   "source": [
    "#### NN settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_pzGq1_cJZp"
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "dropout_rates = [0.1, 0.3, 0.5]\n",
    "\n",
    "# Length-scale\n",
    "length_scale_values = [1e-2]\n",
    "\n",
    "# Model Precision\n",
    "tau_values = [0.1, 0.15, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuTnXABzVFKI"
   },
   "source": [
    "\n",
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1575227752260,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "p19qFgSAVFKS",
    "outputId": "3895eaea-67d8-4780-91f5-8d4be308a397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201912031234\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "n_epoch_values = [40, 400, 4000]\n",
    "\n",
    "# Optimizer learning rate\n",
    "optimizer_learning_rate = 0.001 # PyTorch default value is 0.001\n",
    "\n",
    "# Data batch sizes\n",
    "n_training_batch = 512\n",
    "\n",
    "# Number of test predictions (for each data point)\n",
    "n_prediction = 500\n",
    "\n",
    "# Cross Entropy to minimize\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test start time\n",
    "test_start_time = datetime.datetime.today().strftime('%Y%m%d%H%M')\n",
    "\n",
    "print(test_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PpzPMI8VFKE"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11504639,
     "status": "error",
     "timestamp": 1575239261379,
     "user": {
      "displayName": "Ronald Seoh",
      "photoUrl": "",
      "userId": "10284188050297676522"
     },
     "user_tz": 300
    },
    "id": "m4kavCiTVFKf",
    "outputId": "02b9b726-8f0d-47e0-d92e-d86d31bd2dd1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "subset size = (2500, 32, 32, 3)\n",
      "training set size = 2000\n",
      "test set size = 500\n",
      "\n",
      "subset 0.050000, dropout_rate 0.100000, length_scale 0.010000, tau 0.100000\n",
      "reg_strength = tensor(2.2500e-07)\n",
      "Training with split 0\n",
      "Training for 40 epochs total.\n",
      "epoch 0 loss = 2.275429\n",
      "epoch 1 loss = 2.206273\n",
      "epoch 2 loss = 2.140935\n",
      "epoch 3 loss = 2.069733\n",
      "epoch 4 loss = 1.998701\n",
      "epoch 5 loss = 1.993923\n",
      "epoch 6 loss = 1.941433\n",
      "epoch 7 loss = 1.926011\n",
      "epoch 8 loss = 1.888519\n",
      "epoch 9 loss = 1.838508\n",
      "epoch 10 loss = 1.833360\n",
      "epoch 11 loss = 1.781785\n",
      "epoch 12 loss = 1.747734\n",
      "epoch 13 loss = 1.708642\n",
      "epoch 14 loss = 1.698760\n",
      "epoch 15 loss = 1.651852\n",
      "epoch 16 loss = 1.664166\n",
      "epoch 17 loss = 1.608077\n",
      "epoch 18 loss = 1.595061\n",
      "epoch 19 loss = 1.554573\n",
      "epoch 20 loss = 1.519203\n",
      "epoch 21 loss = 1.474406\n",
      "epoch 22 loss = 1.459662\n",
      "epoch 23 loss = 1.433001\n",
      "epoch 24 loss = 1.435135\n",
      "epoch 25 loss = 1.413993\n",
      "epoch 26 loss = 1.392002\n",
      "epoch 27 loss = 1.386918\n",
      "epoch 28 loss = 1.292635\n",
      "epoch 29 loss = 1.342194\n",
      "epoch 30 loss = 1.296417\n",
      "epoch 31 loss = 1.268230\n",
      "epoch 32 loss = 1.251977\n",
      "epoch 33 loss = 1.211061\n",
      "epoch 34 loss = 1.225420\n",
      "epoch 35 loss = 1.194305\n",
      "epoch 36 loss = 1.176480\n",
      "epoch 37 loss = 1.121565\n",
      "epoch 38 loss = 1.116721\n",
      "epoch 39 loss = 1.130380\n",
      "training time = 29.48249912261963 seconds\n",
      "final loss = 1.130380\n",
      "\n",
      "testing time = 26.33944010734558 seconds\n",
      "accuracy_mc = tensor(0.2180)\n",
      "accuracy_non_mc = tensor(0.2170)\n",
      "test_ll_mc = tensor(-0.8084)\n",
      "\n",
      "Training with split 0\n",
      "Training for 400 epochs total.\n",
      "epoch 0 loss = 0.939579\n",
      "epoch 1 loss = 0.897031\n",
      "epoch 2 loss = 0.859413\n",
      "epoch 3 loss = 0.832622\n",
      "epoch 4 loss = 0.811368\n",
      "epoch 5 loss = 0.771624\n",
      "epoch 6 loss = 0.729628\n",
      "epoch 7 loss = 0.695845\n",
      "epoch 8 loss = 0.669824\n",
      "epoch 9 loss = 0.637660\n",
      "epoch 10 loss = 0.611814\n",
      "epoch 11 loss = 0.631518\n",
      "epoch 12 loss = 0.627226\n",
      "epoch 13 loss = 0.567380\n",
      "epoch 14 loss = 0.588685\n",
      "epoch 15 loss = 0.500886\n",
      "epoch 16 loss = 0.520470\n",
      "epoch 17 loss = 0.469743\n",
      "epoch 18 loss = 0.468525\n",
      "epoch 19 loss = 0.420103\n",
      "epoch 20 loss = 0.421302\n",
      "epoch 21 loss = 0.413038\n",
      "epoch 22 loss = 0.378508\n",
      "epoch 23 loss = 0.349975\n",
      "epoch 24 loss = 0.327355\n",
      "epoch 25 loss = 0.316538\n",
      "epoch 26 loss = 0.318366\n",
      "epoch 27 loss = 0.322249\n",
      "epoch 28 loss = 0.306448\n",
      "epoch 29 loss = 0.271631\n",
      "epoch 30 loss = 0.265298\n",
      "epoch 31 loss = 0.259934\n",
      "epoch 32 loss = 0.255710\n",
      "epoch 33 loss = 0.213260\n",
      "epoch 34 loss = 0.211340\n",
      "epoch 35 loss = 0.189267\n",
      "epoch 36 loss = 0.186192\n",
      "epoch 37 loss = 0.178656\n",
      "epoch 38 loss = 0.162637\n",
      "epoch 39 loss = 0.156900\n",
      "epoch 40 loss = 0.140900\n",
      "epoch 41 loss = 0.136563\n",
      "epoch 42 loss = 0.124607\n",
      "epoch 43 loss = 0.122628\n",
      "epoch 44 loss = 0.108726\n",
      "epoch 45 loss = 0.107185\n",
      "epoch 46 loss = 0.098600\n",
      "epoch 47 loss = 0.095390\n",
      "epoch 48 loss = 0.086620\n",
      "epoch 49 loss = 0.083385\n",
      "epoch 50 loss = 0.077460\n",
      "epoch 51 loss = 0.074054\n",
      "epoch 52 loss = 0.067923\n",
      "epoch 53 loss = 0.065008\n",
      "epoch 54 loss = 0.062692\n",
      "epoch 55 loss = 0.058391\n",
      "epoch 56 loss = 0.054454\n",
      "epoch 57 loss = 0.051681\n",
      "epoch 58 loss = 0.049893\n",
      "epoch 59 loss = 0.046579\n",
      "epoch 60 loss = 0.043397\n",
      "epoch 61 loss = 0.042354\n",
      "epoch 62 loss = 0.039760\n",
      "epoch 63 loss = 0.036964\n",
      "epoch 64 loss = 0.035960\n",
      "epoch 65 loss = 0.033283\n",
      "epoch 66 loss = 0.032036\n",
      "epoch 67 loss = 0.030398\n",
      "epoch 68 loss = 0.028559\n",
      "epoch 69 loss = 0.027578\n",
      "epoch 70 loss = 0.026026\n",
      "epoch 71 loss = 0.025138\n",
      "epoch 72 loss = 0.023562\n",
      "epoch 73 loss = 0.022718\n",
      "epoch 74 loss = 0.021675\n",
      "epoch 75 loss = 0.020863\n",
      "epoch 76 loss = 0.019809\n",
      "epoch 77 loss = 0.019106\n",
      "epoch 78 loss = 0.018174\n",
      "epoch 79 loss = 0.017623\n",
      "epoch 80 loss = 0.016864\n",
      "epoch 81 loss = 0.016195\n",
      "epoch 82 loss = 0.015598\n",
      "epoch 83 loss = 0.015082\n",
      "epoch 84 loss = 0.014489\n",
      "epoch 85 loss = 0.013949\n",
      "epoch 86 loss = 0.013514\n",
      "epoch 87 loss = 0.013027\n",
      "epoch 88 loss = 0.012588\n",
      "epoch 89 loss = 0.012185\n",
      "epoch 90 loss = 0.011782\n",
      "epoch 91 loss = 0.011385\n",
      "epoch 92 loss = 0.011031\n",
      "epoch 93 loss = 0.010708\n",
      "epoch 94 loss = 0.010358\n",
      "epoch 95 loss = 0.010040\n",
      "epoch 96 loss = 0.009769\n",
      "epoch 97 loss = 0.009474\n",
      "epoch 98 loss = 0.009183\n",
      "epoch 99 loss = 0.008949\n",
      "epoch 100 loss = 0.008703\n",
      "epoch 101 loss = 0.008451\n",
      "epoch 102 loss = 0.008235\n",
      "epoch 103 loss = 0.008015\n",
      "epoch 104 loss = 0.007800\n",
      "epoch 105 loss = 0.007592\n",
      "epoch 106 loss = 0.007420\n",
      "epoch 107 loss = 0.007230\n",
      "epoch 108 loss = 0.007056\n",
      "epoch 109 loss = 0.006877\n",
      "epoch 110 loss = 0.006708\n",
      "epoch 111 loss = 0.006561\n",
      "epoch 112 loss = 0.006403\n",
      "epoch 113 loss = 0.006252\n",
      "epoch 114 loss = 0.006113\n",
      "epoch 115 loss = 0.005976\n",
      "epoch 116 loss = 0.005834\n",
      "epoch 117 loss = 0.005709\n",
      "epoch 118 loss = 0.005585\n",
      "epoch 119 loss = 0.005460\n",
      "epoch 120 loss = 0.005342\n",
      "epoch 121 loss = 0.005231\n",
      "epoch 122 loss = 0.005119\n",
      "epoch 123 loss = 0.005014\n",
      "epoch 124 loss = 0.004909\n",
      "epoch 125 loss = 0.004809\n",
      "epoch 126 loss = 0.004716\n",
      "epoch 127 loss = 0.004619\n",
      "epoch 128 loss = 0.004529\n",
      "epoch 129 loss = 0.004443\n",
      "epoch 130 loss = 0.004358\n",
      "epoch 131 loss = 0.004275\n",
      "epoch 132 loss = 0.004195\n",
      "epoch 133 loss = 0.004118\n",
      "epoch 134 loss = 0.004036\n",
      "epoch 135 loss = 0.003967\n",
      "epoch 136 loss = 0.003894\n",
      "epoch 137 loss = 0.003826\n",
      "epoch 138 loss = 0.003757\n",
      "epoch 139 loss = 0.003690\n",
      "epoch 140 loss = 0.003625\n",
      "epoch 141 loss = 0.003565\n",
      "epoch 142 loss = 0.003501\n",
      "epoch 143 loss = 0.003444\n",
      "epoch 144 loss = 0.003385\n",
      "epoch 145 loss = 0.003327\n",
      "epoch 146 loss = 0.003275\n",
      "epoch 147 loss = 0.003222\n",
      "epoch 148 loss = 0.003168\n",
      "epoch 149 loss = 0.003119\n",
      "epoch 150 loss = 0.003069\n",
      "epoch 151 loss = 0.003020\n",
      "epoch 152 loss = 0.002974\n",
      "epoch 153 loss = 0.002928\n",
      "epoch 154 loss = 0.002883\n",
      "epoch 155 loss = 0.002837\n",
      "epoch 156 loss = 0.002795\n",
      "epoch 157 loss = 0.002753\n",
      "epoch 158 loss = 0.002710\n",
      "epoch 159 loss = 0.002674\n",
      "epoch 160 loss = 0.002632\n",
      "epoch 161 loss = 0.002593\n",
      "epoch 162 loss = 0.002558\n",
      "epoch 163 loss = 0.002520\n",
      "epoch 164 loss = 0.002482\n",
      "epoch 165 loss = 0.002449\n",
      "epoch 166 loss = 0.002412\n",
      "epoch 167 loss = 0.002381\n",
      "epoch 168 loss = 0.002347\n",
      "epoch 169 loss = 0.002314\n",
      "epoch 170 loss = 0.002284\n",
      "epoch 171 loss = 0.002252\n",
      "epoch 172 loss = 0.002222\n",
      "epoch 173 loss = 0.002192\n",
      "epoch 174 loss = 0.002163\n",
      "epoch 175 loss = 0.002135\n",
      "epoch 176 loss = 0.002107\n",
      "epoch 177 loss = 0.002078\n",
      "epoch 178 loss = 0.002053\n",
      "epoch 179 loss = 0.002027\n",
      "epoch 180 loss = 0.002001\n",
      "epoch 181 loss = 0.001975\n",
      "epoch 182 loss = 0.001949\n",
      "epoch 183 loss = 0.001925\n",
      "epoch 184 loss = 0.001902\n",
      "epoch 185 loss = 0.001877\n",
      "epoch 186 loss = 0.001855\n",
      "epoch 187 loss = 0.001832\n",
      "epoch 188 loss = 0.001810\n",
      "epoch 189 loss = 0.001789\n",
      "epoch 190 loss = 0.001766\n",
      "epoch 191 loss = 0.001745\n",
      "epoch 192 loss = 0.001726\n",
      "epoch 193 loss = 0.001705\n",
      "epoch 194 loss = 0.001684\n",
      "epoch 195 loss = 0.001665\n",
      "epoch 196 loss = 0.001646\n",
      "epoch 197 loss = 0.001626\n",
      "epoch 198 loss = 0.001609\n",
      "epoch 199 loss = 0.001590\n",
      "epoch 200 loss = 0.001572\n",
      "epoch 201 loss = 0.001555\n",
      "epoch 202 loss = 0.001537\n",
      "epoch 203 loss = 0.001519\n",
      "epoch 204 loss = 0.001504\n",
      "epoch 205 loss = 0.001486\n",
      "epoch 206 loss = 0.001470\n",
      "epoch 207 loss = 0.001454\n",
      "epoch 208 loss = 0.001439\n",
      "epoch 209 loss = 0.001423\n",
      "epoch 210 loss = 0.001408\n",
      "epoch 211 loss = 0.001392\n",
      "epoch 212 loss = 0.001378\n",
      "epoch 213 loss = 0.001363\n",
      "epoch 214 loss = 0.001349\n",
      "epoch 215 loss = 0.001335\n",
      "epoch 216 loss = 0.001321\n",
      "epoch 217 loss = 0.001308\n",
      "epoch 218 loss = 0.001294\n",
      "epoch 219 loss = 0.001281\n",
      "epoch 220 loss = 0.001267\n",
      "epoch 221 loss = 0.001255\n",
      "epoch 222 loss = 0.001242\n",
      "epoch 223 loss = 0.001229\n",
      "epoch 224 loss = 0.001217\n",
      "epoch 225 loss = 0.001205\n",
      "epoch 226 loss = 0.001193\n",
      "epoch 227 loss = 0.001181\n",
      "epoch 228 loss = 0.001169\n",
      "epoch 229 loss = 0.001158\n",
      "epoch 230 loss = 0.001147\n",
      "epoch 231 loss = 0.001135\n",
      "epoch 232 loss = 0.001125\n",
      "epoch 233 loss = 0.001113\n",
      "epoch 234 loss = 0.001103\n",
      "epoch 235 loss = 0.001092\n",
      "epoch 236 loss = 0.001082\n",
      "epoch 237 loss = 0.001072\n",
      "epoch 238 loss = 0.001061\n",
      "epoch 239 loss = 0.001052\n",
      "epoch 240 loss = 0.001042\n",
      "epoch 241 loss = 0.001032\n",
      "epoch 242 loss = 0.001022\n",
      "epoch 243 loss = 0.001013\n",
      "epoch 244 loss = 0.001004\n",
      "epoch 245 loss = 0.000995\n",
      "epoch 246 loss = 0.000986\n",
      "epoch 247 loss = 0.000977\n",
      "epoch 248 loss = 0.000968\n",
      "epoch 249 loss = 0.000959\n",
      "epoch 250 loss = 0.000951\n",
      "epoch 251 loss = 0.000942\n",
      "epoch 252 loss = 0.000934\n",
      "epoch 253 loss = 0.000926\n",
      "epoch 254 loss = 0.000917\n",
      "epoch 255 loss = 0.000910\n",
      "epoch 256 loss = 0.000901\n",
      "epoch 257 loss = 0.000894\n",
      "epoch 258 loss = 0.000886\n",
      "epoch 259 loss = 0.000878\n",
      "epoch 260 loss = 0.000871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 261 loss = 0.000863\n",
      "epoch 262 loss = 0.000856\n",
      "epoch 263 loss = 0.000848\n",
      "epoch 264 loss = 0.000841\n",
      "epoch 265 loss = 0.000834\n",
      "epoch 266 loss = 0.000827\n",
      "epoch 267 loss = 0.000820\n",
      "epoch 268 loss = 0.000813\n",
      "epoch 269 loss = 0.000806\n",
      "epoch 270 loss = 0.000800\n",
      "epoch 271 loss = 0.000794\n",
      "epoch 272 loss = 0.000787\n",
      "epoch 273 loss = 0.000781\n",
      "epoch 274 loss = 0.000774\n",
      "epoch 275 loss = 0.000768\n",
      "epoch 276 loss = 0.000762\n",
      "epoch 277 loss = 0.000756\n",
      "epoch 278 loss = 0.000749\n",
      "epoch 279 loss = 0.000744\n",
      "epoch 280 loss = 0.000738\n",
      "epoch 281 loss = 0.000731\n",
      "epoch 282 loss = 0.000726\n",
      "epoch 283 loss = 0.000720\n",
      "epoch 284 loss = 0.000714\n",
      "epoch 285 loss = 0.000709\n",
      "epoch 286 loss = 0.000703\n",
      "epoch 287 loss = 0.000698\n",
      "epoch 288 loss = 0.000692\n",
      "epoch 289 loss = 0.000687\n",
      "epoch 290 loss = 0.000682\n",
      "epoch 291 loss = 0.000676\n",
      "epoch 292 loss = 0.000671\n",
      "epoch 293 loss = 0.000666\n",
      "epoch 294 loss = 0.000661\n",
      "epoch 295 loss = 0.000656\n",
      "epoch 296 loss = 0.000651\n",
      "epoch 297 loss = 0.000646\n",
      "epoch 298 loss = 0.000641\n",
      "epoch 299 loss = 0.000637\n",
      "epoch 300 loss = 0.000632\n",
      "epoch 301 loss = 0.000627\n",
      "epoch 302 loss = 0.000622\n",
      "epoch 303 loss = 0.000618\n",
      "epoch 304 loss = 0.000613\n",
      "epoch 305 loss = 0.000609\n",
      "epoch 306 loss = 0.000604\n",
      "epoch 307 loss = 0.000600\n",
      "epoch 308 loss = 0.000596\n",
      "epoch 309 loss = 0.000591\n",
      "epoch 310 loss = 0.000587\n",
      "epoch 311 loss = 0.000583\n",
      "epoch 312 loss = 0.000578\n",
      "epoch 313 loss = 0.000574\n",
      "epoch 314 loss = 0.000570\n",
      "epoch 315 loss = 0.000566\n",
      "epoch 316 loss = 0.000562\n",
      "epoch 317 loss = 0.000558\n",
      "epoch 318 loss = 0.000554\n",
      "epoch 319 loss = 0.000550\n",
      "epoch 320 loss = 0.000547\n",
      "epoch 321 loss = 0.000543\n",
      "epoch 322 loss = 0.000539\n",
      "epoch 323 loss = 0.000535\n",
      "epoch 324 loss = 0.000531\n",
      "epoch 325 loss = 0.000528\n",
      "epoch 326 loss = 0.000524\n",
      "epoch 327 loss = 0.000520\n",
      "epoch 328 loss = 0.000517\n",
      "epoch 329 loss = 0.000513\n",
      "epoch 330 loss = 0.000510\n",
      "epoch 331 loss = 0.000506\n",
      "epoch 332 loss = 0.000503\n",
      "epoch 333 loss = 0.000500\n",
      "epoch 334 loss = 0.000496\n",
      "epoch 335 loss = 0.000493\n",
      "epoch 336 loss = 0.000490\n",
      "epoch 337 loss = 0.000486\n",
      "epoch 338 loss = 0.000483\n",
      "epoch 339 loss = 0.000480\n",
      "epoch 340 loss = 0.000477\n",
      "epoch 341 loss = 0.000473\n",
      "epoch 342 loss = 0.000470\n",
      "epoch 343 loss = 0.000467\n",
      "epoch 344 loss = 0.000464\n",
      "epoch 345 loss = 0.000461\n",
      "epoch 346 loss = 0.000458\n",
      "epoch 347 loss = 0.000455\n",
      "epoch 348 loss = 0.000452\n",
      "epoch 349 loss = 0.000449\n",
      "epoch 350 loss = 0.000447\n",
      "epoch 351 loss = 0.000444\n",
      "epoch 352 loss = 0.000441\n",
      "epoch 353 loss = 0.000438\n",
      "epoch 354 loss = 0.000435\n",
      "epoch 355 loss = 0.000433\n",
      "epoch 356 loss = 0.000430\n",
      "epoch 357 loss = 0.000427\n",
      "epoch 358 loss = 0.000424\n",
      "epoch 359 loss = 0.000422\n",
      "training time = 220.61053609848022 seconds\n",
      "final loss = 0.000422\n",
      "\n",
      "testing time = 26.80299186706543 seconds\n",
      "accuracy_mc = tensor(0.1990)\n",
      "accuracy_non_mc = tensor(0.2040)\n",
      "test_ll_mc = tensor(-3.6633)\n",
      "\n",
      "Training with split 0\n",
      "Training for 4000 epochs total.\n",
      "epoch 0 loss = 0.000419\n",
      "epoch 1 loss = 0.000416\n",
      "epoch 2 loss = 0.000414\n",
      "epoch 3 loss = 0.000411\n",
      "epoch 4 loss = 0.000409\n",
      "epoch 5 loss = 0.000406\n",
      "epoch 6 loss = 0.000404\n",
      "epoch 7 loss = 0.000401\n",
      "epoch 8 loss = 0.000399\n",
      "epoch 9 loss = 0.000396\n",
      "epoch 10 loss = 0.000394\n",
      "epoch 11 loss = 0.000391\n",
      "epoch 12 loss = 0.000389\n",
      "epoch 13 loss = 0.000387\n",
      "epoch 14 loss = 0.000384\n",
      "epoch 15 loss = 0.000382\n",
      "epoch 16 loss = 0.000380\n",
      "epoch 17 loss = 0.000378\n",
      "epoch 18 loss = 0.000375\n",
      "epoch 19 loss = 0.000373\n",
      "epoch 20 loss = 0.000371\n",
      "epoch 21 loss = 0.000369\n",
      "epoch 22 loss = 0.000366\n",
      "epoch 23 loss = 0.000364\n",
      "epoch 24 loss = 0.000362\n",
      "epoch 25 loss = 0.000360\n",
      "epoch 26 loss = 0.000358\n",
      "epoch 27 loss = 0.000356\n",
      "epoch 28 loss = 0.000354\n",
      "epoch 29 loss = 0.000352\n",
      "epoch 30 loss = 0.000350\n",
      "epoch 31 loss = 0.000347\n",
      "epoch 32 loss = 0.000345\n",
      "epoch 33 loss = 0.000343\n",
      "epoch 34 loss = 0.000341\n",
      "epoch 35 loss = 0.000339\n",
      "epoch 36 loss = 0.000337\n",
      "epoch 37 loss = 0.000336\n",
      "epoch 38 loss = 0.000334\n",
      "epoch 39 loss = 0.000332\n",
      "epoch 40 loss = 0.000330\n",
      "epoch 41 loss = 0.000328\n",
      "epoch 42 loss = 0.000326\n",
      "epoch 43 loss = 0.000324\n",
      "epoch 44 loss = 0.000322\n",
      "epoch 45 loss = 0.000320\n",
      "epoch 46 loss = 0.000319\n",
      "epoch 47 loss = 0.000317\n",
      "epoch 48 loss = 0.000315\n",
      "epoch 49 loss = 0.000313\n",
      "epoch 50 loss = 0.000312\n",
      "epoch 51 loss = 0.000310\n",
      "epoch 52 loss = 0.000308\n",
      "epoch 53 loss = 0.000306\n",
      "epoch 54 loss = 0.000305\n",
      "epoch 55 loss = 0.000303\n",
      "epoch 56 loss = 0.000301\n",
      "epoch 57 loss = 0.000300\n",
      "epoch 58 loss = 0.000298\n",
      "epoch 59 loss = 0.000296\n",
      "epoch 60 loss = 0.000295\n",
      "epoch 61 loss = 0.000293\n",
      "epoch 62 loss = 0.000292\n",
      "epoch 63 loss = 0.000290\n",
      "epoch 64 loss = 0.000288\n",
      "epoch 65 loss = 0.000287\n",
      "epoch 66 loss = 0.000285\n",
      "epoch 67 loss = 0.000284\n",
      "epoch 68 loss = 0.000282\n",
      "epoch 69 loss = 0.000281\n",
      "epoch 70 loss = 0.000279\n",
      "epoch 71 loss = 0.000278\n",
      "epoch 72 loss = 0.000276\n",
      "epoch 73 loss = 0.000275\n",
      "epoch 74 loss = 0.000273\n",
      "epoch 75 loss = 0.000272\n",
      "epoch 76 loss = 0.000270\n",
      "epoch 77 loss = 0.000269\n",
      "epoch 78 loss = 0.000268\n",
      "epoch 79 loss = 0.000266\n",
      "epoch 80 loss = 0.000265\n",
      "epoch 81 loss = 0.000263\n",
      "epoch 82 loss = 0.000262\n",
      "epoch 83 loss = 0.000261\n",
      "epoch 84 loss = 0.000259\n",
      "epoch 85 loss = 0.000258\n",
      "epoch 86 loss = 0.000257\n",
      "epoch 87 loss = 0.000255\n",
      "epoch 88 loss = 0.000254\n",
      "epoch 89 loss = 0.000253\n",
      "epoch 90 loss = 0.000251\n",
      "epoch 91 loss = 0.000250\n",
      "epoch 92 loss = 0.000249\n",
      "epoch 93 loss = 0.000247\n",
      "epoch 94 loss = 0.000246\n",
      "epoch 95 loss = 0.000245\n",
      "epoch 96 loss = 0.000244\n",
      "epoch 97 loss = 0.000242\n",
      "epoch 98 loss = 0.000241\n",
      "epoch 99 loss = 0.000240\n",
      "epoch 100 loss = 0.000239\n",
      "epoch 101 loss = 0.000238\n",
      "epoch 102 loss = 0.000236\n",
      "epoch 103 loss = 0.000235\n",
      "epoch 104 loss = 0.000234\n",
      "epoch 105 loss = 0.000233\n",
      "epoch 106 loss = 0.000232\n",
      "epoch 107 loss = 0.000230\n",
      "epoch 108 loss = 0.000229\n",
      "epoch 109 loss = 0.000228\n",
      "epoch 110 loss = 0.000227\n",
      "epoch 111 loss = 0.000226\n",
      "epoch 112 loss = 0.000225\n",
      "epoch 113 loss = 0.000224\n",
      "epoch 114 loss = 0.000223\n",
      "epoch 115 loss = 0.000221\n",
      "epoch 116 loss = 0.000220\n",
      "epoch 117 loss = 0.000219\n",
      "epoch 118 loss = 0.000218\n",
      "epoch 119 loss = 0.000217\n",
      "epoch 120 loss = 0.000216\n",
      "epoch 121 loss = 0.000215\n",
      "epoch 122 loss = 0.000214\n",
      "epoch 123 loss = 0.000213\n",
      "epoch 124 loss = 0.000212\n",
      "epoch 125 loss = 0.000211\n",
      "epoch 126 loss = 0.000210\n",
      "epoch 127 loss = 0.000209\n",
      "epoch 128 loss = 0.000208\n",
      "epoch 129 loss = 0.000207\n",
      "epoch 130 loss = 0.000206\n",
      "epoch 131 loss = 0.000205\n",
      "epoch 132 loss = 0.000204\n",
      "epoch 133 loss = 0.000203\n",
      "epoch 134 loss = 0.000202\n",
      "epoch 135 loss = 0.000201\n",
      "epoch 136 loss = 0.000200\n",
      "epoch 137 loss = 0.000199\n",
      "epoch 138 loss = 0.000198\n",
      "epoch 139 loss = 0.000197\n",
      "epoch 140 loss = 0.000196\n",
      "epoch 141 loss = 0.000195\n",
      "epoch 142 loss = 0.000194\n",
      "epoch 143 loss = 0.000193\n",
      "epoch 144 loss = 0.000193\n",
      "epoch 145 loss = 0.000192\n",
      "epoch 146 loss = 0.000191\n",
      "epoch 147 loss = 0.000190\n",
      "epoch 148 loss = 0.000189\n",
      "epoch 149 loss = 0.000188\n",
      "epoch 150 loss = 0.000187\n",
      "epoch 151 loss = 0.000186\n",
      "epoch 152 loss = 0.000186\n",
      "epoch 153 loss = 0.000185\n",
      "epoch 154 loss = 0.000184\n",
      "epoch 155 loss = 0.000183\n",
      "epoch 156 loss = 0.000182\n",
      "epoch 157 loss = 0.000181\n",
      "epoch 158 loss = 0.000180\n",
      "epoch 159 loss = 0.000180\n",
      "epoch 160 loss = 0.000179\n",
      "epoch 161 loss = 0.000178\n",
      "epoch 162 loss = 0.000177\n",
      "epoch 163 loss = 0.000176\n",
      "epoch 164 loss = 0.000176\n",
      "epoch 165 loss = 0.000175\n",
      "epoch 166 loss = 0.000174\n",
      "epoch 167 loss = 0.000173\n",
      "epoch 168 loss = 0.000172\n",
      "epoch 169 loss = 0.000172\n",
      "epoch 170 loss = 0.000171\n",
      "epoch 171 loss = 0.000170\n",
      "epoch 172 loss = 0.000169\n",
      "epoch 173 loss = 0.000169\n",
      "epoch 174 loss = 0.000168\n",
      "epoch 175 loss = 0.000167\n",
      "epoch 176 loss = 0.000166\n",
      "epoch 177 loss = 0.000166\n",
      "epoch 178 loss = 0.000165\n",
      "epoch 179 loss = 0.000164\n",
      "epoch 180 loss = 0.000163\n",
      "epoch 181 loss = 0.000163\n",
      "epoch 182 loss = 0.000162\n",
      "epoch 183 loss = 0.000161\n",
      "epoch 184 loss = 0.000161\n",
      "epoch 185 loss = 0.000160\n",
      "epoch 186 loss = 0.000159\n",
      "epoch 187 loss = 0.000158\n",
      "epoch 188 loss = 0.000158\n",
      "epoch 189 loss = 0.000157\n",
      "epoch 190 loss = 0.000156\n",
      "epoch 191 loss = 0.000156\n",
      "epoch 192 loss = 0.000155\n",
      "epoch 193 loss = 0.000154\n",
      "epoch 194 loss = 0.000154\n",
      "epoch 195 loss = 0.000153\n",
      "epoch 196 loss = 0.000152\n",
      "epoch 197 loss = 0.000152\n",
      "epoch 198 loss = 0.000151\n",
      "epoch 199 loss = 0.000150\n",
      "epoch 200 loss = 0.000150\n",
      "epoch 201 loss = 0.000149\n",
      "epoch 202 loss = 0.000148\n",
      "epoch 203 loss = 0.000148\n",
      "epoch 204 loss = 0.000147\n",
      "epoch 205 loss = 0.000146\n",
      "epoch 206 loss = 0.000146\n",
      "epoch 207 loss = 0.000145\n",
      "epoch 208 loss = 0.000145\n",
      "epoch 209 loss = 0.000144\n",
      "epoch 210 loss = 0.000143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 211 loss = 0.000143\n",
      "epoch 212 loss = 0.000142\n",
      "epoch 213 loss = 0.000142\n",
      "epoch 214 loss = 0.000141\n",
      "epoch 215 loss = 0.000140\n",
      "epoch 216 loss = 0.000140\n",
      "epoch 217 loss = 0.000139\n",
      "epoch 218 loss = 0.000139\n",
      "epoch 219 loss = 0.000138\n",
      "epoch 220 loss = 0.000137\n",
      "epoch 221 loss = 0.000137\n",
      "epoch 222 loss = 0.000136\n",
      "epoch 223 loss = 0.000136\n",
      "epoch 224 loss = 0.000135\n",
      "epoch 225 loss = 0.000135\n",
      "epoch 226 loss = 0.000134\n",
      "epoch 227 loss = 0.000133\n",
      "epoch 228 loss = 0.000133\n",
      "epoch 229 loss = 0.000132\n",
      "epoch 230 loss = 0.000132\n",
      "epoch 231 loss = 0.000131\n",
      "epoch 232 loss = 0.000131\n",
      "epoch 233 loss = 0.000130\n",
      "epoch 234 loss = 0.000130\n",
      "epoch 235 loss = 0.000129\n",
      "epoch 236 loss = 0.000129\n",
      "epoch 237 loss = 0.000128\n",
      "epoch 238 loss = 0.000127\n",
      "epoch 239 loss = 0.000127\n",
      "epoch 240 loss = 0.000126\n",
      "epoch 241 loss = 0.000126\n",
      "epoch 242 loss = 0.000125\n",
      "epoch 243 loss = 0.000125\n",
      "epoch 244 loss = 0.000124\n",
      "epoch 245 loss = 0.000124\n",
      "epoch 246 loss = 0.000123\n",
      "epoch 247 loss = 0.000123\n",
      "epoch 248 loss = 0.000122\n",
      "epoch 249 loss = 0.000122\n",
      "epoch 250 loss = 0.000121\n",
      "epoch 251 loss = 0.000121\n",
      "epoch 252 loss = 0.000120\n",
      "epoch 253 loss = 0.000120\n",
      "epoch 254 loss = 0.000119\n",
      "epoch 255 loss = 0.000119\n",
      "epoch 256 loss = 0.000119\n",
      "epoch 257 loss = 0.000118\n",
      "epoch 258 loss = 0.000118\n",
      "epoch 259 loss = 0.000117\n",
      "epoch 260 loss = 0.000117\n",
      "epoch 261 loss = 0.000116\n",
      "epoch 262 loss = 0.000116\n",
      "epoch 263 loss = 0.000115\n",
      "epoch 264 loss = 0.000115\n",
      "epoch 265 loss = 0.000114\n",
      "epoch 266 loss = 0.000114\n",
      "epoch 267 loss = 0.000113\n",
      "epoch 268 loss = 0.000113\n",
      "epoch 269 loss = 0.000112\n",
      "epoch 270 loss = 0.000112\n",
      "epoch 271 loss = 0.000112\n",
      "epoch 272 loss = 0.000111\n",
      "epoch 273 loss = 0.000111\n",
      "epoch 274 loss = 0.000110\n",
      "epoch 275 loss = 0.000110\n",
      "epoch 276 loss = 0.000109\n",
      "epoch 277 loss = 0.000109\n",
      "epoch 278 loss = 0.000109\n",
      "epoch 279 loss = 0.000108\n",
      "epoch 280 loss = 0.000108\n",
      "epoch 281 loss = 0.000107\n",
      "epoch 282 loss = 0.000107\n",
      "epoch 283 loss = 0.000106\n",
      "epoch 284 loss = 0.000106\n",
      "epoch 285 loss = 0.000106\n",
      "epoch 286 loss = 0.000105\n",
      "epoch 287 loss = 0.000105\n",
      "epoch 288 loss = 0.000104\n",
      "epoch 289 loss = 0.000104\n",
      "epoch 290 loss = 0.000104\n",
      "epoch 291 loss = 0.000103\n",
      "epoch 292 loss = 0.000103\n",
      "epoch 293 loss = 0.000102\n",
      "epoch 294 loss = 0.000102\n",
      "epoch 295 loss = 0.000102\n",
      "epoch 296 loss = 0.000101\n",
      "epoch 297 loss = 0.000101\n",
      "epoch 298 loss = 0.000100\n",
      "epoch 299 loss = 0.000100\n",
      "epoch 300 loss = 0.000100\n",
      "epoch 301 loss = 0.000099\n",
      "epoch 302 loss = 0.000099\n",
      "epoch 303 loss = 0.000099\n",
      "epoch 304 loss = 0.000098\n",
      "epoch 305 loss = 0.000098\n",
      "epoch 306 loss = 0.000097\n",
      "epoch 307 loss = 0.000097\n",
      "epoch 308 loss = 0.000097\n",
      "epoch 309 loss = 0.000096\n",
      "epoch 310 loss = 0.000096\n",
      "epoch 311 loss = 0.000096\n",
      "epoch 312 loss = 0.000095\n",
      "epoch 313 loss = 0.000095\n",
      "epoch 314 loss = 0.000095\n",
      "epoch 315 loss = 0.000094\n",
      "epoch 316 loss = 0.000094\n",
      "epoch 317 loss = 0.000094\n",
      "epoch 318 loss = 0.000093\n",
      "epoch 319 loss = 0.000093\n",
      "epoch 320 loss = 0.000092\n",
      "epoch 321 loss = 0.000092\n",
      "epoch 322 loss = 0.000092\n",
      "epoch 323 loss = 0.000091\n",
      "epoch 324 loss = 0.000091\n",
      "epoch 325 loss = 0.000091\n",
      "epoch 326 loss = 0.000090\n",
      "epoch 327 loss = 0.000090\n",
      "epoch 328 loss = 0.000090\n",
      "epoch 329 loss = 0.000089\n",
      "epoch 330 loss = 0.000089\n",
      "epoch 331 loss = 0.000089\n",
      "epoch 332 loss = 0.000088\n",
      "epoch 333 loss = 0.000088\n",
      "epoch 334 loss = 0.000088\n",
      "epoch 335 loss = 0.000087\n",
      "epoch 336 loss = 0.000087\n",
      "epoch 337 loss = 0.000087\n",
      "epoch 338 loss = 0.000086\n",
      "epoch 339 loss = 0.000086\n",
      "epoch 340 loss = 0.000086\n",
      "epoch 341 loss = 0.000086\n",
      "epoch 342 loss = 0.000085\n",
      "epoch 343 loss = 0.000085\n",
      "epoch 344 loss = 0.000085\n",
      "epoch 345 loss = 0.000084\n",
      "epoch 346 loss = 0.000084\n",
      "epoch 347 loss = 0.000084\n",
      "epoch 348 loss = 0.000083\n",
      "epoch 349 loss = 0.000083\n",
      "epoch 350 loss = 0.000083\n",
      "epoch 351 loss = 0.000082\n",
      "epoch 352 loss = 0.000082\n",
      "epoch 353 loss = 0.000082\n",
      "epoch 354 loss = 0.000082\n",
      "epoch 355 loss = 0.000081\n",
      "epoch 356 loss = 0.000081\n",
      "epoch 357 loss = 0.000081\n",
      "epoch 358 loss = 0.000080\n",
      "epoch 359 loss = 0.000080\n",
      "epoch 360 loss = 0.000080\n",
      "epoch 361 loss = 0.000080\n",
      "epoch 362 loss = 0.000079\n",
      "epoch 363 loss = 0.000079\n",
      "epoch 364 loss = 0.000079\n",
      "epoch 365 loss = 0.000078\n",
      "epoch 366 loss = 0.000078\n",
      "epoch 367 loss = 0.000078\n",
      "epoch 368 loss = 0.000078\n",
      "epoch 369 loss = 0.000077\n",
      "epoch 370 loss = 0.000077\n",
      "epoch 371 loss = 0.000077\n",
      "epoch 372 loss = 0.000076\n",
      "epoch 373 loss = 0.000076\n",
      "epoch 374 loss = 0.000076\n",
      "epoch 375 loss = 0.000076\n",
      "epoch 376 loss = 0.000075\n",
      "epoch 377 loss = 0.000075\n",
      "epoch 378 loss = 0.000075\n",
      "epoch 379 loss = 0.000075\n",
      "epoch 380 loss = 0.000074\n",
      "epoch 381 loss = 0.000074\n",
      "epoch 382 loss = 0.000074\n",
      "epoch 383 loss = 0.000074\n",
      "epoch 384 loss = 0.000073\n",
      "epoch 385 loss = 0.000073\n",
      "epoch 386 loss = 0.000073\n",
      "epoch 387 loss = 0.000073\n",
      "epoch 388 loss = 0.000072\n",
      "epoch 389 loss = 0.000072\n",
      "epoch 390 loss = 0.000072\n",
      "epoch 391 loss = 0.000072\n",
      "epoch 392 loss = 0.000071\n",
      "epoch 393 loss = 0.000071\n",
      "epoch 394 loss = 0.000071\n",
      "epoch 395 loss = 0.000071\n",
      "epoch 396 loss = 0.000070\n",
      "epoch 397 loss = 0.000070\n",
      "epoch 398 loss = 0.000070\n",
      "epoch 399 loss = 0.000070\n",
      "epoch 400 loss = 0.000069\n",
      "epoch 401 loss = 0.000069\n",
      "epoch 402 loss = 0.000069\n",
      "epoch 403 loss = 0.000069\n",
      "epoch 404 loss = 0.000068\n",
      "epoch 405 loss = 0.000068\n",
      "epoch 406 loss = 0.000068\n",
      "epoch 407 loss = 0.000068\n",
      "epoch 408 loss = 0.000067\n",
      "epoch 409 loss = 0.000067\n",
      "epoch 410 loss = 0.000067\n",
      "epoch 411 loss = 0.000067\n",
      "epoch 412 loss = 0.000067\n",
      "epoch 413 loss = 0.000066\n",
      "epoch 414 loss = 0.000066\n",
      "epoch 415 loss = 0.000066\n",
      "epoch 416 loss = 0.000066\n",
      "epoch 417 loss = 0.000065\n",
      "epoch 418 loss = 0.000065\n",
      "epoch 419 loss = 0.000065\n",
      "epoch 420 loss = 0.000065\n",
      "epoch 421 loss = 0.000064\n",
      "epoch 422 loss = 0.000064\n",
      "epoch 423 loss = 0.000064\n",
      "epoch 424 loss = 0.000064\n",
      "epoch 425 loss = 0.000064\n",
      "epoch 426 loss = 0.000063\n",
      "epoch 427 loss = 0.000063\n",
      "epoch 428 loss = 0.000063\n",
      "epoch 429 loss = 0.000063\n",
      "epoch 430 loss = 0.000063\n",
      "epoch 431 loss = 0.000062\n",
      "epoch 432 loss = 0.000062\n",
      "epoch 433 loss = 0.000062\n",
      "epoch 434 loss = 0.000062\n",
      "epoch 435 loss = 0.000062\n",
      "epoch 436 loss = 0.000061\n",
      "epoch 437 loss = 0.000061\n",
      "epoch 438 loss = 0.000061\n",
      "epoch 439 loss = 0.000061\n",
      "epoch 440 loss = 0.000060\n",
      "epoch 441 loss = 0.000060\n",
      "epoch 442 loss = 0.000060\n",
      "epoch 443 loss = 0.000060\n",
      "epoch 444 loss = 0.000060\n",
      "epoch 445 loss = 0.000059\n",
      "epoch 446 loss = 0.000059\n",
      "epoch 447 loss = 0.000059\n",
      "epoch 448 loss = 0.000059\n",
      "epoch 449 loss = 0.000059\n",
      "epoch 450 loss = 0.000058\n",
      "epoch 451 loss = 0.000058\n",
      "epoch 452 loss = 0.000058\n",
      "epoch 453 loss = 0.000058\n",
      "epoch 454 loss = 0.000058\n",
      "epoch 455 loss = 0.000058\n",
      "epoch 456 loss = 0.000057\n",
      "epoch 457 loss = 0.000057\n",
      "epoch 458 loss = 0.000057\n",
      "epoch 459 loss = 0.000057\n",
      "epoch 460 loss = 0.000057\n",
      "epoch 461 loss = 0.000056\n",
      "epoch 462 loss = 0.000056\n",
      "epoch 463 loss = 0.000056\n",
      "epoch 464 loss = 0.000056\n",
      "epoch 465 loss = 0.000056\n",
      "epoch 466 loss = 0.000055\n",
      "epoch 467 loss = 0.000055\n",
      "epoch 468 loss = 0.000055\n",
      "epoch 469 loss = 0.000055\n",
      "epoch 470 loss = 0.000055\n",
      "epoch 471 loss = 0.000055\n",
      "epoch 472 loss = 0.000054\n",
      "epoch 473 loss = 0.000054\n",
      "epoch 474 loss = 0.000054\n",
      "epoch 475 loss = 0.000054\n",
      "epoch 476 loss = 0.000054\n",
      "epoch 477 loss = 0.000054\n",
      "epoch 478 loss = 0.000053\n",
      "epoch 479 loss = 0.000053\n",
      "epoch 480 loss = 0.000053\n",
      "epoch 481 loss = 0.000053\n",
      "epoch 482 loss = 0.000053\n",
      "epoch 483 loss = 0.000053\n",
      "epoch 484 loss = 0.000052\n",
      "epoch 485 loss = 0.000052\n",
      "epoch 486 loss = 0.000052\n",
      "epoch 487 loss = 0.000052\n",
      "epoch 488 loss = 0.000052\n",
      "epoch 489 loss = 0.000052\n",
      "epoch 490 loss = 0.000051\n",
      "epoch 491 loss = 0.000051\n",
      "epoch 492 loss = 0.000051\n",
      "epoch 493 loss = 0.000051\n",
      "epoch 494 loss = 0.000051\n",
      "epoch 495 loss = 0.000051\n",
      "epoch 496 loss = 0.000050\n",
      "epoch 497 loss = 0.000050\n",
      "epoch 498 loss = 0.000050\n",
      "epoch 499 loss = 0.000050\n",
      "epoch 500 loss = 0.000050\n",
      "epoch 501 loss = 0.000050\n",
      "epoch 502 loss = 0.000049\n",
      "epoch 503 loss = 0.000049\n",
      "epoch 504 loss = 0.000049\n",
      "epoch 505 loss = 0.000049\n",
      "epoch 506 loss = 0.000049\n",
      "epoch 507 loss = 0.000049\n",
      "epoch 508 loss = 0.000048\n",
      "epoch 509 loss = 0.000048\n",
      "epoch 510 loss = 0.000048\n",
      "epoch 511 loss = 0.000048\n",
      "epoch 512 loss = 0.000048\n",
      "epoch 513 loss = 0.000048\n",
      "epoch 514 loss = 0.000048\n",
      "epoch 515 loss = 0.000047\n",
      "epoch 516 loss = 0.000047\n",
      "epoch 517 loss = 0.000047\n",
      "epoch 518 loss = 0.000047\n",
      "epoch 519 loss = 0.000047\n",
      "epoch 520 loss = 0.000047\n",
      "epoch 521 loss = 0.000046\n",
      "epoch 522 loss = 0.000046\n",
      "epoch 523 loss = 0.000046\n",
      "epoch 524 loss = 0.000046\n",
      "epoch 525 loss = 0.000046\n",
      "epoch 526 loss = 0.000046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 527 loss = 0.000046\n",
      "epoch 528 loss = 0.000045\n",
      "epoch 529 loss = 0.000045\n",
      "epoch 530 loss = 0.000045\n",
      "epoch 531 loss = 0.000045\n",
      "epoch 532 loss = 0.000045\n",
      "epoch 533 loss = 0.000045\n",
      "epoch 534 loss = 0.000045\n",
      "epoch 535 loss = 0.000045\n",
      "epoch 536 loss = 0.000044\n",
      "epoch 537 loss = 0.000044\n",
      "epoch 538 loss = 0.000044\n",
      "epoch 539 loss = 0.000044\n",
      "epoch 540 loss = 0.000044\n",
      "epoch 541 loss = 0.000044\n",
      "epoch 542 loss = 0.000044\n",
      "epoch 543 loss = 0.000043\n",
      "epoch 544 loss = 0.000043\n",
      "epoch 545 loss = 0.000043\n",
      "epoch 546 loss = 0.000043\n",
      "epoch 547 loss = 0.000043\n",
      "epoch 548 loss = 0.000043\n",
      "epoch 549 loss = 0.000043\n",
      "epoch 550 loss = 0.000042\n",
      "epoch 551 loss = 0.000042\n",
      "epoch 552 loss = 0.000042\n",
      "epoch 553 loss = 0.000042\n",
      "epoch 554 loss = 0.000042\n",
      "epoch 555 loss = 0.000042\n",
      "epoch 556 loss = 0.000042\n",
      "epoch 557 loss = 0.000042\n",
      "epoch 558 loss = 0.000041\n",
      "epoch 559 loss = 0.000041\n",
      "epoch 560 loss = 0.000041\n",
      "epoch 561 loss = 0.000041\n",
      "epoch 562 loss = 0.000041\n",
      "epoch 563 loss = 0.000041\n",
      "epoch 564 loss = 0.000041\n",
      "epoch 565 loss = 0.000041\n",
      "epoch 566 loss = 0.000040\n",
      "epoch 567 loss = 0.000040\n",
      "epoch 568 loss = 0.000040\n",
      "epoch 569 loss = 0.000040\n",
      "epoch 570 loss = 0.000040\n",
      "epoch 571 loss = 0.000040\n",
      "epoch 572 loss = 0.000040\n",
      "epoch 573 loss = 0.000040\n",
      "epoch 574 loss = 0.000039\n",
      "epoch 575 loss = 0.000039\n",
      "epoch 576 loss = 0.000039\n",
      "epoch 577 loss = 0.000039\n",
      "epoch 578 loss = 0.000039\n",
      "epoch 579 loss = 0.000039\n",
      "epoch 580 loss = 0.000039\n",
      "epoch 581 loss = 0.000039\n",
      "epoch 582 loss = 0.000038\n",
      "epoch 583 loss = 0.000038\n",
      "epoch 584 loss = 0.000038\n",
      "epoch 585 loss = 0.000038\n",
      "epoch 586 loss = 0.000038\n",
      "epoch 587 loss = 0.000038\n",
      "epoch 588 loss = 0.000038\n",
      "epoch 589 loss = 0.000038\n",
      "epoch 590 loss = 0.000038\n",
      "epoch 591 loss = 0.000037\n",
      "epoch 592 loss = 0.000037\n",
      "epoch 593 loss = 0.000037\n",
      "epoch 594 loss = 0.000037\n",
      "epoch 595 loss = 0.000037\n",
      "epoch 596 loss = 0.000037\n",
      "epoch 597 loss = 0.000037\n",
      "epoch 598 loss = 0.000037\n",
      "epoch 599 loss = 0.000037\n",
      "epoch 600 loss = 0.000036\n",
      "epoch 601 loss = 0.000036\n",
      "epoch 602 loss = 0.000036\n",
      "epoch 603 loss = 0.000036\n",
      "epoch 604 loss = 0.000036\n",
      "epoch 605 loss = 0.000036\n",
      "epoch 606 loss = 0.000036\n",
      "epoch 607 loss = 0.000036\n",
      "epoch 608 loss = 0.000036\n",
      "epoch 609 loss = 0.000035\n",
      "epoch 610 loss = 0.000035\n",
      "epoch 611 loss = 0.000035\n",
      "epoch 612 loss = 0.000035\n",
      "epoch 613 loss = 0.000035\n",
      "epoch 614 loss = 0.000035\n",
      "epoch 615 loss = 0.000035\n",
      "epoch 616 loss = 0.000035\n",
      "epoch 617 loss = 0.000035\n",
      "epoch 618 loss = 0.000035\n",
      "epoch 619 loss = 0.000034\n",
      "epoch 620 loss = 0.000034\n",
      "epoch 621 loss = 0.000034\n",
      "epoch 622 loss = 0.000034\n",
      "epoch 623 loss = 0.000034\n",
      "epoch 624 loss = 0.000034\n",
      "epoch 625 loss = 0.000034\n",
      "epoch 626 loss = 0.000034\n",
      "epoch 627 loss = 0.000034\n"
     ]
    }
   ],
   "source": [
    "for subset_prop in subset_proportions:\n",
    "    \n",
    "    \"\"\"\n",
    "    Dataset multiple splits prep\n",
    "    \"\"\"\n",
    "    # Prepare new subset of the original dataset\n",
    "    subset = datasets.CIFAR10(\n",
    "        root='./datasets_files', limit_size=subset_prop, transform=transform, download=True)\n",
    "\n",
    "    # Determine sizes of training and testing set\n",
    "    train_size = int(dataset_train_size * len(subset))\n",
    "    test_size = len(subset) - train_size\n",
    "\n",
    "    # Print the size of the subset\n",
    "    print(\"subset size = \" + str(subset.data.shape))\n",
    "    print(\"training set size = %d\" % train_size)\n",
    "    print(\"test set size = %d\" % test_size)\n",
    "    print()\n",
    "\n",
    "    # Prepare multiple sets of random train-test splits \n",
    "    # to test the parameter combination\n",
    "    subset_splits = []\n",
    "\n",
    "    for _ in range(n_splits):\n",
    "        train, test = random_split(subset, lengths=[train_size, test_size])\n",
    "        subset_splits.append((train, test))\n",
    "\n",
    "    # With all the splits, test out each combination of hyperparameters\n",
    "    for dropout_rate, length_scale, tau in itertools.product(\n",
    "        dropout_rates, length_scale_values, tau_values,\n",
    "    ):  \n",
    "        # Reset the random number generator for each method (to produce identical results)\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        pyro.set_rng_seed(random_seed)\n",
    "\n",
    "        # Print parameter combinations being tested\n",
    "        print(\n",
    "            \"subset %f, dropout_rate %f, length_scale %f, tau %f\"\n",
    "            % (subset_prop, dropout_rate, length_scale, tau))\n",
    "\n",
    "        \"\"\"\n",
    "        Training & testing\n",
    "        \"\"\"\n",
    "\n",
    "        # Try learning with different splits\n",
    "        for s, (train, test) in enumerate(subset_splits):\n",
    "            train_loader = DataLoader(train, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "            # Prepare network\n",
    "            network = models.SimpleCIFAR10(\n",
    "                dropout_rate=dropout_rate,\n",
    "                dropout_type='bernoulli',\n",
    "            )\n",
    "\n",
    "            # Send the whole model to the selected torch.device\n",
    "            network.to(torch_device)\n",
    "\n",
    "            # Model to train mode\n",
    "            network.train()\n",
    "\n",
    "            # Adam optimizer\n",
    "            # https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam\n",
    "\n",
    "            # NOTE: Need to set L2 regularization from here\n",
    "            reg_strength = utils.reg_strength(dropout_rate, length_scale, train_size, tau)\n",
    "\n",
    "            print('reg_strength = ' + str(reg_strength))\n",
    "\n",
    "            optimizer = optim.Adam(\n",
    "                network.parameters(),\n",
    "                lr=optimizer_learning_rate,\n",
    "                weight_decay=reg_strength, # L2 regularization\n",
    "            )\n",
    "\n",
    "            accumulated_epochs = 0\n",
    "\n",
    "            for n_epoch in n_epoch_values:\n",
    "\n",
    "                \"\"\"\n",
    "                Training\n",
    "                \"\"\"\n",
    "\n",
    "                print('Training with split %d' % s)\n",
    "                print(\"Training for %d epochs total.\" % n_epoch)\n",
    "            \n",
    "                # Record training start time (for this split)\n",
    "                tic = time.time()\n",
    "\n",
    "                for epoch in range(n_epoch-accumulated_epochs): # loop over the dataset multiple times\n",
    "                    # Mini-batches\n",
    "                    for data in train_loader:\n",
    "                        # get the inputs; data is a list of [inputs, labels]\n",
    "                        inputs, targets = data\n",
    "\n",
    "                        # Store the batch to torch_device's memory\n",
    "                        inputs = inputs.to(torch_device)\n",
    "                        targets = targets.to(torch_device)\n",
    "\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = network(inputs)\n",
    "\n",
    "                        loss = objective(outputs, targets)\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "\n",
    "                    print(\"epoch %d loss = %f\" % (epoch, loss.item()))\n",
    "\n",
    "                # Record training end time\n",
    "                toc = time.time()\n",
    "                \n",
    "                # Track the number of epochs done so far\n",
    "                accumulated_epochs += (n_epoch - accumulated_epochs)\n",
    "                \n",
    "                # Report the total training time\n",
    "                print(\"training time = \" + str(toc - tic) + \" seconds\")\n",
    "                \n",
    "                # Report the final loss\n",
    "                print(\"final loss = %f\" % (loss.item()))\n",
    "                print()\n",
    "\n",
    "                \"\"\"\n",
    "                Testing\n",
    "                \"\"\"\n",
    "\n",
    "                # Model to eval mode\n",
    "                network.eval()\n",
    "\n",
    "                # Store the batch to torch_device's memory\n",
    "                test_loader = DataLoader(test, batch_size=n_training_batch, pin_memory=use_pin_memory)\n",
    "\n",
    "                # Record testing start time\n",
    "                tic_testing = time.time()\n",
    "\n",
    "                _, mean, metrics = network.predict_dist(test_loader, n_prediction)\n",
    "\n",
    "                # Record testing end time\n",
    "                toc_testing = time.time()\n",
    "                \n",
    "                # Report the total testing time\n",
    "                print(\"testing time = \" + str(toc_testing - tic_testing) + \" seconds\")\n",
    "\n",
    "                # Record all the scores to the score files\n",
    "                \"\"\"\n",
    "                Results file storage\n",
    "                \"\"\"\n",
    "\n",
    "                # Create directory to store results for the current test configuration\n",
    "                test_results_path = os.path.join(\n",
    "                    './test_results',\n",
    "                    'error_convergence_2',\n",
    "                    'CIFAR-10',\n",
    "                    test_start_time,\n",
    "                    (\n",
    "                        str(subset_prop)\n",
    "                        + '_' + str(dropout_rate) \n",
    "                        + '_' + str(length_scale)\n",
    "                        + '_' + str(tau)\n",
    "                        + '_' + str(n_epoch)),\n",
    "                )\n",
    "\n",
    "                os.makedirs(test_results_path, exist_ok=True)\n",
    "\n",
    "                test_results_accuracy_mc_path = os.path.join(\n",
    "                    test_results_path,\n",
    "                    \"accuracy_mc.txt\"\n",
    "                )\n",
    "\n",
    "                test_results_accuracy_non_mc_path = os.path.join(\n",
    "                    test_results_path,\n",
    "                    \"accuracy_non_mc.txt\"\n",
    "                )\n",
    "\n",
    "                test_results_lls_mc_path = os.path.join(\n",
    "                    test_results_path,\n",
    "                    \"lls_mc.txt\"\n",
    "                )\n",
    "\n",
    "                if len(metrics) > 0:\n",
    "                    for key, value in metrics.items():\n",
    "                        print(str(key) + \" = \" + str(value))\n",
    "\n",
    "                        if key == 'accuracy_mc':\n",
    "                            with open(test_results_accuracy_mc_path, 'a+') as accuracy_mc_file:\n",
    "                                accuracy_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "                        elif key == 'accuracy_non_mc':\n",
    "                            with open(test_results_accuracy_non_mc_path, 'a+') as accuracy_non_mc_file:\n",
    "                                accuracy_non_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "                        elif key == 'test_ll_mc':\n",
    "                            with open(test_results_lls_mc_path, 'a+') as lls_mc_file:\n",
    "                                lls_mc_file.write('%d %f \\n' % (s, value))\n",
    "\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whZbn3VTTUFa"
   },
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDxkRM5aVrdf"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "experiment_root_directory = os.path.join(\n",
    "    './test_results',\n",
    "    'error_convergence_2',\n",
    "    'CIFAR-10',\n",
    "    test_start_time,\n",
    ")\n",
    "\n",
    "for subset_prop, dropout_rate, length_scale, tau, n_epoch in itertools.product(\n",
    "    subset_proportions, dropout_rates, length_scale_values, tau_values, n_epochs\n",
    "):\n",
    "    for metric_name in ['lls_mc', 'accuracy_mc', 'accuracy_non_mc']:\n",
    "        figure_file_name = (\n",
    "            '3d_' + str(subset_prop)\n",
    "            + '_' + str(dropout_rate)\n",
    "            + '_' + str(length_scale)\n",
    "            + '_' + str(tau)\n",
    "            + '_' + str(n_epoch)\n",
    "            + '_' + metric_name + '.png'\n",
    "        )\n",
    "        \n",
    "        figure_title = (\n",
    "            metric_name \n",
    "            + (' subset %f, dropout rate = %f, length_scale %f, tau %f, n_epoch = %d' \n",
    "               % (subset_prop, dropout_rate, length_scale, tau, n_epoch))\n",
    "        )\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        fig.tight_layout(pad=2, rect=[0, 0.00, 2, 2])\n",
    "        \n",
    "        hidden_dims_data = []\n",
    "        hidden_layers_data = []\n",
    "        scores_data = []\n",
    "        \n",
    "        for hidden_dim in network_hidden_dims:\n",
    "            for n_layer in network_hidden_layers:\n",
    "                # Open the score file\n",
    "                score_file_path = os.path.join(\n",
    "                    experiment_root_directory,\n",
    "                    (\n",
    "                        str(subset_prop) \n",
    "                        + '_' + str(hidden_dim)\n",
    "                        + '_' + str(n_layer) \n",
    "                        + '_' + str(dropout_rate) \n",
    "                        + '_' + str(length_scale)\n",
    "                        + '_' + str(tau)\n",
    "                        + '_' + str(n_epoch)\n",
    "                    ),\n",
    "                    metric_name + '.txt',\n",
    "                )\n",
    "\n",
    "                scores = np.loadtxt(score_file_path).T\n",
    "                \n",
    "                for s in scores[1]:\n",
    "                    # Multiple values (for each split) for\n",
    "                    # each (hidden_dim, n_layer) combination\n",
    "                    hidden_dims_data.append(hidden_dim)\n",
    "                    hidden_layers_data.append(n_layer)\n",
    "                    scores_data.append(s)\n",
    "                    \n",
    "                mean = np.mean(scores[1])\n",
    "                var = np.var(scores[1])\n",
    "\n",
    "        ax.set_xlabel('hidden layer units')\n",
    "        ax.set_ylabel('number of hidden layers')\n",
    "        \n",
    "        #if metric_name in ('rmse_mc', 'rmse_non_mc'):\n",
    "        #    ax.set_zlim([5, 20])\n",
    "        #elif metric_name == 'lls_mc':\n",
    "        #    ax.set_zlim([-10, 0])\n",
    "    \n",
    "        ax.scatter3D(hidden_dims_data, hidden_layers_data, scores_data, c=scores_data)\n",
    "\n",
    "        fig.suptitle(figure_title, y=2.05)        \n",
    "        \n",
    "        plt.savefig(\n",
    "            os.path.join(experiment_root_directory, figure_file_name),\n",
    "            dpi=600,\n",
    "            bbox_inches='tight',\n",
    "        )\n",
    "        \n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiment_error_convergence_2_cifar10_simplecnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
